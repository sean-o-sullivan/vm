{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can generate the triplets for training the model using triplet loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c193b4340e40dfbfe1816d67bfee32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing authors:   0%|                                                                       | 0/10 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing author ID: JimGilchrist, Number of texts: 100\n",
      "4060\n",
      "Processing author ID: JohnMastrini, Number of texts: 100\n",
      "8120\n",
      "Processing author ID: KevinDrawbaugh, Number of texts: 100\n",
      "12180\n",
      "Processing author ID: MarcelMichelson, Number of texts: 100\n",
      "16240\n",
      "Processing author ID: MarkBendeich, Number of texts: 100\n",
      "20300\n",
      "Processing author ID: MartinWolk, Number of texts: 100\n",
      "24360\n",
      "Processing author ID: MatthewBunce, Number of texts: 100\n",
      "28420\n",
      "Processing author ID: SarahDavison, Number of texts: 100\n",
      "32480\n",
      "Processing author ID: TanEeLyn, Number of texts: 100\n",
      "36540\n",
      "Processing author ID: ToddNissen, Number of texts: 100\n",
      "40600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de7bde265354a5eb015e2b287d3de18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating false pairs:   0%|                                                                | 0/40600 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['anchor_embedding', 'positive_embedding'], dtype='object')\n",
      "Index(['anchor_embedding', 'negative_embedding'], dtype='object')\n",
      "0    [0.6313577898690252, 0.5812030501144747, 0.469...\n",
      "1    [0.6585505094633697, 0.6110594246712918, 0.582...\n",
      "2    [0.6639580080082189, 0.6174643094158528, 0.582...\n",
      "3    [0.6231171525545147, 0.5814809733239562, 0.480...\n",
      "4    [0.6432079573471893, 0.5938726540462341, 0.516...\n",
      "Name: anchor_embedding, dtype: object\n",
      "0    [0.6313577898690252, 0.5812030501144747, 0.469...\n",
      "1    [0.6585505094633697, 0.6110594246712918, 0.582...\n",
      "2    [0.6639580080082189, 0.6174643094158528, 0.582...\n",
      "3    [0.6231171525545147, 0.5814809733239562, 0.480...\n",
      "4    [0.6432079573471893, 0.5938726540462341, 0.516...\n",
      "Name: anchor_embedding, dtype: object\n",
      "\n",
      "\n",
      "The triplet has the following columns: Index(['anchor_embedding', 'positive_embedding', 'negative_embedding'], dtype='object')\n",
      "\n",
      "\n",
      "                                        anchor_embedding  \\\n",
      "0      [0.6313577898690252, 0.5812030501144747, 0.469...   \n",
      "1      [0.6585505094633697, 0.6110594246712918, 0.582...   \n",
      "2      [0.6639580080082189, 0.6174643094158528, 0.582...   \n",
      "3      [0.6231171525545147, 0.5814809733239562, 0.480...   \n",
      "4      [0.6432079573471893, 0.5938726540462341, 0.516...   \n",
      "...                                                  ...   \n",
      "40595  [0.666916511525108, 0.6112667245391807, 0.6650...   \n",
      "40596  [0.6662782866668117, 0.6000627768452459, 0.667...   \n",
      "40597  [0.6707191803361553, 0.6058242959326775, 0.665...   \n",
      "40598  [0.6667716263762989, 0.6030960215894469, 0.665...   \n",
      "40599  [0.6601147560039937, 0.5989455267227057, 0.670...   \n",
      "\n",
      "                                      positive_embedding  \\\n",
      "0      [0.7141767502549758, 0.6425886797826635, 0.496...   \n",
      "1      [0.6617688597933226, 0.615484677177126, 0.6275...   \n",
      "2      [0.6916683660514892, 0.6130223850601622, 0.649...   \n",
      "3      [0.6394469597431703, 0.589252186836019, 0.6118...   \n",
      "4      [0.6801067707737269, 0.6245715099342631, 0.678...   \n",
      "...                                                  ...   \n",
      "40595  [0.6561964506616184, 0.6016573892736075, 0.563...   \n",
      "40596  [0.6684286632645523, 0.6182928179648172, 0.670...   \n",
      "40597  [0.6524951693440056, 0.5947748386160505, 0.671...   \n",
      "40598  [0.6827896561746186, 0.621416249272035, 0.6438...   \n",
      "40599  [0.675976016240072, 0.6171789290637519, 0.6791...   \n",
      "\n",
      "                                      negative_embedding  \n",
      "0      [0.6339021402814695, 0.5629612330899345, 0.370...  \n",
      "1      [0.6727278811333998, 0.6133840949924119, 0.659...  \n",
      "2      [0.6410638405967194, 0.6038523173941496, 0.616...  \n",
      "3      [0.6628360009147014, 0.6037879646540841, 0.597...  \n",
      "4      [0.6797861476896391, 0.6246290678958198, 0.674...  \n",
      "...                                                  ...  \n",
      "40595  [0.6641440164523419, 0.6085330455672463, 0.675...  \n",
      "40596  [0.6658118468877878, 0.6088494498650358, 0.676...  \n",
      "40597  [0.6561861038607371, 0.6075204367289917, 0.643...  \n",
      "40598  [0.6436993695139522, 0.5961979365460609, 0.601...  \n",
      "40599  [0.6670945304619297, 0.6092537008728932, 0.675...  \n",
      "\n",
      "[40600 rows x 3 columns]\n",
      "Triplet dataset is saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import itertools\n",
    "from math import comb\n",
    "from random import sample\n",
    "import ast\n",
    "\n",
    "\n",
    "def generate_all_true_pairs(df, virtual_text_limit=20):\n",
    "    max_pairs_limit = comb(virtual_text_limit, 3) * (virtual_text_limit - 3)\n",
    "\n",
    "    all_true_pairs = []\n",
    "    all_context_embeddings_with_index = []\n",
    "    context_index = 0  # Initialize context_index here\n",
    "\n",
    "    for author_id, group in tqdm(df.groupby('author'), desc='Processing authors', dynamic_ncols=True):\n",
    "        print(f\"Processing author ID: {\n",
    "              author_id}, Number of texts: {len(group)}\")\n",
    "\n",
    "        author_texts = group.drop(columns='author')\n",
    "\n",
    "        if len(author_texts) > virtual_text_limit:\n",
    "            sampled_indices = sample(\n",
    "                range(len(author_texts)), virtual_text_limit)\n",
    "            author_texts_sampled = author_texts.iloc[sampled_indices]\n",
    "        else:\n",
    "            author_texts_sampled = author_texts\n",
    "\n",
    "        pairs, context_embeddings = generate_true_pairs_for_author(\n",
    "            author_texts_sampled, author_id, context_index)\n",
    "\n",
    "        for pair, embedding in zip(pairs[:max_pairs_limit], context_embeddings[:max_pairs_limit]):\n",
    "            all_true_pairs.append([context_index, pair[1]])\n",
    "            all_context_embeddings_with_index.append(\n",
    "                [context_index] + embedding)\n",
    "            context_index += 1  # Increment for each pair\n",
    "\n",
    "        print(len(all_true_pairs))\n",
    "\n",
    "    return all_true_pairs, all_context_embeddings_with_index\n",
    "\n",
    "\n",
    "def generate_true_pairs_for_author(author_texts, author_id, start_context_index, max_context_size=3):\n",
    "    true_pairs_list = []\n",
    "    context_embeddings_list = []\n",
    "\n",
    "    context_index = start_context_index  # Start from the passed context_index\n",
    "\n",
    "    for context_combination in itertools.combinations(author_texts.index, max_context_size):\n",
    "        context_embedding = author_texts.loc[list(\n",
    "            context_combination)].mean().values.tolist()\n",
    "        context_embeddings_list.append(context_embedding + [author_id])\n",
    "\n",
    "        remaining_texts = list(set(author_texts.index) -\n",
    "                               set(context_combination))\n",
    "        for check_index in remaining_texts:\n",
    "            check_embedding = author_texts.loc[check_index].values.tolist(\n",
    "            ) + [author_id]\n",
    "            true_pairs_list.append([context_index, check_embedding])\n",
    "        context_index += 1\n",
    "\n",
    "    return true_pairs_list, context_embeddings_list\n",
    "\n",
    "\n",
    "# datasetpath = r\"C:\\Users\\S\\Desktop\\VerifyMe\\datasets\\BAWE\\Authors_20.csv\"\n",
    "datasetpath = r\"C:\\Users\\S\\Desktop\\VerifyMe\\datasets\\Reuters\\RAuthors_20.csv\"\n",
    "\n",
    "# Usage with your DataFrame\n",
    "df = pd.read_csv(datasetpath)\n",
    "\n",
    "true_pairs_list, context_embeddings_with_index = generate_all_true_pairs(df)\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "true_pairs_df = pd.DataFrame(true_pairs_list, columns=[\n",
    "                             'context_index', 'positive_embedding'])\n",
    "\n",
    "# true_pairs_df.to_csv(r\"C:\\Users\\Ger\\Desktop\\Reuterb4TrueCheck-BAWE.csv\", index=False)\n",
    "true_pairs_df = true_pairs_df.drop('context_index', axis=1)\n",
    "\n",
    "# Save the true pairs DataFrame to CSV\n",
    "true_pairs_df.to_csv(r\"TRUEcheck.csv\", index=False)\n",
    "\n",
    "# Now create the DataFrame\n",
    "context_embeddings_df = pd.DataFrame(context_embeddings_with_index, columns=[\n",
    "                                     'context_index'] + [str(i) for i in range(len(context_embeddings_with_index[0]) - 2)] + ['author'])\n",
    "\n",
    "\n",
    "context_embeddings_df = context_embeddings_df.drop('context_index', axis=1)\n",
    "\n",
    "context_embeddings_df.to_csv(r\"Cleaned_Context.csv\", index=False)\n",
    "\n",
    "# Load the DataFrames\n",
    "context_embeddings_df = pd.read_csv(r\"Cleaned_Context.csv\")\n",
    "true_pairs_df = pd.read_csv(r\"TRUEcheck.csv\")\n",
    "\n",
    "# Function to remove the last element from the embedding list\n",
    "\n",
    "\n",
    "def remove_last_item_from_embedding(embedding_str):\n",
    "    embedding_list = ast.literal_eval(embedding_str)  # Convert string to list\n",
    "    modified_embedding_list = embedding_list[:-1]  # Remove the last element\n",
    "    return str(modified_embedding_list)  # Convert back to string\n",
    "\n",
    "\n",
    "# Apply the function to the 'positive_embedding' column\n",
    "true_pairs_df['positive_embedding'] = true_pairs_df['positive_embedding'].apply(\n",
    "    remove_last_item_from_embedding)\n",
    "\n",
    "# Drop the 'author' column from the context embeddings DataFrame\n",
    "context_embeddings_df = context_embeddings_df.drop('author', axis=1)\n",
    "\n",
    "\n",
    "# Convert context embeddings to the desired format\n",
    "context_embeddings_df['anchor_embedding'] = context_embeddings_df.apply(\n",
    "    lambda row: str(row.tolist()), axis=1)\n",
    "\n",
    "# Keep only the 'positive_embedding' column from true_pairs_df\n",
    "true_pairs_df = true_pairs_df[['positive_embedding']]\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_df = pd.concat(\n",
    "    [context_embeddings_df['anchor_embedding'], true_pairs_df], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(r\"TRUE-Pairs.csv\", index=False)\n",
    "\n",
    "\n",
    "context_embeddings_df = pd.read_csv(r\"Cleaned_Context.csv\")\n",
    "\n",
    "data = pd.read_csv(datasetpath)\n",
    "\n",
    "# Initialize the false pairs list of dictionaries\n",
    "false_pairs_list = []\n",
    "\n",
    "# Generate false pairs\n",
    "for index, context_row in tqdm(context_embeddings_df.iterrows(), total=context_embeddings_df.shape[0], desc='Generating false pairs', dynamic_ncols=True):\n",
    "    # Get the context embedding from the row\n",
    "    # Assuming the last column is 'author'\n",
    "    context_embed = context_row[:-1].tolist()\n",
    "    author = context_row['author']\n",
    "\n",
    "    # Filter the original dataset to exclude the current author\n",
    "    other_authors_data = data[data['author'] != author]\n",
    "\n",
    "    # Randomly select a check embedding from a different author\n",
    "    negative_embed = other_authors_data.sample(\n",
    "        # Assuming the last column is 'author'\n",
    "        1).iloc[:, :-1].values.flatten().tolist()\n",
    "\n",
    "    # Add the pair to the false pairs list\n",
    "    false_pairs_dict = {'anchor_embedding': context_embed,\n",
    "                        'negative_embedding': negative_embed}\n",
    "    false_pairs_list.append(false_pairs_dict)\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "false_pairs_df = pd.DataFrame(false_pairs_list)\n",
    "\n",
    "# Save the false pairs to a CSV file\n",
    "false_pairs_df.to_csv(r\"FALSE-Pairs.csv\", index=False)\n",
    "\n",
    "# Load the true and false pairs\n",
    "true_pairs = pd.read_csv(r\"True-Pairs.csv\")\n",
    "false_pairs = pd.read_csv(r\"False-Pairs.csv\")\n",
    "\n",
    "print(true_pairs.columns)\n",
    "print(false_pairs.columns)\n",
    "\n",
    "print(true_pairs[\"anchor_embedding\"].head())\n",
    "print(false_pairs[\"anchor_embedding\"].head())\n",
    "\n",
    "triplets = pd.concat([true_pairs, false_pairs[\"negative_embedding\"]], axis=1)\n",
    "print(\"\\n\")\n",
    "print(\"The triplet has the following columns: \" + str(triplets.columns))\n",
    "print(\"\\n\")\n",
    "print(triplets)\n",
    "\n",
    "# Shuffle the rows\n",
    "triplets_shuffled = triplets.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the final training file\n",
    "triplets_shuffled.to_csv(r\"30_3_RFinal-Triplets_Test.csv\", index=False)\n",
    "\n",
    "print(\"Triplet dataset is saved!\")\n",
    "\n",
    "\n",
    "# surely we can make multiples more training data now, and there wont really be the issue of class imbalances\n",
    "# 50:50 balance no longer matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 overlapping positive embeddings between training and testing sets.\n",
      "Found 0 overlapping negative embeddings between training and testing sets.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Function to safely convert stringified lists back to lists and round the floats\n",
    "def str_to_list_and_round(embedding_str, precision=5):\n",
    "    embedding_list = ast.literal_eval(embedding_str)\n",
    "    # Round each float in the list to the specified precision\n",
    "    rounded_embedding = [round(float(num), precision) for num in embedding_list]\n",
    "    return rounded_embedding\n",
    "\n",
    "# Specify the precision for rounding\n",
    "precision = 3\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"C:\\Users\\S\\Desktop\\VerifyMe\\Home\\10_3_RFinal-Triplets_Test.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\S\\Desktop\\VerifyMe\\Home\\10_3_RFinal-Triplets_Train.csv\")\n",
    "\n",
    "# Convert embeddings from string to list and round\n",
    "train_df['positive_embedding'] = train_df['positive_embedding'].apply(lambda x: str_to_list_and_round(x, precision))\n",
    "train_df['negative_embedding'] = train_df['negative_embedding'].apply(lambda x: str_to_list_and_round(x, precision))\n",
    "\n",
    "test_df['positive_embedding'] = test_df['positive_embedding'].apply(lambda x: str_to_list_and_round(x, precision))\n",
    "test_df['negative_embedding'] = test_df['negative_embedding'].apply(lambda x: str_to_list_and_round(x, precision))\n",
    "\n",
    "# Flatten the embeddings into lists\n",
    "train_pos_embeddings = list(train_df['positive_embedding'])\n",
    "train_neg_embeddings = list(train_df['negative_embedding'])\n",
    "\n",
    "test_pos_embeddings = list(test_df['positive_embedding'])\n",
    "test_neg_embeddings = list(test_df['negative_embedding'])\n",
    "\n",
    "# Convert lists of embeddings to sets of tuple for comparison\n",
    "train_pos_set = set(tuple(embedding) for embedding in train_pos_embeddings)\n",
    "train_neg_set = set(tuple(embedding) for embedding in train_neg_embeddings)\n",
    "\n",
    "test_pos_set = set(tuple(embedding) for embedding in test_pos_embeddings)\n",
    "test_neg_set = set(tuple(embedding) for embedding in test_neg_embeddings)\n",
    "\n",
    "\n",
    "# Check for overlaps\n",
    "overlap_pos = train_pos_set.intersection(test_pos_set)\n",
    "overlap_neg = train_neg_set.intersection(test_neg_set)\n",
    "\n",
    "neg_ops = train_neg_set.intersection(test_pos_set)\n",
    "# Print results\n",
    "print(f\"Found {len(overlap_pos)} overlapping positive embeddings between training and testing sets.\")\n",
    "print(f\"Found {len(overlap_neg)} overlapping negative embeddings between training and testing sets.\")\n",
    "\n",
    "if overlap_pos:\n",
    "    print(\"Overlapping positive embeddings detected.\")\n",
    "if overlap_neg:\n",
    "    print(\"Overlapping negative embeddings detected.\")\n",
    "print(len(neg_ops))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (verifyMeEnv)",
   "language": "python",
   "name": "verifymeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
