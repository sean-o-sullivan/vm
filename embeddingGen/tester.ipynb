{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 01:45:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 38.9MB/s]                    \n",
      "2024-08-24 01:45:26 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:45:27 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 01:45:27 INFO: Using device: cpu\n",
      "2024-08-24 01:45:27 INFO: Loading: tokenize\n",
      "2024-08-24 01:45:27 INFO: Loading: mwt\n",
      "2024-08-24 01:45:27 INFO: Loading: pos\n",
      "2024-08-24 01:45:27 INFO: Loading: lemma\n",
      "2024-08-24 01:45:27 INFO: Loading: constituency\n",
      "2024-08-24 01:45:27 INFO: Loading: depparse\n",
      "2024-08-24 01:45:27 INFO: Loading: sentiment\n",
      "2024-08-24 01:45:27 INFO: Loading: ner\n",
      "2024-08-24 01:45:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the doc is [\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"The\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 3,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"quick\",\n",
      "      \"lemma\": \"quick\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 4,\n",
      "      \"end_char\": 9,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"brown\",\n",
      "      \"lemma\": \"brown\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 10,\n",
      "      \"end_char\": 15,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"fox\",\n",
      "      \"lemma\": \"fox\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 16,\n",
      "      \"end_char\": 19,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"jumps\",\n",
      "      \"lemma\": \"jump\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 20,\n",
      "      \"end_char\": 25,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"over\",\n",
      "      \"lemma\": \"over\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 26,\n",
      "      \"end_char\": 30,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 31,\n",
      "      \"end_char\": 34,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"lazy\",\n",
      "      \"lemma\": \"lazy\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 39,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"dog\",\n",
      "      \"lemma\": \"dog\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 40,\n",
      "      \"end_char\": 43,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 43,\n",
      "      \"end_char\": 44,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"He\",\n",
      "      \"lemma\": \"he\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 45,\n",
      "      \"end_char\": 47,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"then\",\n",
      "      \"lemma\": \"then\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"feats\": \"PronType=Dem\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 48,\n",
      "      \"end_char\": 52,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"runs\",\n",
      "      \"lemma\": \"run\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 53,\n",
      "      \"end_char\": 57,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"away\",\n",
      "      \"lemma\": \"away\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 58,\n",
      "      \"end_char\": 62,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 62,\n",
      "      \"end_char\": 63,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"leaving\",\n",
      "      \"lemma\": \"leave\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBG\",\n",
      "      \"feats\": \"VerbForm=Ger\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"advcl\",\n",
      "      \"start_char\": 64,\n",
      "      \"end_char\": 71,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 72,\n",
      "      \"end_char\": 75,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"dog\",\n",
      "      \"lemma\": \"dog\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 76,\n",
      "      \"end_char\": 79,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"behind\",\n",
      "      \"lemma\": \"behind\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"compound:prt\",\n",
      "      \"start_char\": 80,\n",
      "      \"end_char\": 86,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 86,\n",
      "      \"end_char\": 87,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"After\",\n",
      "      \"lemma\": \"after\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 88,\n",
      "      \"end_char\": 93,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"that\",\n",
      "      \"lemma\": \"that\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Number=Sing|PronType=Dem\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 94,\n",
      "      \"end_char\": 98,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 98,\n",
      "      \"end_char\": 99,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 100,\n",
      "      \"end_char\": 103,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"fox\",\n",
      "      \"lemma\": \"fox\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 104,\n",
      "      \"end_char\": 107,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"went\",\n",
      "      \"lemma\": \"go\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBD\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 108,\n",
      "      \"end_char\": 112,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 113,\n",
      "      \"end_char\": 115,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 116,\n",
      "      \"end_char\": 119,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"forest\",\n",
      "      \"lemma\": \"forest\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 120,\n",
      "      \"end_char\": 126,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"PART\",\n",
      "      \"xpos\": \"TO\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"mark\",\n",
      "      \"start_char\": 127,\n",
      "      \"end_char\": 129,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 11,\n",
      "      \"text\": \"find\",\n",
      "      \"lemma\": \"find\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VB\",\n",
      "      \"feats\": \"VerbForm=Inf\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"advcl\",\n",
      "      \"start_char\": 130,\n",
      "      \"end_char\": 134,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 12,\n",
      "      \"text\": \"food\",\n",
      "      \"lemma\": \"food\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 135,\n",
      "      \"end_char\": 139,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 13,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 139,\n",
      "      \"end_char\": 140,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    }\n",
      "  ]\n",
      "]\n",
      "Clause 1: The quick brown fox jumps over the lazy dog .\n",
      "Clause 2: He then runs away , leaving the dog behind .\n",
      "Clause 3: After that , the fox went to the forest to find food .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def extract_clauses(doc):\n",
    "\n",
    "    clauses = []\n",
    "    current_clause = []\n",
    "\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        \n",
    "        for word in sent.words:\n",
    "            if word.deprel in {'ccomp', 'xcomp', 'acl'}:  \n",
    "                if current_clause:\n",
    "                    clauses.append(current_clause)\n",
    "                    current_clause = []\n",
    "            current_clause.append(word.text)\n",
    "\n",
    "        \n",
    "        if current_clause:\n",
    "            clauses.append(current_clause)\n",
    "            current_clause = []\n",
    "\n",
    "    return clauses\n",
    "\n",
    "def main(text):\n",
    "    \n",
    "    nlp = stanza.Pipeline('en')\n",
    "\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    print('the doc is', doc)\n",
    "\n",
    "    \n",
    "    clauses = extract_clauses(doc)\n",
    "\n",
    "    \n",
    "    for i, clause in enumerate(clauses):\n",
    "        print(f\"Clause {i + 1}: {' '.join(clause)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = (\n",
    "        \"The quick brown fox jumps over the lazy dog. \"\n",
    "        \"He then runs away, leaving the dog behind. \"\n",
    "        \"After that, the fox went to the forest to find food.\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    main(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 48.0MB/s]                    \n",
      "2024-08-24 01:47:16 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:47:16 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 01:47:17 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 01:47:18 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 01:47:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 33.8MB/s]                    \n",
      "2024-08-24 01:47:18 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:47:19 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-08-24 01:47:19 INFO: Using device: cpu\n",
      "2024-08-24 01:47:19 INFO: Loading: tokenize\n",
      "2024-08-24 01:47:19 INFO: Loading: mwt\n",
      "2024-08-24 01:47:19 INFO: Loading: pos\n",
      "2024-08-24 01:47:19 INFO: Loading: lemma\n",
      "2024-08-24 01:47:19 INFO: Loading: depparse\n",
      "2024-08-24 01:47:19 INFO: Loading: ner\n",
      "2024-08-24 01:47:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stanza.models.common.doc.Document'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"In the heart of a city that never seemed to sleep, neon lights flickered like the last desperate gasps of a dying firefly. Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "print(type(doc))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clauses(doc):\n",
    "\n",
    "    clauses = []\n",
    "    current_clause = []\n",
    "\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        \n",
    "        for word in sent.words:\n",
    "            \n",
    "            if word.deprel in {'ccomp', 'xcomp', 'acl', 'advcl', 'relcl'}:\n",
    "                if current_clause:\n",
    "                    clauses.append(current_clause)\n",
    "                    current_clause = []\n",
    "            current_clause.append(word.text)\n",
    "\n",
    "        \n",
    "        if current_clause:\n",
    "            clauses.append(current_clause)\n",
    "            current_clause = []\n",
    "\n",
    "    return clauses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 41.7MB/s]                    \n",
      "2024-08-24 01:48:03 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:48:03 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 01:48:05 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 01:48:06 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 01:48:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 42.6MB/s]                    \n",
      "2024-08-24 01:48:06 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:48:07 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-08-24 01:48:07 INFO: Using device: cpu\n",
      "2024-08-24 01:48:07 INFO: Loading: tokenize\n",
      "2024-08-24 01:48:07 INFO: Loading: mwt\n",
      "2024-08-24 01:48:07 INFO: Loading: pos\n",
      "2024-08-24 01:48:07 INFO: Loading: lemma\n",
      "2024-08-24 01:48:07 INFO: Loading: depparse\n",
      "2024-08-24 01:48:07 INFO: Loading: ner\n",
      "2024-08-24 01:48:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1: In the heart of a city that never seemed to\n",
      "Clause 2: sleep , neon lights flickered like the last desperate gasps of a dying firefly .\n",
      "Clause 3: Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise - cracking , undead\n",
      "Clause 4: detective .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n",
    "\n",
    "\n",
    "text = \"In the heart of a city that never seemed to sleep, neon lights flickered like the last desperate gasps of a dying firefly. Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "clauses = extract_clauses(doc)\n",
    "for i, clause in enumerate(clauses):\n",
    "    print(f\"Clause {i + 1}: {' '.join(clause)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominalization(text):\n",
    "\n",
    "    results = readability.getmeasures(text, lang='en')\n",
    "    return results['word usage']['nominalization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominalization Score: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_paragraph = \"\"\"\n",
    "The government's decision to implement stricter regulations was met with widespread criticism. \n",
    "However, the justification for these regulations was primarily based on the perceived need to \n",
    "reduce environmental degradation. The opposition's argument was that such measures would \n",
    "lead to economic stagnation and job losses. Despite these concerns, the administration proceeded \n",
    "with the enforcement of the new laws, emphasizing the long-term benefits over short-term challenges.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "score = nominalization(test_paragraph)\n",
    "print(f\"Nominalization Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 105MB/s]                     \n",
      "2024-08-24 22:29:54 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 22:29:54 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 22:29:55 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 22:29:56 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 22:29:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 40.2MB/s]                    \n",
      "2024-08-24 22:29:56 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 22:29:57 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 22:29:57 INFO: Using device: cpu\n",
      "2024-08-24 22:29:57 INFO: Loading: tokenize\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: mwt\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: pos\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: lemma\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: constituency\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Loading: depparse\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Loading: sentiment\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Loading: ner\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Noisy Text: \n",
      "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
      "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
      "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
      "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
      "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Noisy Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_paragraph\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Reconstructed Text\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m reconstructed_text \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct_text_from_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructed Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreconstructed_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Readability Scores\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mreconstruct_text_from_doc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     23\u001b[0m word_text \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Handle cases where there's no space after the word (e.g., punctuation)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmisc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeats\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpaceAfter=No\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mwords[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfeats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmisc\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     26\u001b[0m     reconstructed_text\u001b[38;5;241m.\u001b[39mappend(word_text)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "from textstat import textstat\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "def reconstruct_text_from_doc(doc):\n",
    "\n",
    "    reconstructed_text = []\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            word_text = token.text\n",
    "            \n",
    "            if 'misc' in token.words[0].feats and 'SpaceAfter=No' in token.words[0].feats['misc']:\n",
    "                reconstructed_text.append(word_text)\n",
    "            else:\n",
    "                reconstructed_text.append(word_text + ' ')\n",
    "    return ''.join(reconstructed_text).strip()\n",
    "\n",
    "\n",
    "def flesch_reading_ease(doc):\n",
    " \n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def GFI(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.gunning_fog(text)\n",
    "\n",
    "def coleman_liau_index(doc):\n",
    " \n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.coleman_liau_index(text)\n",
    "\n",
    "def ari(doc):\n",
    " \n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.automated_readability_index(text)\n",
    "\n",
    "def dale_chall_readability_score(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.dale_chall_readability_score(text)\n",
    "\n",
    "def lix(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.lix(text)\n",
    "\n",
    "def smog_index(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.smog_index(text)\n",
    "\n",
    "def rix(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.rix(text)\n",
    "\n",
    "\n",
    "test_paragraph = \"\"\"\n",
    "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
    "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
    "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
    "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
    "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "doc = nlp(test_paragraph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original Noisy Text: {test_paragraph}\\n\")\n",
    "\n",
    "\n",
    "reconstructed_text = reconstruct_text_from_doc(doc)\n",
    "print(f\"Reconstructed Text: {reconstructed_text}\\n\")\n",
    "\n",
    "\n",
    "flesch_score = flesch_reading_ease(doc)\n",
    "print(f\"Flesch Reading Ease Score: {flesch_score:.2f}\")\n",
    "\n",
    "gfi_score = GFI(doc)\n",
    "print(f\"Gunning Fog Index: {gfi_score:.2f}\")\n",
    "\n",
    "cli_score = coleman_liau_index(doc)\n",
    "print(f\"Coleman-Liau Index: {cli_score:.2f}\")\n",
    "\n",
    "ari_score = ari(doc)\n",
    "print(f\"Automated Readability Index: {ari_score:.2f}\")\n",
    "\n",
    "dale_chall_score = dale_chall_readability_score(doc)\n",
    "print(f\"Dale-Chall Readability Score: {dale_chall_score:.2f}\")\n",
    "\n",
    "lix_score = lix(doc)\n",
    "print(f\"LIX Score: {lix_score:.2f}\")\n",
    "\n",
    "smog_score = smog_index(doc)\n",
    "print(f\"SMOG Index: {smog_score:.2f}\")\n",
    "\n",
    "rix_score = rix(doc)\n",
    "print(f\"RIX Score: {rix_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Noisy Text: \n",
      "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
      "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
      "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
      "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
      "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
      "\n",
      "\n",
      "Reconstructed Text: The government's decision to implement stricter regulations ( Figure 2.4 ) was met with widespread criticism . However , the justification for these regulations was primarily based on the perceived need to reduce environmental degradation . According to recent studies ( Smith et al. , 2020 ) , the degradation could cause economic losses up to $ 2.3 billion . The opposition's argument was that such measures would lead to economic stagnation ( see Table 3 ) , and possibly increase unemployment by 15 % . Despite these concerns , the administration proceeded with the enforcement of the new laws ( see Figure 1 ) , emphasizing the long - term benefits over short - term challenges .\n",
      "\n",
      "Flesch Reading Ease Score: 40.75\n",
      "Gunning Fog Index: 14.28\n",
      "Coleman-Liau Index: 15.01\n",
      "Automated Readability Index: 13.50\n",
      "Dale-Chall Readability Score: 11.79\n",
      "LIX Score: 43.26\n",
      "SMOG Index: 13.90\n",
      "RIX Score: 5.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def reconstruct_text_from_doc(doc):\n",
    "\n",
    "    reconstructed_text = []\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            word_text = token.text\n",
    "            \n",
    "            if token.words[0].feats and 'SpaceAfter=No' in token.words[0].feats:\n",
    "                reconstructed_text.append(word_text)\n",
    "            else:\n",
    "                reconstructed_text.append(word_text + ' ')\n",
    "    return ''.join(reconstructed_text).strip()\n",
    "\n",
    "\n",
    "test_paragraph = \"\"\"\n",
    "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
    "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
    "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
    "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
    "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "doc = nlp(test_paragraph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original Noisy Text: {test_paragraph}\\n\")\n",
    "\n",
    "\n",
    "reconstructed_text = reconstruct_text_from_doc(doc)\n",
    "print(f\"Reconstructed Text: {reconstructed_text}\\n\")\n",
    "\n",
    "\n",
    "flesch_score = flesch_reading_ease(doc)\n",
    "print(f\"Flesch Reading Ease Score: {flesch_score:.2f}\")\n",
    "\n",
    "gfi_score = GFI(doc)\n",
    "print(f\"Gunning Fog Index: {gfi_score:.2f}\")\n",
    "\n",
    "cli_score = coleman_liau_index(doc)\n",
    "print(f\"Coleman-Liau Index: {cli_score:.2f}\")\n",
    "\n",
    "ari_score = ari(doc)\n",
    "print(f\"Automated Readability Index: {ari_score:.2f}\")\n",
    "\n",
    "dale_chall_score = dale_chall_readability_score(doc)\n",
    "print(f\"Dale-Chall Readability Score: {dale_chall_score:.2f}\")\n",
    "\n",
    "lix_score = lix(doc)\n",
    "print(f\"LIX Score: {lix_score:.2f}\")\n",
    "\n",
    "smog_score = smog_index(doc)\n",
    "print(f\"SMOG Index: {smog_score:.2f}\")\n",
    "\n",
    "rix_score = rix(doc)\n",
    "print(f\"RIX Score: {rix_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 23:08:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 88.8MB/s]                    \n",
      "2024-08-24 23:08:43 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:08:44 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:08:44 INFO: Using device: cpu\n",
      "2024-08-24 23:08:44 INFO: Loading: tokenize\n",
      "2024-08-24 23:08:44 INFO: Loading: mwt\n",
      "2024-08-24 23:08:44 INFO: Loading: pos\n",
      "2024-08-24 23:08:44 INFO: Loading: lemma\n",
      "2024-08-24 23:08:44 INFO: Loading: constituency\n",
      "2024-08-24 23:08:44 INFO: Loading: depparse\n",
      "2024-08-24 23:08:45 INFO: Loading: sentiment\n",
      "2024-08-24 23:08:45 INFO: Loading: ner\n",
      "2024-08-24 23:08:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frazier Depth for each sentence:\n",
      "Sentence 1: 7\n",
      "Sentence 2: 5\n",
      "Sentence 3: 8\n",
      "Sentence 4: 5\n",
      "Sentence 5: 5\n",
      "Sentence 6: 7\n",
      "\n",
      "Average Frazier Depth for the entire text: 6.17\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def calculate_frazier_depth(sentence):\n",
    "    \"\"\"\n",
    "    Computes the Frazier Depth for a given sentence based on its dependency parse.\n",
    "    Parameters:\n",
    "    sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "    Returns:\n",
    "    int: The depth of the syntactic structure.\n",
    "    \"\"\"\n",
    "    def build_tree(words):\n",
    "        tree = {word.id: [] for word in words}\n",
    "        for word in words:\n",
    "            if word.head != 0:  \n",
    "                tree[word.head].append(word.id)\n",
    "        return tree\n",
    "\n",
    "    def depth(node_id, tree, current_depth):\n",
    "        max_depth = current_depth\n",
    "        for child_id in tree[node_id]:\n",
    "            max_depth = max(max_depth, depth(child_id, tree, current_depth + 1))\n",
    "        return max_depth\n",
    "\n",
    "    \n",
    "    dependency_tree = build_tree(sentence.words)\n",
    "    \n",
    "    \n",
    "    root_id = next(word.id for word in sentence.words if word.head == 0)\n",
    "    \n",
    "    \n",
    "    return depth(root_id, dependency_tree, 0)\n",
    "\n",
    "def frazier_depth(doc):\n",
    "    \"\"\"\n",
    "    Computes the average Frazier Depth for all sentences in a document.\n",
    "    Parameters:\n",
    "    doc (stanza.Document): A Stanza Document object.\n",
    "    Returns:\n",
    "    float: The average Frazier Depth for the document.\n",
    "    \"\"\"\n",
    "    depths = []\n",
    "    for sentence in doc.sentences:\n",
    "        depths.append(calculate_frazier_depth(sentence))\n",
    "    \n",
    "    if not depths:\n",
    "        return 0.0\n",
    "    \n",
    "    return sum(depths) / len(depths)\n",
    "\n",
    "def test_frazier_depth():\n",
    "    \n",
    "    nlp = stanza.Pipeline('en')\n",
    "\n",
    "    text = \"\"\"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"You know,\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent street vendor offering 'mystical' hot dogs, \"if I had a penny for every time someone tried to sell me something magical, I'd be richer than the most miserly dragon you can imagine.\" Valkyrie shot him a look that combined exasperation with a hint of amusement. \"And if I had a nickel for every time you made a joke that wasn't terrible, I'd still be broke, but at least I'd have a better sense of humor.\" Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark and absurdly dangerous adventure lay ahead.\"\"\"\n",
    "\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    \n",
    "    print(\"Frazier Depth for each sentence:\")\n",
    "    for i, sentence in enumerate(doc.sentences, 1):\n",
    "        depth = calculate_frazier_depth(sentence)\n",
    "        print(f\"Sentence {i}: {depth}\")\n",
    "\n",
    "    \n",
    "    avg_depth = frazier_depth(doc)\n",
    "    print(f\"\\nAverage Frazier Depth for the entire text: {avg_depth:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_frazier_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: emoji in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.12.1)\n",
      "Requirement already satisfied: numpy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (5.27.3)\n",
      "Requirement already satisfied: requests in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: toml in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (4.66.5)\n",
      "Requirement already satisfied: filelock in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 30.4MB/s]                    \n",
      "2024-08-24 23:39:11 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:39:14 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 23:39:15 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 23:39:17 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 23:39:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 33.0MB/s]                    \n",
      "2024-08-24 23:39:17 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:39:18 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:39:18 INFO: Using device: cpu\n",
      "2024-08-24 23:39:18 INFO: Loading: tokenize\n",
      "2024-08-24 23:39:18 INFO: Loading: mwt\n",
      "2024-08-24 23:39:18 INFO: Loading: pos\n",
      "2024-08-24 23:39:18 INFO: Loading: lemma\n",
      "2024-08-24 23:39:18 INFO: Loading: constituency\n",
      "2024-08-24 23:39:18 INFO: Loading: depparse\n",
      "2024-08-24 23:39:19 INFO: Loading: sentiment\n",
      "2024-08-24 23:39:19 INFO: Loading: ner\n",
      "2024-08-24 23:39:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 1.0000\n",
      "expletive_inversion: 0.0000\n",
      "emphatic_inversion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install stanza\n",
    "\n",
    "\n",
    "import stanza\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "\n",
    "    \n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "        elif word.deprel == 'root':\n",
    "            verb_positions.append(i)\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos:\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        for expl_pos in expletive_positions:\n",
    "            for verb_pos in verb_positions:\n",
    "                if expl_pos < verb_pos:\n",
    "                    return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and subject_positions:\n",
    "        for emph_pos in emphatic_positions:\n",
    "            for subj_pos in subject_positions:\n",
    "                if emph_pos < subj_pos:\n",
    "                    return \"emphatic_inversion\"\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    \"\"\"\n",
    "    Computes the normalized frequency of different types of sentence inversions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the normalized frequency of each type of inversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, \"\n",
    "    \"desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of \"\n",
    "    \"nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind \"\n",
    "    \"him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"\n",
    "    \"\\\"You know,\\\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent \"\n",
    "    \"street vendor offering mystical hot dogs, \\\"if I had a penny for every time someone tried to sell me \"\n",
    "    \"something magical, Id be richer than the most miserly dragon you can imagine.\\\" Valkyrie shot him a look \"\n",
    "    \"that combined exasperation with a hint of amusement. \\\"And if I had a nickel for every time you made a \"\n",
    "    \"joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\\\" \"\n",
    "    \"Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark \"\n",
    "    \"and absurdly dangerous adventure lay ahead. There is no greater danger than underestimating the enemy, \"\n",
    "    \"especially when that enemy lurks in the shadows, biding its time, waiting to strike. There in the darkness, \"\n",
    "    \"waiting patiently, lies a threat that no amount of bravado can overcome. Here comes the night, sweeping \"\n",
    "    \"across the city like a veil, concealing the dangers that await the unprepared.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"Inversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: emoji in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.12.1)\n",
      "Requirement already satisfied: numpy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (5.27.3)\n",
      "Requirement already satisfied: requests in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: toml in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (4.66.5)\n",
      "Requirement already satisfied: filelock in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 29.2MB/s]                    \n",
      "2024-08-24 23:40:25 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:40:25 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 23:40:26 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 23:40:28 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 23:40:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 39.0MB/s]                    \n",
      "2024-08-24 23:40:28 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:40:29 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:40:29 INFO: Using device: cpu\n",
      "2024-08-24 23:40:29 INFO: Loading: tokenize\n",
      "2024-08-24 23:40:29 INFO: Loading: mwt\n",
      "2024-08-24 23:40:29 INFO: Loading: pos\n",
      "2024-08-24 23:40:29 INFO: Loading: lemma\n",
      "2024-08-24 23:40:29 INFO: Loading: constituency\n",
      "2024-08-24 23:40:29 INFO: Loading: depparse\n",
      "2024-08-24 23:40:30 INFO: Loading: sentiment\n",
      "2024-08-24 23:40:30 INFO: Loading: ner\n",
      "2024-08-24 23:40:30 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'In the heart of a city that never seemed to sleep, where neon lights flickered like the last, desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.'\n",
      "Word: In, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: heart, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 6\n",
      "Word: never, DepRel: advmod, UPOS: ADV\n",
      "Word: seemed, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sleep, DepRel: xcomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: where, DepRel: advmod, UPOS: ADV\n",
      "Word: neon, DepRel: compound, UPOS: NOUN\n",
      "Word: lights, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: flickered, DepRel: advcl, UPOS: VERB\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: last, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: desperate, DepRel: amod, UPOS: ADJ\n",
      "Word: gasps, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: dying, DepRel: amod, UPOS: NOUN\n",
      "Word: firefly, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 27\n",
      "Word: Pleasant, DepRel: flat, UPOS: PROPN\n",
      "Word: strolled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 29\n",
      "Word: down, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: alley, DepRel: obl, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: kind, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: nonchalance, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 38\n",
      "Word: only, DepRel: advmod, UPOS: ADV\n",
      "Word: comes, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: from, DepRel: mark, UPOS: SCONJ\n",
      "Word: being, DepRel: cop, UPOS: AUX\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: wise, DepRel: compound, UPOS: ADJ\n",
      "Word: -, DepRel: punct, UPOS: PUNCT\n",
      "Word: cracking, DepRel: amod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: undead, DepRel: amod, UPOS: ADJ\n",
      "Word: detective, DepRel: advcl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'His overcoat flapped behind him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble.'\n",
      "Word: His, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: overcoat, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 1\n",
      "Word: flapped, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 2\n",
      "Word: behind, DepRel: case, UPOS: ADP\n",
      "Word: him, DepRel: obl, UPOS: PRON\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: ragged, DepRel: amod, UPOS: ADJ\n",
      "Word: flag, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: rebellion, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: while, DepRel: mark, UPOS: SCONJ\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: eyes, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sharp, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: knowing, DepRel: conj, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: scanned, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: trouble, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"You know,\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent street vendor offering mystical hot dogs, \"if I had a penny for every time someone tried to sell me something magical, Id be richer than the most miserly dragon you can imagine.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: You, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 1\n",
      "Word: know, DepRel: ccomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: said, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 6\n",
      "Word: to, DepRel: case, UPOS: ADP\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: companion, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Valkyrie, DepRel: appos, UPOS: PROPN\n",
      "Word: Cain, DepRel: flat, UPOS: PROPN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: who, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: busy, DepRel: acl:relcl, UPOS: ADJ\n",
      "Word: fending, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: particularly, DepRel: advmod, UPOS: ADV\n",
      "Word: persistent, DepRel: amod, UPOS: ADJ\n",
      "Word: street, DepRel: compound, UPOS: NOUN\n",
      "Word: vendor, DepRel: obj, UPOS: NOUN\n",
      "Word: offering, DepRel: acl, UPOS: VERB\n",
      "Word: , DepRel: punct, UPOS: PUNCT\n",
      "Word: mystical, DepRel: nmod:poss, UPOS: ADJ\n",
      "Word: , DepRel: case, UPOS: PART\n",
      "Word: hot, DepRel: amod, UPOS: ADJ\n",
      "Word: dogs, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 33\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: penny, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: someone, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 40\n",
      "Word: tried, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sell, DepRel: xcomp, UPOS: VERB\n",
      "Word: me, DepRel: iobj, UPOS: PRON\n",
      "Word: something, DepRel: obj, UPOS: PRON\n",
      "Word: magical, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 48\n",
      "Word: d, DepRel: aux, UPOS: AUX\n",
      "Word: be, DepRel: cop, UPOS: AUX\n",
      "Word: richer, DepRel: parataxis, UPOS: ADJ\n",
      "Word: than, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: most, DepRel: advmod, UPOS: ADV\n",
      "Word: miserly, DepRel: amod, UPOS: ADJ\n",
      "Word: dragon, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 57\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: imagine, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Valkyrie shot him a look that combined exasperation with a hint of amusement.'\n",
      "Word: Valkyrie, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: shot, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: him, DepRel: iobj, UPOS: PRON\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: look, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: combined, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: exasperation, DepRel: obj, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: hint, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: amusement, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"And if I had a nickel for every time you made a joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: And, DepRel: cc, UPOS: CCONJ\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 3\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: nickel, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 10\n",
      "Word: made, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: joke, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: nt, DepRel: advmod, UPOS: PART\n",
      "Word: terrible, DepRel: parataxis, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 19\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: still, DepRel: advmod, UPOS: ADV\n",
      "Word: be, DepRel: aux, UPOS: AUX\n",
      "Word: broke, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 23\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: but, DepRel: cc, UPOS: CCONJ\n",
      "Word: at, DepRel: advmod, UPOS: ADP\n",
      "Word: least, DepRel: fixed, UPOS: ADJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 28\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: have, DepRel: conj, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: better, DepRel: amod, UPOS: ADJ\n",
      "Word: sense, DepRel: obj, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: humor, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark and absurdly dangerous adventure lay ahead.'\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: chuckled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: sound, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: echoing, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: graffiti-, DepRel: advmod, UPOS: ADJ\n",
      "Word: covered, DepRel: amod, UPOS: VERB\n",
      "Word: walls, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: as, DepRel: mark, UPOS: SCONJ\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 13\n",
      "Word: prepared, DepRel: advcl, UPOS: VERB\n",
      "Word: for, DepRel: mark, UPOS: ADP\n",
      "Word: whatever, DepRel: det, UPOS: DET\n",
      "Word: dark, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: absurdly, DepRel: advmod, UPOS: ADV\n",
      "Word: dangerous, DepRel: conj, UPOS: ADJ\n",
      "Word: adventure, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 21\n",
      "Word: lay, DepRel: advcl, UPOS: VERB\n",
      "Word: ahead, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is no greater danger than underestimating the enemy, especially when that enemy lurks in the shadows, biding its time, waiting to strike.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: greater, DepRel: amod, UPOS: ADJ\n",
      "Word: danger, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: than, DepRel: mark, UPOS: SCONJ\n",
      "Word: underestimating, DepRel: acl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: especially, DepRel: advmod, UPOS: ADV\n",
      "Word: when, DepRel: advmod, UPOS: ADV\n",
      "Word: that, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: lurks, DepRel: advcl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: biding, DepRel: advcl, UPOS: VERB\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: time, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: strike, DepRel: xcomp, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There in the darkness, waiting patiently, lies a threat that no amount of bravado can overcome.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: darkness, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: patiently, DepRel: advmod, UPOS: ADV\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 8\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: threat, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: mark, UPOS: SCONJ\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: amount, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: bravado, DepRel: nmod, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: overcome, DepRel: acl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the night, sweeping across the city like a veil, concealing the dangers that await the unprepared.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: night, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sweeping, DepRel: advcl, UPOS: VERB\n",
      "Word: across, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: obl, UPOS: NOUN\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: veil, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: concealing, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: dangers, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 16\n",
      "Word: await, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: unprepared, DepRel: obj, UPOS: ADJ\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 1.0000\n",
      "expletive_inversion: 0.0000\n",
      "emphatic_inversion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install stanza\n",
    "\n",
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "\n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel == 'root':\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb (root) found at position {i}\")\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        for expl_pos in expletive_positions:\n",
    "            for verb_pos in verb_positions:\n",
    "                if expl_pos < verb_pos:\n",
    "                    print(\" -> Expletive inversion detected\")\n",
    "                    return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and subject_positions:\n",
    "        for emph_pos in emphatic_positions:\n",
    "            for subj_pos in subject_positions:\n",
    "                if emph_pos < subj_pos:\n",
    "                    print(\" -> Emphatic inversion detected\")\n",
    "                    return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    \"\"\"\n",
    "    Computes the normalized frequency of different types of sentence inversions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the normalized frequency of each type of inversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, \"\n",
    "    \"desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of \"\n",
    "    \"nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind \"\n",
    "    \"him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"\n",
    "    \"\\\"You know,\\\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent \"\n",
    "    \"street vendor offering mystical hot dogs, \\\"if I had a penny for every time someone tried to sell me \"\n",
    "    \"something magical, Id be richer than the most miserly dragon you can imagine.\\\" Valkyrie shot him a look \"\n",
    "    \"that combined exasperation with a hint of amusement. \\\"And if I had a nickel for every time you made a \"\n",
    "    \"joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\\\" \"\n",
    "    \"Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark \"\n",
    "    \"and absurdly dangerous adventure lay ahead. There is no greater danger than underestimating the enemy, \"\n",
    "    \"especially when that enemy lurks in the shadows, biding its time, waiting to strike. There in the darkness, \"\n",
    "    \"waiting patiently, lies a threat that no amount of bravado can overcome. Here comes the night, sweeping \"\n",
    "    \"across the city like a veil, concealing the dangers that await the unprepared.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.5MB/s]                    \n",
      "2024-08-24 23:44:52 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:44:52 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 23:44:53 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 23:44:55 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 23:44:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 26.0MB/s]                    \n",
      "2024-08-24 23:44:55 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:44:56 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:44:56 INFO: Using device: cpu\n",
      "2024-08-24 23:44:56 INFO: Loading: tokenize\n",
      "2024-08-24 23:44:56 INFO: Loading: mwt\n",
      "2024-08-24 23:44:56 INFO: Loading: pos\n",
      "2024-08-24 23:44:56 INFO: Loading: lemma\n",
      "2024-08-24 23:44:56 INFO: Loading: constituency\n",
      "2024-08-24 23:44:57 INFO: Loading: depparse\n",
      "2024-08-24 23:44:57 INFO: Loading: sentiment\n",
      "2024-08-24 23:44:57 INFO: Loading: ner\n",
      "2024-08-24 23:44:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'In the heart of a city that never seemed to sleep, where neon lights flickered like the last, desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.'\n",
      "Word: In, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: heart, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 6\n",
      "Word: never, DepRel: advmod, UPOS: ADV\n",
      "Word: seemed, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sleep, DepRel: xcomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: where, DepRel: advmod, UPOS: ADV\n",
      "Word: neon, DepRel: compound, UPOS: NOUN\n",
      "Word: lights, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: flickered, DepRel: advcl, UPOS: VERB\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: last, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: desperate, DepRel: amod, UPOS: ADJ\n",
      "Word: gasps, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: dying, DepRel: amod, UPOS: NOUN\n",
      "Word: firefly, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 27\n",
      "Word: Pleasant, DepRel: flat, UPOS: PROPN\n",
      "Word: strolled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 29\n",
      "Word: down, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: alley, DepRel: obl, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: kind, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: nonchalance, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 38\n",
      "Word: only, DepRel: advmod, UPOS: ADV\n",
      "Word: comes, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: from, DepRel: mark, UPOS: SCONJ\n",
      "Word: being, DepRel: cop, UPOS: AUX\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: wise, DepRel: compound, UPOS: ADJ\n",
      "Word: -, DepRel: punct, UPOS: PUNCT\n",
      "Word: cracking, DepRel: amod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: undead, DepRel: amod, UPOS: ADJ\n",
      "Word: detective, DepRel: advcl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'His overcoat flapped behind him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble.'\n",
      "Word: His, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: overcoat, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 1\n",
      "Word: flapped, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 2\n",
      "Word: behind, DepRel: case, UPOS: ADP\n",
      "Word: him, DepRel: obl, UPOS: PRON\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: ragged, DepRel: amod, UPOS: ADJ\n",
      "Word: flag, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: rebellion, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: while, DepRel: mark, UPOS: SCONJ\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: eyes, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sharp, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: knowing, DepRel: conj, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: scanned, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: trouble, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"You know,\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent street vendor offering mystical hot dogs, \"if I had a penny for every time someone tried to sell me something magical, Id be richer than the most miserly dragon you can imagine.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: You, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 1\n",
      "Word: know, DepRel: ccomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: said, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 6\n",
      "Word: to, DepRel: case, UPOS: ADP\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: companion, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Valkyrie, DepRel: appos, UPOS: PROPN\n",
      "Word: Cain, DepRel: flat, UPOS: PROPN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: who, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: busy, DepRel: acl:relcl, UPOS: ADJ\n",
      "Word: fending, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: particularly, DepRel: advmod, UPOS: ADV\n",
      "Word: persistent, DepRel: amod, UPOS: ADJ\n",
      "Word: street, DepRel: compound, UPOS: NOUN\n",
      "Word: vendor, DepRel: obj, UPOS: NOUN\n",
      "Word: offering, DepRel: acl, UPOS: VERB\n",
      "Word: , DepRel: punct, UPOS: PUNCT\n",
      "Word: mystical, DepRel: nmod:poss, UPOS: ADJ\n",
      "Word: , DepRel: case, UPOS: PART\n",
      "Word: hot, DepRel: amod, UPOS: ADJ\n",
      "Word: dogs, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 33\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: penny, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: someone, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 40\n",
      "Word: tried, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sell, DepRel: xcomp, UPOS: VERB\n",
      "Word: me, DepRel: iobj, UPOS: PRON\n",
      "Word: something, DepRel: obj, UPOS: PRON\n",
      "Word: magical, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 48\n",
      "Word: d, DepRel: aux, UPOS: AUX\n",
      "Word: be, DepRel: cop, UPOS: AUX\n",
      "Word: richer, DepRel: parataxis, UPOS: ADJ\n",
      "Word: than, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: most, DepRel: advmod, UPOS: ADV\n",
      "Word: miserly, DepRel: amod, UPOS: ADJ\n",
      "Word: dragon, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 57\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: imagine, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Valkyrie shot him a look that combined exasperation with a hint of amusement.'\n",
      "Word: Valkyrie, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: shot, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: him, DepRel: iobj, UPOS: PRON\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: look, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: combined, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: exasperation, DepRel: obj, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: hint, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: amusement, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"And if I had a nickel for every time you made a joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: And, DepRel: cc, UPOS: CCONJ\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 3\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: nickel, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 10\n",
      "Word: made, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: joke, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: nt, DepRel: advmod, UPOS: PART\n",
      "Word: terrible, DepRel: parataxis, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 19\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: still, DepRel: advmod, UPOS: ADV\n",
      "Word: be, DepRel: aux, UPOS: AUX\n",
      "Word: broke, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 23\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: but, DepRel: cc, UPOS: CCONJ\n",
      "Word: at, DepRel: advmod, UPOS: ADP\n",
      "Word: least, DepRel: fixed, UPOS: ADJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 28\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: have, DepRel: conj, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: better, DepRel: amod, UPOS: ADJ\n",
      "Word: sense, DepRel: obj, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: humor, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark and absurdly dangerous adventure lay ahead.'\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: chuckled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: sound, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: echoing, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: graffiti-, DepRel: advmod, UPOS: ADJ\n",
      "Word: covered, DepRel: amod, UPOS: VERB\n",
      "Word: walls, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: as, DepRel: mark, UPOS: SCONJ\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 13\n",
      "Word: prepared, DepRel: advcl, UPOS: VERB\n",
      "Word: for, DepRel: mark, UPOS: ADP\n",
      "Word: whatever, DepRel: det, UPOS: DET\n",
      "Word: dark, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: absurdly, DepRel: advmod, UPOS: ADV\n",
      "Word: dangerous, DepRel: conj, UPOS: ADJ\n",
      "Word: adventure, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 21\n",
      "Word: lay, DepRel: advcl, UPOS: VERB\n",
      "Word: ahead, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is no greater danger than underestimating the enemy, especially when that enemy lurks in the shadows, biding its time, waiting to strike.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: greater, DepRel: amod, UPOS: ADJ\n",
      "Word: danger, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: than, DepRel: mark, UPOS: SCONJ\n",
      "Word: underestimating, DepRel: acl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: especially, DepRel: advmod, UPOS: ADV\n",
      "Word: when, DepRel: advmod, UPOS: ADV\n",
      "Word: that, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: lurks, DepRel: advcl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: biding, DepRel: advcl, UPOS: VERB\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: time, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: strike, DepRel: xcomp, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There in the darkness, waiting patiently, lies a threat that no amount of bravado can overcome.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: darkness, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: patiently, DepRel: advmod, UPOS: ADV\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 8\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: threat, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: mark, UPOS: SCONJ\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: amount, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: bravado, DepRel: nmod, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: overcome, DepRel: acl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the night, sweeping across the city like a veil, concealing the dangers that await the unprepared.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: night, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sweeping, DepRel: advcl, UPOS: VERB\n",
      "Word: across, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: obl, UPOS: NOUN\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: veil, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: concealing, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: dangers, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 16\n",
      "Word: await, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: unprepared, DepRel: obj, UPOS: ADJ\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 1.0000\n",
      "expletive_inversion: 0.0000\n",
      "emphatic_inversion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "\n",
    "    \n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel == 'root':\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb (root) found at position {i}\")\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        for expl_pos in expletive_positions:\n",
    "            for verb_pos in verb_positions:\n",
    "                if expl_pos < verb_pos:\n",
    "                    print(\" -> Expletive inversion detected\")\n",
    "                    return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and subject_positions:\n",
    "        for emph_pos in emphatic_positions:\n",
    "            for subj_pos in subject_positions:\n",
    "                if emph_pos < subj_pos:\n",
    "                    print(\" -> Emphatic inversion detected\")\n",
    "                    return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, \"\n",
    "    \"desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of \"\n",
    "    \"nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind \"\n",
    "    \"him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"\n",
    "    \"\\\"You know,\\\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent \"\n",
    "    \"street vendor offering mystical hot dogs, \\\"if I had a penny for every time someone tried to sell me \"\n",
    "    \"something magical, Id be richer than the most miserly dragon you can imagine.\\\" Valkyrie shot him a look \"\n",
    "    \"that combined exasperation with a hint of amusement. \\\"And if I had a nickel for every time you made a \"\n",
    "    \"joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\\\" \"\n",
    "    \"Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark \"\n",
    "    \"and absurdly dangerous adventure lay ahead. There is no greater danger than underestimating the enemy, \"\n",
    "    \"especially when that enemy lurks in the shadows, biding its time, waiting to strike. There in the darkness, \"\n",
    "    \"waiting patiently, lies a threat that no amount of bravado can overcome. Here comes the night, sweeping \"\n",
    "    \"across the city like a veil, concealing the dangers that await the unprepared.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 31.0MB/s]                    \n",
      "2024-08-25 13:56:51 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:56:51 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 13:56:52 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 13:56:54 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 13:56:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 33.8MB/s]                    \n",
      "2024-08-25 13:56:54 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:56:55 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 13:56:55 INFO: Using device: cpu\n",
      "2024-08-25 13:56:55 INFO: Loading: tokenize\n",
      "2024-08-25 13:56:55 INFO: Loading: mwt\n",
      "2024-08-25 13:56:55 INFO: Loading: pos\n",
      "2024-08-25 13:56:55 INFO: Loading: lemma\n",
      "2024-08-25 13:56:55 INFO: Loading: constituency\n",
      "2024-08-25 13:56:55 INFO: Loading: depparse\n",
      "2024-08-25 13:56:56 INFO: Loading: sentiment\n",
      "2024-08-25 13:56:56 INFO: Loading: ner\n",
      "2024-08-25 13:56:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 47.3MB/s]                    \n",
      "2024-08-25 13:56:57 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:56:57 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 13:56:58 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 13:56:59 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 13:56:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 35.8MB/s]                    \n",
      "2024-08-25 13:56:59 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:57:00 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 13:57:00 INFO: Using device: cpu\n",
      "2024-08-25 13:57:00 INFO: Loading: tokenize\n",
      "2024-08-25 13:57:00 INFO: Loading: mwt\n",
      "2024-08-25 13:57:00 INFO: Loading: pos\n",
      "2024-08-25 13:57:00 INFO: Loading: lemma\n",
      "2024-08-25 13:57:01 INFO: Loading: constituency\n",
      "2024-08-25 13:57:01 INFO: Loading: depparse\n",
      "2024-08-25 13:57:01 INFO: Loading: sentiment\n",
      "2024-08-25 13:57:01 INFO: Loading: ner\n",
      "2024-08-25 13:57:02 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 57.2MB/s]                    \n",
      "2024-08-25 13:57:02 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:57:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 13:57:04 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 13:57:05 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 13:57:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.9MB/s]                    \n",
      "2024-08-25 13:57:05 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:57:06 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 13:57:06 INFO: Using device: cpu\n",
      "2024-08-25 13:57:06 INFO: Loading: tokenize\n",
      "2024-08-25 13:57:06 INFO: Loading: mwt\n",
      "2024-08-25 13:57:06 INFO: Loading: pos\n",
      "2024-08-25 13:57:06 INFO: Loading: lemma\n",
      "2024-08-25 13:57:06 INFO: Loading: constituency\n",
      "2024-08-25 13:57:06 INFO: Loading: depparse\n",
      "2024-08-25 13:57:06 INFO: Loading: sentiment\n",
      "2024-08-25 13:57:07 INFO: Loading: ner\n",
      "2024-08-25 13:57:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "  \n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel in ('root', 'aux', 'aux:pass'):\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb (root or auxiliary) found at position {i}\")\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos and not expletive_positions and not emphatic_positions:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        print(\" -> Expletive inversion detected\")\n",
    "        return \"expletive_inversion\"\n",
    "\n",
    "    if emphatic_positions:\n",
    "        print(\" -> Emphatic inversion detected\")\n",
    "        return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    \"\"\"\n",
    "    Computes the normalized frequency of different types of sentence inversions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the normalized frequency of each type of inversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 10.9MB/s]                    \n",
      "2024-08-25 21:16:55 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 21:16:55 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 21:16:56 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 21:16:57 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 21:16:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 26.2MB/s]                    \n",
      "2024-08-25 21:16:57 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 21:16:58 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 21:16:58 INFO: Using device: cpu\n",
      "2024-08-25 21:16:58 INFO: Loading: tokenize\n",
      "2024-08-25 21:16:58 INFO: Loading: mwt\n",
      "2024-08-25 21:16:58 INFO: Loading: pos\n",
      "2024-08-25 21:16:58 INFO: Loading: lemma\n",
      "2024-08-25 21:16:58 INFO: Loading: constituency\n",
      "2024-08-25 21:16:58 INFO: Loading: depparse\n",
      "2024-08-25 21:16:59 INFO: Loading: sentiment\n",
      "2024-08-25 21:16:59 INFO: Loading: ner\n",
      "2024-08-25 21:16:59 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 6\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb)\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      " - Verb found at position 8\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb)\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      " - Verb found at position 7\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "    adv_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.upos in ('VERB', 'AUX'):\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb found at position {i}\")\n",
    "        elif word.deprel == 'expl':\n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.text.lower() in ('here', 'there') and word.deprel == 'advmod':\n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "        elif word.upos == 'ADV':\n",
    "            adv_positions.append(i)\n",
    "            print(f\" - Adverb found at position {i}\")\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        if min(expletive_positions) < min(verb_positions):\n",
    "            print(\" -> Expletive inversion detected\")\n",
    "            return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and verb_positions:\n",
    "        if min(emphatic_positions) < min(verb_positions):\n",
    "            print(\" -> Emphatic inversion detected\")\n",
    "            return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    if subject_positions and verb_positions:\n",
    "        if min(subject_positions) > min(verb_positions):\n",
    "            \n",
    "            if adv_positions and min(adv_positions) == 0:\n",
    "                print(\" -> Classic inversion detected (with initial adverb)\")\n",
    "                return \"classic_inversion\"\n",
    "            elif not adv_positions:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 123MB/s]                     \n",
      "2024-08-26 08:06:20 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:06:20 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:06:21 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:06:22 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:06:22 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 52.0MB/s]                    \n",
      "2024-08-26 08:06:22 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:06:23 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:06:23 INFO: Using device: cpu\n",
      "2024-08-26 08:06:23 INFO: Loading: tokenize\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: mwt\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: pos\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: lemma\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: constituency\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Loading: depparse\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Loading: sentiment\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Loading: ner\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 5\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 6\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb or prepositional phrase)\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 6\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      " - Verb found at position 8\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb or prepositional phrase)\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 9\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb or prepositional phrase)\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      " - Verb found at position 7\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 8\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.4286\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "    adv_positions = []\n",
    "    potential_subject_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel in ('obj', 'iobj') and word.upos == 'NOUN':\n",
    "            potential_subject_positions.append(i)\n",
    "            print(f\" - Potential subject found at position {i}\")\n",
    "        elif word.upos in ('VERB', 'AUX'):\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb found at position {i}\")\n",
    "        elif word.deprel == 'expl':\n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.text.lower() in ('here', 'there') and word.deprel == 'advmod':\n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "        elif word.upos == 'ADV':\n",
    "            adv_positions.append(i)\n",
    "            print(f\" - Adverb found at position {i}\")\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        if min(expletive_positions) < min(verb_positions):\n",
    "            print(\" -> Expletive inversion detected\")\n",
    "            return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and verb_positions:\n",
    "        if min(emphatic_positions) < min(verb_positions):\n",
    "            print(\" -> Emphatic inversion detected\")\n",
    "            return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    all_subject_positions = subject_positions + potential_subject_positions\n",
    "    if all_subject_positions and verb_positions:\n",
    "        if min(all_subject_positions) > min(verb_positions):\n",
    "            \n",
    "            if (adv_positions and min(adv_positions) == 0) or (sentence.words[0].upos == 'ADP'):\n",
    "                print(\" -> Classic inversion detected (with initial adverb or prepositional phrase)\")\n",
    "                return \"classic_inversion\"\n",
    "            else:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifiers_per_noun_phrase(doc):\n",
    "\n",
    "    total_modifiers = 0\n",
    "    total_noun_phrases = 0\n",
    "    sentence_modifiers = []  \n",
    "    \n",
    "    \n",
    "    modifier_relations = {'amod', 'nmod', 'acl', 'advmod', 'det', 'appos'}\n",
    "    \n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        noun_phrases = [word for word in sentence.words if word.upos == 'NOUN' or word.upos == 'PROPN']\n",
    "        \n",
    "        if not noun_phrases:\n",
    "            print(f\"DEBUG: No noun phrases found in sentence: '{sentence.text}'\")\n",
    "            continue\n",
    "        \n",
    "        total_noun_phrases += len(noun_phrases)\n",
    "        sentence_mod_count = 0  \n",
    "        \n",
    "        \n",
    "        for noun_phrase in noun_phrases:\n",
    "            modifiers = [word for word in sentence.words if word.deprel in modifier_relations and word.head == noun_phrase.id]\n",
    "            print(f\"DEBUG: Noun phrase '{noun_phrase.text}' has {len(modifiers)} modifiers.\")\n",
    "            total_modifiers += len(modifiers)\n",
    "            sentence_mod_count += len(modifiers)\n",
    "        \n",
    "        sentence_modifiers.append(sentence_mod_count)\n",
    "        print(f\"DEBUG: Sentence '{sentence.text}' has {sentence_mod_count} total modifiers.\")\n",
    "    \n",
    "    \n",
    "    if total_noun_phrases == 0:\n",
    "        print(\"DEBUG: No noun phrases found in the entire document.\")\n",
    "        return 0  \n",
    "\n",
    "    avg_modifiers_per_noun_phrase = total_modifiers / total_noun_phrases\n",
    "    \n",
    "    print(f\"DEBUG: Total noun phrases = {total_noun_phrases}, Total modifiers = {total_modifiers}, Average = {avg_modifiers_per_noun_phrase}\")\n",
    "    print(f\"DEBUG: Modifiers per sentence: {sentence_modifiers}\")\n",
    "    \n",
    "    return avg_modifiers_per_noun_phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: True\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 6, Ratio = 1.0\n",
      "\n",
      "Embedded Clause Ratio: 1.0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Noun phrase 'fox' has 3 modifiers.\n",
      "DEBUG: Noun phrase 'dog' has 2 modifiers.\n",
      "DEBUG: Sentence 'The quick brown fox jumps over the lazy dog.' has 5 total modifiers.\n",
      "DEBUG: Noun phrase 'rabbit' has 2 modifiers.\n",
      "DEBUG: Noun phrase 'speeding' has 0 modifiers.\n",
      "DEBUG: Noun phrase 'car' has 2 modifiers.\n",
      "DEBUG: Sentence 'An incredibly fast and agile rabbit dodged the speeding car.' has 4 total modifiers.\n",
      "DEBUG: Noun phrase 'house' has 3 modifiers.\n",
      "DEBUG: Noun phrase 'figure' has 2 modifiers.\n",
      "DEBUG: Noun phrase 'shadows' has 1 modifiers.\n",
      "DEBUG: Sentence 'In the old abandoned house, the mysterious figure lurked in the shadows.' has 6 total modifiers.\n",
      "DEBUG: Noun phrase 'bouquet' has 3 modifiers.\n",
      "DEBUG: Noun phrase 'flowers' has 1 modifiers.\n",
      "DEBUG: Sentence 'She gave her a beautiful bouquet of fresh flowers.' has 4 total modifiers.\n",
      "DEBUG: Total noun phrases = 10, Total modifiers = 19, Average = 1.9\n",
      "DEBUG: Modifiers per sentence: [5, 4, 6, 4]\n",
      "\n",
      "Average Number of Modifiers per Noun Phrase: 1.9000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_text = (\n",
    "    \"The quick brown fox jumps over the lazy dog. \"  \n",
    "    \"An incredibly fast and agile rabbit dodged the speeding car. \"  \n",
    "    \"In the old abandoned house, the mysterious figure lurked in the shadows.\"  \n",
    "    \"She gave her a beautiful bouquet of fresh flowers.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "avg_modifiers = modifiers_per_noun_phrase(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nAverage Number of Modifiers per Noun Phrase: {avg_modifiers:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: False\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: False\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: False\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 2, Ratio = 0.3333\n",
      "\n",
      "Embedded Clause Ratio: 0.3333\n"
     ]
    }
   ],
   "source": [
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded_clause = any(is_embedded_clause(token) for token in sent.words)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded_clause}\")\n",
    "        if has_embedded_clause:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"  \n",
    "    \"I believe that he is correct. \"  \n",
    "    \"She said that she would come to the party. \"  \n",
    "    \"The cake, which was delicious, was eaten quickly. \"  \n",
    "    \"He runs every morning. \"  \n",
    "    \"The book I bought yesterday is on the table.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 68.0MB/s]                    \n",
      "2024-08-26 08:29:48 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:29:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:29:49 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:29:50 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:29:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 55.8MB/s]                    \n",
      "2024-08-26 08:29:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:29:51 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:29:51 INFO: Using device: cpu\n",
      "2024-08-26 08:29:51 INFO: Loading: tokenize\n",
      "2024-08-26 08:29:51 INFO: Loading: mwt\n",
      "2024-08-26 08:29:51 INFO: Loading: pos\n",
      "2024-08-26 08:29:52 INFO: Loading: lemma\n",
      "2024-08-26 08:29:52 INFO: Loading: constituency\n",
      "2024-08-26 08:29:52 INFO: Loading: depparse\n",
      "2024-08-26 08:29:52 INFO: Loading: sentiment\n",
      "2024-08-26 08:29:52 INFO: Loading: ner\n",
      "2024-08-26 08:29:53 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: False\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: False\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: False\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 2, Ratio = 0.3333\n",
      "\n",
      "Embedded Clause Ratio: 0.3333\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            return True\n",
    "        \n",
    "        if word.deprel == \"acl\" and word.upos == \"VERB\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 55.2MB/s]                    \n",
      "2024-08-26 08:30:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:30:53 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:30:54 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:30:56 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:30:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 46.5MB/s]                    \n",
      "2024-08-26 08:30:56 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:30:57 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:30:57 INFO: Using device: cpu\n",
      "2024-08-26 08:30:57 INFO: Loading: tokenize\n",
      "2024-08-26 08:30:57 INFO: Loading: mwt\n",
      "2024-08-26 08:30:57 INFO: Loading: pos\n",
      "2024-08-26 08:30:57 INFO: Loading: lemma\n",
      "2024-08-26 08:30:57 INFO: Loading: constituency\n",
      "2024-08-26 08:30:57 INFO: Loading: depparse\n",
      "2024-08-26 08:30:57 INFO: Loading: sentiment\n",
      "2024-08-26 08:30:58 INFO: Loading: ner\n",
      "2024-08-26 08:30:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedded clause detected: sitting (acl:relcl)\n",
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: correct (ccomp)\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: come (ccomp)\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: delicious (acl:relcl)\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: bought (acl:relcl)\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 5, Ratio = 0.8333\n",
      "\n",
      "Embedded Clause Ratio: 0.8333\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"acl:relcl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            print(f\"DEBUG: Embedded clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "        \n",
    "        if word.deprel == \"acl\" and word.upos == \"VERB\":\n",
    "            print(f\"DEBUG: Reduced relative clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 77.3MB/s]                    \n",
      "2024-08-26 08:35:48 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:35:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:35:49 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:35:50 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:35:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.2MB/s]                    \n",
      "2024-08-26 08:35:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:35:51 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:35:51 INFO: Using device: cpu\n",
      "2024-08-26 08:35:51 INFO: Loading: tokenize\n",
      "2024-08-26 08:35:51 INFO: Loading: mwt\n",
      "2024-08-26 08:35:51 INFO: Loading: pos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Download and load the English model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m stanza\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_embedded_clause\u001b[39m(token):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/pipeline/core.py:308\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(curr_processor_config)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name] \u001b[38;5;241m=\u001b[39m \u001b[43mNAME_TO_PROCESSOR_CLASS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_processor_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessorRequirementsException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     pipeline_reqs_exceptions\u001b[38;5;241m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/pipeline/processor.py:193\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_variant\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# build the final config for the processor\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py:32\u001b[0m, in \u001b[0;36mPOSProcessor._set_up_model\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharlm_forward_file\u001b[39m\u001b[38;5;124m'\u001b[39m: config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_charlm_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharlm_backward_file\u001b[39m\u001b[38;5;124m'\u001b[39m: config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward_charlm_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)}\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# set up trainer\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:34\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, vocab, pretrain, model_file, device, foundation_cache)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pretrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, foundation_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# load everything from file\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# build model from scratch\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:178\u001b[0m, in \u001b[0;36mTrainer.load\u001b[0;34m(self, filename, pretrain, args, foundation_cache)\u001b[0m\n\u001b[1;32m    176\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m has a finetuned transformer.  Not using transformer cache to make sure the finetuned version of the transformer isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt accidentally used elsewhere\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m    177\u001b[0m     foundation_cache \u001b[38;5;241m=\u001b[39m NoTransformerFoundationCache(foundation_cache)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_hid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshare_hid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_bert_saved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_bert_saved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/model.py:114\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, args, vocab, emb_matrix, share_hid, foundation_cache, bert_model, bert_tokenizer, force_bert_saved, peft_name)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf\u001b[38;5;241m.\u001b[39mappend(clf_constructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposite_deep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], l))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf \u001b[38;5;241m=\u001b[39m \u001b[43mclf_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeep_biaff_hidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m share_hid:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/model.py:107\u001b[0m, in \u001b[0;36mTagger.__init__.<locals>.<lambda>\u001b[0;34m(insize, outsize)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_hid \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpos\u001b[39m\u001b[38;5;124m'\u001b[39m], CompositeVocab) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposite_deep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mufeats_hid \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposite_deep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m     clf_constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m insize, outsize: \u001b[43mBiaffineScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag_emb_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpos\u001b[39m\u001b[38;5;124m'\u001b[39m], CompositeVocab):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/biaffine.py:38\u001b[0m, in \u001b[0;36mBiaffineScorer.__init__\u001b[0;34m(self, input1_size, input2_size, output_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1_size, input2_size, output_size):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_bilin \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBilinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_bilin\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_bilin\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/modules/linear.py:192\u001b[0m, in \u001b[0;36mBilinear.__init__\u001b[0;34m(self, in1_features, in2_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/modules/linear.py:196\u001b[0m, in \u001b[0;36mBilinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 196\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         init\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;241m-\u001b[39mbound, bound)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/init.py:149\u001b[0m, in \u001b[0;36muniform_\u001b[0;34m(tensor, a, b, generator)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    147\u001b[0m         uniform_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, a\u001b[38;5;241m=\u001b[39ma, b\u001b[38;5;241m=\u001b[39mb, generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m    148\u001b[0m     )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/init.py:16\u001b[0m, in \u001b[0;36m_no_grad_uniform_\u001b[0;34m(tensor, a, b, generator)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_uniform_\u001b[39m(tensor, a, b, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"acl:relcl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            print(f\"DEBUG: Embedded clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "        \n",
    "        if word.deprel in {\"acl\", \"acl:relcl\"} and word.upos == \"VERB\":\n",
    "            print(f\"DEBUG: Reduced relative clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table. \"\n",
    "    \"When it rains, the streets get wet. \"\n",
    "    \"The man wearing a red hat walked by. \"\n",
    "    \"I wonder whether she'll arrive on time. \"\n",
    "    \"They decided to go to the beach. \"\n",
    "    \"The car, although old, still runs well. \"\n",
    "    \"She sang while she was cooking dinner. \"\n",
    "    \"The flowers in the garden are beautiful. \"\n",
    "    \"He asked where I had been. \"\n",
    "    \"The movie we watched last night was exciting. \"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedded clause detected: sitting (acl:relcl)\n",
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: correct (ccomp)\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: come (ccomp)\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: delicious (acl:relcl)\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: bought (acl:relcl)\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: rains (advcl)\n",
      "DEBUG: Sentence 'When it rains, the streets get wet.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: wearing (acl)\n",
      "DEBUG: Sentence 'The man wearing a red hat walked by.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: arrive (ccomp)\n",
      "DEBUG: Sentence 'I wonder whether she'll arrive on time.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: go (xcomp)\n",
      "DEBUG: Sentence 'They decided to go to the beach.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: old (advcl)\n",
      "DEBUG: Sentence 'The car, although old, still runs well.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: cooking (advcl)\n",
      "DEBUG: Sentence 'She sang while she was cooking dinner.' has embedded clause: True\n",
      "DEBUG: Sentence 'The flowers in the garden are beautiful.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: where (ccomp)\n",
      "DEBUG: Sentence 'He asked where I had been.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: watched (acl:relcl)\n",
      "DEBUG: Sentence 'The movie we watched last night was exciting.' has embedded clause: True\n",
      "DEBUG: Total sentences = 15, Sentences with embedded clauses = 13, Ratio = 0.8667\n",
      "\n",
      "Embedded Clause Ratio: 0.8667\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: When it rains, the streets get wet.\n",
      "Word: When, POS: ADV, Dependency: advmod\n",
      "Word: it, POS: PRON, Dependency: nsubj\n",
      "Word: rains, POS: VERB, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: streets, POS: NOUN, Dependency: nsubj\n",
      "Word: get, POS: VERB, Dependency: root\n",
      "Word: wet, POS: ADJ, Dependency: xcomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The man wearing a red hat walked by.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: man, POS: NOUN, Dependency: nsubj\n",
      "Word: wearing, POS: VERB, Dependency: acl\n",
      "Word: a, POS: DET, Dependency: det\n",
      "Word: red, POS: ADJ, Dependency: amod\n",
      "Word: hat, POS: NOUN, Dependency: obj\n",
      "Word: walked, POS: VERB, Dependency: root\n",
      "Word: by, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I wonder whether she'll arrive on time.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: wonder, POS: VERB, Dependency: root\n",
      "Word: whether, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: 'll, POS: AUX, Dependency: aux\n",
      "Word: arrive, POS: VERB, Dependency: ccomp\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: time, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: They decided to go to the beach.\n",
      "Word: They, POS: PRON, Dependency: nsubj\n",
      "Word: decided, POS: VERB, Dependency: root\n",
      "Word: to, POS: PART, Dependency: mark\n",
      "Word: go, POS: VERB, Dependency: xcomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: beach, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The car, although old, still runs well.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: car, POS: NOUN, Dependency: nsubj\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: although, POS: SCONJ, Dependency: mark\n",
      "Word: old, POS: ADJ, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: still, POS: ADV, Dependency: advmod\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: well, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She sang while she was cooking dinner.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: sang, POS: VERB, Dependency: root\n",
      "Word: while, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: cooking, POS: VERB, Dependency: advcl\n",
      "Word: dinner, POS: NOUN, Dependency: obj\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The flowers in the garden are beautiful.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: flowers, POS: NOUN, Dependency: nsubj\n",
      "Word: in, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: garden, POS: NOUN, Dependency: nmod\n",
      "Word: are, POS: AUX, Dependency: cop\n",
      "Word: beautiful, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He asked where I had been.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: asked, POS: VERB, Dependency: root\n",
      "Word: where, POS: ADV, Dependency: ccomp\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: had, POS: AUX, Dependency: aux\n",
      "Word: been, POS: AUX, Dependency: cop\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The movie we watched last night was exciting.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: movie, POS: NOUN, Dependency: nsubj\n",
      "Word: we, POS: PRON, Dependency: nsubj\n",
      "Word: watched, POS: VERB, Dependency: acl:relcl\n",
      "Word: last, POS: ADJ, Dependency: amod\n",
      "Word: night, POS: NOUN, Dependency: obl:tmod\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: exciting, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table. \"\n",
    "    \"When it rains, the streets get wet. \"\n",
    "    \"The man wearing a red hat walked by. \"\n",
    "    \"I wonder whether she'll arrive on time. \"\n",
    "    \"They decided to go to the beach. \"\n",
    "    \"The car, although old, still runs well. \"\n",
    "    \"She sang while she was cooking dinner. \"\n",
    "    \"The flowers in the garden are beautiful. \"\n",
    "    \"He asked where I had been. \"\n",
    "    \"The movie we watched last night was exciting. \"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 40.6MB/s]                    \n",
      "2024-08-26 08:36:00 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:36:00 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:36:01 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:36:02 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:36:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.7MB/s]                    \n",
      "2024-08-26 08:36:02 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:36:03 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:36:03 INFO: Using device: cpu\n",
      "2024-08-26 08:36:03 INFO: Loading: tokenize\n",
      "2024-08-26 08:36:03 INFO: Loading: mwt\n",
      "2024-08-26 08:36:03 INFO: Loading: pos\n",
      "2024-08-26 08:36:04 INFO: Loading: lemma\n",
      "2024-08-26 08:36:04 INFO: Loading: constituency\n",
      "2024-08-26 08:36:04 INFO: Loading: depparse\n",
      "2024-08-26 08:36:04 INFO: Loading: sentiment\n",
      "2024-08-26 08:36:04 INFO: Loading: ner\n",
      "2024-08-26 08:36:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedded clause detected: sitting (acl:relcl)\n",
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: correct (ccomp)\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: come (ccomp)\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: delicious (acl:relcl)\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: bought (acl:relcl)\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: rains (advcl)\n",
      "DEBUG: Sentence 'When it rains, the streets get wet.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: wearing (acl)\n",
      "DEBUG: Sentence 'The man wearing a red hat walked by.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: arrive (ccomp)\n",
      "DEBUG: Sentence 'I wonder whether she'll arrive on time.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: go (xcomp)\n",
      "DEBUG: Sentence 'They decided to go to the beach.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: old (advcl)\n",
      "DEBUG: Sentence 'The car, although old, still runs well.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: cooking (advcl)\n",
      "DEBUG: Sentence 'She sang while she was cooking dinner.' has embedded clause: True\n",
      "DEBUG: Sentence 'The flowers in the garden are beautiful.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: where (ccomp)\n",
      "DEBUG: Sentence 'He asked where I had been.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: watched (acl:relcl)\n",
      "DEBUG: Sentence 'The movie we watched last night was exciting.' has embedded clause: True\n",
      "DEBUG: Total sentences = 15, Sentences with embedded clauses = 13, Ratio = 0.8667\n",
      "\n",
      "Embedded Clause Ratio: 0.8667\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: When it rains, the streets get wet.\n",
      "Word: When, POS: ADV, Dependency: advmod\n",
      "Word: it, POS: PRON, Dependency: nsubj\n",
      "Word: rains, POS: VERB, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: streets, POS: NOUN, Dependency: nsubj\n",
      "Word: get, POS: VERB, Dependency: root\n",
      "Word: wet, POS: ADJ, Dependency: xcomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The man wearing a red hat walked by.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: man, POS: NOUN, Dependency: nsubj\n",
      "Word: wearing, POS: VERB, Dependency: acl\n",
      "Word: a, POS: DET, Dependency: det\n",
      "Word: red, POS: ADJ, Dependency: amod\n",
      "Word: hat, POS: NOUN, Dependency: obj\n",
      "Word: walked, POS: VERB, Dependency: root\n",
      "Word: by, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I wonder whether she'll arrive on time.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: wonder, POS: VERB, Dependency: root\n",
      "Word: whether, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: 'll, POS: AUX, Dependency: aux\n",
      "Word: arrive, POS: VERB, Dependency: ccomp\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: time, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: They decided to go to the beach.\n",
      "Word: They, POS: PRON, Dependency: nsubj\n",
      "Word: decided, POS: VERB, Dependency: root\n",
      "Word: to, POS: PART, Dependency: mark\n",
      "Word: go, POS: VERB, Dependency: xcomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: beach, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The car, although old, still runs well.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: car, POS: NOUN, Dependency: nsubj\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: although, POS: SCONJ, Dependency: mark\n",
      "Word: old, POS: ADJ, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: still, POS: ADV, Dependency: advmod\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: well, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She sang while she was cooking dinner.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: sang, POS: VERB, Dependency: root\n",
      "Word: while, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: cooking, POS: VERB, Dependency: advcl\n",
      "Word: dinner, POS: NOUN, Dependency: obj\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The flowers in the garden are beautiful.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: flowers, POS: NOUN, Dependency: nsubj\n",
      "Word: in, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: garden, POS: NOUN, Dependency: nmod\n",
      "Word: are, POS: AUX, Dependency: cop\n",
      "Word: beautiful, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He asked where I had been.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: asked, POS: VERB, Dependency: root\n",
      "Word: where, POS: ADV, Dependency: ccomp\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: had, POS: AUX, Dependency: aux\n",
      "Word: been, POS: AUX, Dependency: cop\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The movie we watched last night was exciting.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: movie, POS: NOUN, Dependency: nsubj\n",
      "Word: we, POS: PRON, Dependency: nsubj\n",
      "Word: watched, POS: VERB, Dependency: acl:relcl\n",
      "Word: last, POS: ADJ, Dependency: amod\n",
      "Word: night, POS: NOUN, Dependency: obl:tmod\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: exciting, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"acl:relcl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            print(f\"DEBUG: Embedded clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "        \n",
    "        if word.deprel in {\"acl\", \"acl:relcl\"} and word.upos == \"VERB\":\n",
    "            print(f\"DEBUG: Reduced relative clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table. \"\n",
    "    \"When it rains, the streets get wet. \"\n",
    "    \"The man wearing a red hat walked by. \"\n",
    "    \"I wonder whether she'll arrive on time. \"\n",
    "    \"They decided to go to the beach. \"\n",
    "    \"The car, although old, still runs well. \"\n",
    "    \"She sang while she was cooking dinner. \"\n",
    "    \"The flowers in the garden are beautiful. \"\n",
    "    \"He asked where I had been. \"\n",
    "    \"The movie we watched last night was exciting. \"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 275MB/s]                     \n",
      "2024-08-26 08:55:58 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:55:58 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:55:59 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:56:01 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:56:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 47.2MB/s]                    \n",
      "2024-08-26 08:56:01 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:56:02 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:56:02 INFO: Using device: cpu\n",
      "2024-08-26 08:56:02 INFO: Loading: tokenize\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:02 INFO: Loading: mwt\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:02 INFO: Loading: pos\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:02 INFO: Loading: lemma\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:02 INFO: Loading: constituency\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:02 INFO: Loading: depparse\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:03 INFO: Loading: sentiment\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:03 INFO: Loading: ner\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:56:03 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_passive_voice(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is in passive voice based on dependency parsing.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentence is in passive voice, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    has_auxpass = any(word.deprel == 'auxpass' for word in sentence.words)\n",
    "    has_vbn = any(word.upos == 'VERB' and word.feats and 'VerbForm=Part' in word.feats for word in sentence.words)\n",
    "\n",
    "    return has_auxpass and has_vbn\n",
    "\n",
    "def ratio_of_passive_voice(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of passive voice constructions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object containing parsed text.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of passive voice constructions in the text.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "    passive_count = sum(1 for sentence in sentences if is_passive_voice(sentence))\n",
    "    \n",
    "    \n",
    "    ratio = passive_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "sample_text = \"The report was prepared by the committee. A decision was made to delay the project. The cake was eaten by the children. It was the manager who decided to cancel the meeting. What John did was bake a cake for the party. It was only after midnight that they arrived. The dog chased the cat. The cat climbed the tree. The tree was tall. The car stopped at the traffic light. The light turned green, and the car drove away. The teacher explained the lesson. The lesson was difficult, but the students understood it.\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "print(ratio_of_passive_voice(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 110MB/s]                     \n",
      "2024-08-26 08:57:24 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:57:24 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:57:25 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:57:26 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:57:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 58.1MB/s]                    \n",
      "2024-08-26 08:57:26 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:57:27 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:57:27 INFO: Using device: cpu\n",
      "2024-08-26 08:57:27 INFO: Loading: tokenize\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:57:27 INFO: Loading: mwt\n",
      "2024-08-26 08:57:27 INFO: Loading: pos\n",
      "2024-08-26 08:57:27 INFO: Loading: lemma\n",
      "2024-08-26 08:57:27 INFO: Loading: constituency\n",
      "2024-08-26 08:57:28 INFO: Loading: depparse\n",
      "2024-08-26 08:57:28 INFO: Loading: sentiment\n",
      "2024-08-26 08:57:28 INFO: Loading: ner\n",
      "2024-08-26 08:57:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_passive_voice(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is in passive voice based on dependency parsing.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentence is in passive voice, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    \n",
    "    \n",
    "    has_vbn = any(word.upos == 'VERB' and word.feats and 'VerbForm=Part' in word.feats for word in sentence.words)\n",
    "    \n",
    "    \n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "\n",
    "    return (has_aux_pass or has_nsubj_pass) and has_vbn\n",
    "\n",
    "def ratio_of_passive_voice(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of passive voice constructions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object containing parsed text.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of passive voice constructions in the text.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "    passive_count = sum(1 for sentence in sentences if is_passive_voice(sentence))\n",
    "    \n",
    "    \n",
    "    ratio = passive_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "sample_text = \"The report was prepared by the committee. A decision was made to delay the project. The cake was eaten by the children. It was the manager who decided to cancel the meeting. What John did was bake a cake for the party. It was only after midnight that they arrived. The dog chased the cat. The cat climbed the tree. The tree was tall. The car stopped at the traffic light. The light turned green, and the car drove away. The teacher explained the lesson. The lesson was difficult, but the students understood it.\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "print(ratio_of_passive_voice(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 50.4MB/s]                    \n",
      "2024-08-26 08:58:16 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:58:16 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:58:17 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:58:19 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:58:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 51.4MB/s]                    \n",
      "2024-08-26 08:58:19 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:58:20 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:58:20 INFO: Using device: cpu\n",
      "2024-08-26 08:58:20 INFO: Loading: tokenize\n",
      "2024-08-26 08:58:20 INFO: Loading: mwt\n",
      "2024-08-26 08:58:20 INFO: Loading: pos\n",
      "2024-08-26 08:58:20 INFO: Loading: lemma\n",
      "2024-08-26 08:58:20 INFO: Loading: constituency\n",
      "2024-08-26 08:58:20 INFO: Loading: depparse\n",
      "2024-08-26 08:58:20 INFO: Loading: sentiment\n",
      "2024-08-26 08:58:20 INFO: Loading: ner\n",
      "2024-08-26 08:58:21 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The cat chased the mouse.' - Passive: False\n",
      "'The mouse was chased by the cat.' - Passive: True\n",
      "'The report was written.' - Passive: True\n",
      "'The cake is being baked.' - Passive: True\n",
      "'The document had been signed before noon.' - Passive: True\n",
      "'Mistakes were made.' - Passive: True\n",
      "'It is believed that the project will succeed.' - Passive: True\n",
      "'The house is painted white.' - Passive: False\n",
      "'The meeting was postponed due to bad weather.' - Passive: True\n",
      "'John was elected president.' - Passive: True\n",
      "'The book will be published next month.' - Passive: True\n",
      "'The dog is walking.' - Passive: False\n",
      "'The building was destroyed by the earthquake.' - Passive: True\n",
      "'It was decided to cancel the event.' - Passive: True\n",
      "'The results are expected soon.' - Passive: True\n",
      "'The paper has been submitted.' - Passive: True\n",
      "'The city was founded in 1850.' - Passive: True\n",
      "'The problem is solved easily.' - Passive: True\n",
      "'The concert was attended by thousands.' - Passive: True\n",
      "'The decision is made by the committee.' - Passive: True\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_passive_voice(sentence):\n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    has_vbn = any(word.upos == 'VERB' and word.feats and 'VerbForm=Part' in word.feats for word in sentence.words)\n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "    return (has_aux_pass or has_nsubj_pass) and has_vbn\n",
    "\n",
    "def analyze_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    is_passive = is_passive_voice(doc.sentences[0])\n",
    "    return f\"'{text}' - Passive: {is_passive}\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The cat chased the mouse.\",  \n",
    "    \"The mouse was chased by the cat.\",  \n",
    "    \"The report was written.\",  \n",
    "    \"The cake is being baked.\",  \n",
    "    \"The document had been signed before noon.\",  \n",
    "    \"Mistakes were made.\",  \n",
    "    \"It is believed that the project will succeed.\",  \n",
    "    \"The house is painted white.\",  \n",
    "    \"The meeting was postponed due to bad weather.\",  \n",
    "    \"John was elected president.\",  \n",
    "    \"The book will be published next month.\",  \n",
    "    \"The dog is walking.\",  \n",
    "    \"The building was destroyed by the earthquake.\",  \n",
    "    \"It was decided to cancel the event.\",  \n",
    "    \"The results are expected soon.\",  \n",
    "    \"The paper has been submitted.\",  \n",
    "    \"The city was founded in 1850.\",  \n",
    "    \"The problem is solved easily.\",  \n",
    "    \"The concert was attended by thousands.\",  \n",
    "    \"The decision is made by the committee.\",  \n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(analyze_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 64.0MB/s]                    \n",
      "2024-08-26 09:01:33 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:01:33 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:01:34 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:01:36 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:01:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 37.7MB/s]                    \n",
      "2024-08-26 09:01:36 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:01:36 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:01:36 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:01:36 INFO: Using device: cpu\n",
      "2024-08-26 09:01:36 INFO: Loading: tokenize\n",
      "2024-08-26 09:01:36 INFO: Loading: mwt\n",
      "2024-08-26 09:01:36 INFO: Loading: pos\n",
      "2024-08-26 09:01:36 INFO: Loading: lemma\n",
      "2024-08-26 09:01:36 INFO: Loading: depparse\n",
      "2024-08-26 09:01:37 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The cat chased the mouse.' - Passive: False\n",
      "'The mouse was chased by the cat.' - Passive: True\n",
      "'The report was written.' - Passive: True\n",
      "'The cake is being baked.' - Passive: True\n",
      "'The document had been signed before noon.' - Passive: True\n",
      "'Mistakes were made.' - Passive: True\n",
      "'It is believed that the project will succeed.' - Passive: True\n",
      "'The house is painted white.' - Passive: False\n",
      "'The meeting was postponed due to bad weather.' - Passive: True\n",
      "'John was elected president.' - Passive: True\n",
      "'The book will be published next month.' - Passive: True\n",
      "'The dog is walking.' - Passive: False\n",
      "'The building was destroyed by the earthquake.' - Passive: True\n",
      "'It was decided to cancel the event.' - Passive: True\n",
      "'The results are expected soon.' - Passive: True\n",
      "'The paper has been submitted.' - Passive: True\n",
      "'The city was founded in 1850.' - Passive: True\n",
      "'The problem is solved easily.' - Passive: True\n",
      "'The concert was attended by thousands.' - Passive: True\n",
      "'The decision is made by the committee.' - Passive: True\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def is_passive_voice(sentence):\n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "    \n",
    "    has_vbn = any(\n",
    "        word.upos == 'VERB' and \n",
    "        word.feats and 'VerbForm=Part' in word.feats and\n",
    "        not (word.deprel == 'amod' or  \n",
    "             (word.deprel == 'root' and not has_aux_pass and not has_nsubj_pass))\n",
    "        for word in sentence.words\n",
    "    )\n",
    "    \n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    is_copular = any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words)\n",
    "    \n",
    "    \n",
    "    has_adverb = any(\n",
    "        word.deprel == 'advmod' and \n",
    "        isinstance(word.head, int) and \n",
    "        0 <= word.head < len(sentence.words) and\n",
    "        sentence.words[word.head].upos == 'VERB' and \n",
    "        sentence.words[word.head].feats and \n",
    "        'VerbForm=Part' in sentence.words[word.head].feats \n",
    "        for word in sentence.words\n",
    "    )\n",
    "    \n",
    "    return ((has_aux_pass or has_nsubj_pass) and has_vbn and not is_copular) or (is_copular and has_vbn and has_adverb)\n",
    "\n",
    "def analyze_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    is_passive = is_passive_voice(doc.sentences[0])\n",
    "    return f\"'{text}' - Passive: {is_passive}\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The mouse was chased by the cat.\",\n",
    "    \"The report was written.\",\n",
    "    \"The cake is being baked.\",\n",
    "    \"The document had been signed before noon.\",\n",
    "    \"Mistakes were made.\",\n",
    "    \"It is believed that the project will succeed.\",\n",
    "    \"The house is painted white.\",\n",
    "    \"The meeting was postponed due to bad weather.\",\n",
    "    \"John was elected president.\",\n",
    "    \"The book will be published next month.\",\n",
    "    \"The dog is walking.\",\n",
    "    \"The building was destroyed by the earthquake.\",\n",
    "    \"It was decided to cancel the event.\",\n",
    "    \"The results are expected soon.\",\n",
    "    \"The paper has been submitted.\",\n",
    "    \"The city was founded in 1850.\",\n",
    "    \"The problem is solved easily.\",\n",
    "    \"The concert was attended by thousands.\",\n",
    "    \"The decision is made by the committee.\",\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(analyze_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 49.3MB/s]                    \n",
      "2024-08-26 09:11:59 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:11:59 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:12:00 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:12:02 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:12:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 59.5MB/s]                    \n",
      "2024-08-26 09:12:02 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:12:02 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:12:02 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:12:02 INFO: Using device: cpu\n",
      "2024-08-26 09:12:02 INFO: Loading: tokenize\n",
      "2024-08-26 09:12:02 INFO: Loading: mwt\n",
      "2024-08-26 09:12:02 INFO: Loading: pos\n",
      "2024-08-26 09:12:02 INFO: Loading: lemma\n",
      "2024-08-26 09:12:03 INFO: Loading: depparse\n",
      "2024-08-26 09:12:03 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The cat chased the mouse.' - Passive: False\n",
      "'The mouse was chased by the cat.' - Passive: True\n",
      "'The report was written.' - Passive: True\n",
      "'The cake is being baked.' - Passive: True\n",
      "'The document had been signed before noon.' - Passive: True\n",
      "'Mistakes were made.' - Passive: True\n",
      "'It is believed that the project will succeed.' - Passive: True\n",
      "'The house is painted white.' - Passive: False\n",
      "'The meeting was postponed due to bad weather.' - Passive: True\n",
      "'John was elected president.' - Passive: True\n",
      "'The book will be published next month.' - Passive: True\n",
      "'The dog is walking.' - Passive: False\n",
      "'The building was destroyed by the earthquake.' - Passive: True\n",
      "'It was decided to cancel the event.' - Passive: True\n",
      "'The results are expected soon.' - Passive: True\n",
      "'The paper has been submitted.' - Passive: True\n",
      "'The city was founded in 1850.' - Passive: True\n",
      "'The problem is solved easily.' - Passive: True\n",
      "'The concert was attended by thousands.' - Passive: True\n",
      "'The decision is made by the committee.' - Passive: True\n",
      "'He got promoted last week.' - Passive: True\n",
      "'The project has been completed on time.' - Passive: True\n",
      "'The floor got waxed yesterday.' - Passive: True\n",
      "'She is gone.' - Passive: False\n",
      "'The work is done.' - Passive: True\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def is_passive_voice(sentence):\n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "    \n",
    "    has_vbn = any(\n",
    "        word.upos == 'VERB' and \n",
    "        word.feats and 'VerbForm=Part' in word.feats and\n",
    "        not (word.deprel == 'amod' or  \n",
    "             (word.deprel == 'root' and not has_aux_pass and not has_nsubj_pass))\n",
    "        for word in sentence.words\n",
    "    )\n",
    "    \n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    is_copular = any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words)\n",
    "    \n",
    "    \n",
    "    state_verbs = {'solved', 'finished', 'completed', 'done', 'decided', 'resolved', 'settled'}\n",
    "    action_verbs = {'chased', 'written', 'baked', 'signed', 'made', 'believed', 'painted', 'postponed', \n",
    "                    'elected', 'published', 'destroyed', 'expected', 'submitted', 'founded', 'attended'}\n",
    "    \n",
    "    main_verb = next((word for word in sentence.words if word.upos == 'VERB' and word.deprel == 'root'), None)\n",
    "    \n",
    "    if main_verb:\n",
    "        if main_verb.lemma in state_verbs:\n",
    "            \n",
    "            return has_aux_pass or has_nsubj_pass\n",
    "        elif main_verb.lemma in action_verbs:\n",
    "            \n",
    "            return True if (has_aux_pass or has_nsubj_pass) and has_vbn else False\n",
    "    \n",
    "    \n",
    "    return ((has_aux_pass or has_nsubj_pass) and has_vbn and not is_copular)\n",
    "\n",
    "def analyze_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    is_passive = is_passive_voice(doc.sentences[0])\n",
    "    return f\"'{text}' - Passive: {is_passive}\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The mouse was chased by the cat.\",\n",
    "    \"The report was written.\",\n",
    "    \"The cake is being baked.\",\n",
    "    \"The document had been signed before noon.\",\n",
    "    \"Mistakes were made.\",\n",
    "    \"It is believed that the project will succeed.\",\n",
    "    \"The house is painted white.\",\n",
    "    \"The meeting was postponed due to bad weather.\",\n",
    "    \"John was elected president.\",\n",
    "    \"The book will be published next month.\",\n",
    "    \"The dog is walking.\",\n",
    "    \"The building was destroyed by the earthquake.\",\n",
    "    \"It was decided to cancel the event.\",\n",
    "    \"The results are expected soon.\",\n",
    "    \"The paper has been submitted.\",\n",
    "    \"The city was founded in 1850.\",\n",
    "    \"The problem is solved easily.\",\n",
    "    \"The concert was attended by thousands.\",\n",
    "    \"The decision is made by the committee.\",\n",
    "    \"He got promoted last week.\",  \n",
    "    \"The project has been completed on time.\",  \n",
    "    \"The floor got waxed yesterday.\",  \n",
    "    \"She is gone.\",  \n",
    "    \"The work is done.\",  \n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(analyze_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.strings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/gensim/parsing/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     preprocess_documents,\n\u001b[1;32m      6\u001b[0m     preprocess_string,\n\u001b[1;32m      7\u001b[0m     read_file,\n\u001b[1;32m      8\u001b[0m     read_files,\n\u001b[1;32m      9\u001b[0m     remove_stopwords,\n\u001b[1;32m     10\u001b[0m     split_alphanum,\n\u001b[1;32m     11\u001b[0m     stem_text,\n\u001b[1;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[1;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[1;32m     14\u001b[0m     strip_numeric,\n\u001b[1;32m     15\u001b[0m     strip_punctuation,\n\u001b[1;32m     16\u001b[0m     strip_short,\n\u001b[1;32m     17\u001b[0m     strip_tags,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/gensim/parsing/preprocessing.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     30\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manyway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthrough\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewhere\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     59\u001b[0m ])\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/gensim/utils.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/scipy/sparse/__init__.py:294\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/scipy/sparse/_base.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[1;32m      8\u001b[0m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[1;32m      9\u001b[0m                        matrix, validateaxis,)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/scipy/_lib/_util.py:18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Optional,\n\u001b[1;32m     12\u001b[0m     Union,\n\u001b[1;32m     13\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     14\u001b[0m     TypeVar,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[1;32m     21\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[1;32m     22\u001b[0m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/scipy/_lib/_array_api.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     is_array_api_obj,\n\u001b[1;32m     19\u001b[0m     size,\n\u001b[1;32m     20\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/numpy/__init__.py:379\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mac_os_check\u001b[39m():\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;124;03m    Quick Sanity check for Mac OS look for accelerate build bugs.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m    Testing numpy polyfit calls init_dgelsd(LAPACK)\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m         c \u001b[38;5;241m=\u001b[39m array([\u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m1.\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('path/to/your/word2vec/model.bin', binary=True)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_synonyms(word, pos):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return list(synonyms)\n",
    "\n",
    "def semantic_similarity(word, category):\n",
    "    if word not in word_vectors.key_to_index:\n",
    "        return 0\n",
    "    \n",
    "    category_vectors = [word_vectors[w] for w in category if w in word_vectors.key_to_index]\n",
    "    if not category_vectors:\n",
    "        return 0\n",
    "    \n",
    "    word_vector = word_vectors[word]\n",
    "    similarities = [np.dot(word_vector, cat_vec) / (np.linalg.norm(word_vector) * np.linalg.norm(cat_vec)) \n",
    "                    for cat_vec in category_vectors]\n",
    "    return max(similarities)\n",
    "\n",
    "def is_passive_voice(sentence, state_verbs, action_verbs):\n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "    \n",
    "    has_vbn = any(\n",
    "        word.upos == 'VERB' and \n",
    "        word.feats and 'VerbForm=Part' in word.feats and\n",
    "        not (word.deprel == 'amod' or  \n",
    "             (word.deprel == 'root' and not has_aux_pass and not has_nsubj_pass))\n",
    "        for word in sentence.words\n",
    "    )\n",
    "    \n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    is_copular = any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words)\n",
    "    \n",
    "    main_verb = next((word for word in sentence.words if word.upos == 'VERB' and word.deprel == 'root'), None)\n",
    "    \n",
    "    if main_verb:\n",
    "        state_similarity = semantic_similarity(main_verb.lemma, state_verbs)\n",
    "        action_similarity = semantic_similarity(main_verb.lemma, action_verbs)\n",
    "        \n",
    "        if state_similarity > action_similarity:\n",
    "            \n",
    "            return has_aux_pass or has_nsubj_pass\n",
    "        else:\n",
    "            \n",
    "            return True if (has_aux_pass or has_nsubj_pass) and has_vbn else False\n",
    "    \n",
    "    \n",
    "    return ((has_aux_pass or has_nsubj_pass) and has_vbn and not is_copular)\n",
    "\n",
    "def analyze_sentence(text, state_verbs, action_verbs):\n",
    "    doc = nlp(text)\n",
    "    is_passive = is_passive_voice(doc.sentences[0], state_verbs, action_verbs)\n",
    "    return f\"'{text}' - Passive: {is_passive}\"\n",
    "\n",
    "\n",
    "state_verbs = ['solve', 'finish', 'complete', 'decide', 'resolve', 'settle']\n",
    "action_verbs = ['chase', 'write', 'bake', 'sign', 'make', 'believe', 'paint', 'postpone', \n",
    "                'elect', 'publish', 'destroy', 'expect', 'submit', 'found', 'attend']\n",
    "\n",
    "\n",
    "state_verbs_expanded = []\n",
    "action_verbs_expanded = []\n",
    "\n",
    "for verb in state_verbs:\n",
    "    pos = get_wordnet_pos(nltk.pos_tag([verb])[0][1])\n",
    "    state_verbs_expanded.extend(get_synonyms(verb, pos))\n",
    "\n",
    "for verb in action_verbs:\n",
    "    pos = get_wordnet_pos(nltk.pos_tag([verb])[0][1])\n",
    "    action_verbs_expanded.extend(get_synonyms(verb, pos))\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The mouse was chased by the cat.\",\n",
    "    \"The report was written.\",\n",
    "    \"The cake is being baked.\",\n",
    "    \"The document had been signed before noon.\",\n",
    "    \"Mistakes were made.\",\n",
    "    \"It is believed that the project will succeed.\",\n",
    "    \"The house is painted white.\",\n",
    "    \"The meeting was postponed due to bad weather.\",\n",
    "    \"John was elected president.\",\n",
    "    \"The book will be published next month.\",\n",
    "    \"The dog is walking.\",\n",
    "    \"The building was destroyed by the earthquake.\",\n",
    "    \"It was decided to cancel the event.\",\n",
    "    \"The results are expected soon.\",\n",
    "    \"The paper has been submitted.\",\n",
    "    \"The city was founded in 1850.\",\n",
    "    \"The problem is solved easily.\",\n",
    "    \"The concert was attended by thousands.\",\n",
    "    \"The decision is made by the committee.\",\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(analyze_sentence(sentence, state_verbs_expanded, action_verbs_expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 78.6MB/s]                    \n",
      "2024-08-26 09:14:04 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:14:04 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:14:05 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:14:06 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:14:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 25.1MB/s]                    \n",
      "2024-08-26 09:14:07 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:14:07 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:14:07 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:14:07 INFO: Using device: cpu\n",
      "2024-08-26 09:14:07 INFO: Loading: tokenize\n",
      "2024-08-26 09:14:07 INFO: Loading: mwt\n",
      "2024-08-26 09:14:07 INFO: Loading: pos\n",
      "2024-08-26 09:14:07 INFO: Loading: lemma\n",
      "2024-08-26 09:14:07 INFO: Loading: depparse\n",
      "2024-08-26 09:14:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The cat chased the mouse.' - Passive: False, Confidence: 0.10\n",
      "'The mouse was chased by the cat.' - Passive: True, Confidence: 1.00\n",
      "'The report was written.' - Passive: True, Confidence: 1.00\n",
      "'The cake is being baked.' - Passive: True, Confidence: 0.70\n",
      "'The document had been signed before noon.' - Passive: True, Confidence: 1.00\n",
      "'Mistakes were made.' - Passive: True, Confidence: 1.00\n",
      "'It is believed that the project will succeed.' - Passive: True, Confidence: 0.70\n",
      "'The house is painted white.' - Passive: False, Confidence: 0.00\n",
      "'The meeting was postponed due to bad weather.' - Passive: True, Confidence: 1.00\n",
      "'John was elected president.' - Passive: True, Confidence: 1.00\n",
      "'The book will be published next month.' - Passive: True, Confidence: 1.00\n",
      "'The dog is walking.' - Passive: False, Confidence: 0.00\n",
      "'The building was destroyed by the earthquake.' - Passive: True, Confidence: 1.00\n",
      "'It was decided to cancel the event.' - Passive: True, Confidence: 0.90\n",
      "'The results are expected soon.' - Passive: True, Confidence: 1.00\n",
      "'The paper has been submitted.' - Passive: True, Confidence: 1.00\n",
      "'The city was founded in 1850.' - Passive: True, Confidence: 1.00\n",
      "'The problem is solved easily.' - Passive: True, Confidence: 0.90\n",
      "'The concert was attended by thousands.' - Passive: True, Confidence: 1.00\n",
      "'The decision is made by the committee.' - Passive: True, Confidence: 1.00\n",
      "'He got promoted last week.' - Passive: True, Confidence: 1.00\n",
      "'The project has been completed on time.' - Passive: True, Confidence: 1.00\n",
      "'The floor got waxed yesterday.' - Passive: True, Confidence: 1.00\n",
      "'She is gone.' - Passive: False, Confidence: 0.00\n",
      "'The work is done.' - Passive: True, Confidence: 0.50\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def is_state_description(sentence):\n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    state_adjectives = {'done', 'finished', 'completed', 'solved', 'gone', 'ready'}\n",
    "    \n",
    "    return any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words) and \\\n",
    "           any(word.lemma.lower() in state_adjectives for word in sentence.words)\n",
    "\n",
    "def passive_confidence(sentence):\n",
    "    score = 0\n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "    \n",
    "    if has_aux_pass:\n",
    "        score += 0.4\n",
    "    if has_nsubj_pass:\n",
    "        score += 0.4\n",
    "    \n",
    "    has_vbn = any(\n",
    "        word.upos == 'VERB' and \n",
    "        word.feats and 'VerbForm=Part' in word.feats and\n",
    "        not (word.deprel == 'amod' or  \n",
    "             (word.deprel == 'root' and not has_aux_pass and not has_nsubj_pass))\n",
    "        for word in sentence.words\n",
    "    )\n",
    "    if has_vbn:\n",
    "        score += 0.2\n",
    "    \n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    is_copular = any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words)\n",
    "    if is_copular:\n",
    "        score -= 0.2\n",
    "    \n",
    "    \n",
    "    state_verbs = {'solve', 'finish', 'complete', 'do', 'decide', 'resolve', 'settle'}\n",
    "    action_verbs = {'chase', 'write', 'bake', 'sign', 'make', 'believe', 'paint', 'postpone', \n",
    "                    'elect', 'publish', 'destroy', 'expect', 'submit', 'found', 'attend'}\n",
    "    \n",
    "    main_verb = next((word for word in sentence.words if word.upos == 'VERB' and word.deprel == 'root'), None)\n",
    "    \n",
    "    if main_verb:\n",
    "        if main_verb.lemma in state_verbs:\n",
    "            score -= 0.1\n",
    "        elif main_verb.lemma in action_verbs:\n",
    "            score += 0.1\n",
    "    \n",
    "    \n",
    "    if any(word.lemma == 'get' and word.deprel == 'aux' and \n",
    "           any(w.feats and 'VerbForm=Part' in w.feats for w in sentence.words) \n",
    "           for word in sentence.words):\n",
    "        score += 0.2\n",
    "    \n",
    "    \n",
    "    if any(word.lemma == 'have' and word.deprel == 'aux' and\n",
    "           any(w.lemma == 'be' for w in sentence.words) and\n",
    "           any(w.feats and 'VerbForm=Part' in w.feats for w in sentence.words)\n",
    "           for word in sentence.words):\n",
    "        score += 0.2\n",
    "    \n",
    "    \n",
    "    if is_state_description(sentence):\n",
    "        score -= 0.3\n",
    "    \n",
    "    return max(0, min(score, 1.0))\n",
    "\n",
    "def analyze_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    confidence = passive_confidence(doc.sentences[0])\n",
    "    is_passive = confidence > 0.5\n",
    "    return f\"'{text}' - Passive: {is_passive}, Confidence: {confidence:.2f}\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The mouse was chased by the cat.\",\n",
    "    \"The report was written.\",\n",
    "    \"The cake is being baked.\",\n",
    "    \"The document had been signed before noon.\",\n",
    "    \"Mistakes were made.\",\n",
    "    \"It is believed that the project will succeed.\",\n",
    "    \"The house is painted white.\",\n",
    "    \"The meeting was postponed due to bad weather.\",\n",
    "    \"John was elected president.\",\n",
    "    \"The book will be published next month.\",\n",
    "    \"The dog is walking.\",\n",
    "    \"The building was destroyed by the earthquake.\",\n",
    "    \"It was decided to cancel the event.\",\n",
    "    \"The results are expected soon.\",\n",
    "    \"The paper has been submitted.\",\n",
    "    \"The city was founded in 1850.\",\n",
    "    \"The problem is solved easily.\",\n",
    "    \"The concert was attended by thousands.\",\n",
    "    \"The decision is made by the committee.\",\n",
    "    \"He got promoted last week.\",  \n",
    "    \"The project has been completed on time.\",  \n",
    "    \"The floor got waxed yesterday.\",  \n",
    "    \"She is gone.\",  \n",
    "    \"The work is done.\",  \n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(analyze_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 51.5MB/s]                    \n",
      "2024-08-26 09:16:23 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:16:23 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:16:24 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:16:26 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:16:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 20.7MB/s]                    \n",
      "2024-08-26 09:16:26 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:16:26 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:16:26 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:16:26 INFO: Using device: cpu\n",
      "2024-08-26 09:16:26 INFO: Loading: tokenize\n",
      "2024-08-26 09:16:26 INFO: Loading: mwt\n",
      "2024-08-26 09:16:26 INFO: Loading: pos\n",
      "2024-08-26 09:16:27 INFO: Loading: lemma\n",
      "2024-08-26 09:16:27 INFO: Loading: depparse\n",
      "2024-08-26 09:16:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The cat chased the mouse.' - Passive: False, Confidence: 0.10\n",
      "'The mouse was chased by the cat.' - Passive: True, Confidence: 1.00\n",
      "'The report was written.' - Passive: True, Confidence: 1.00\n",
      "'The cake is being baked.' - Passive: True, Confidence: 0.70\n",
      "'The document had been signed before noon.' - Passive: True, Confidence: 1.00\n",
      "'Mistakes were made.' - Passive: True, Confidence: 1.00\n",
      "'It is believed that the project will succeed.' - Passive: True, Confidence: 0.80\n",
      "'The house is painted white.' - Passive: False, Confidence: 0.00\n",
      "'The meeting was postponed due to bad weather.' - Passive: True, Confidence: 1.00\n",
      "'John was elected president.' - Passive: True, Confidence: 1.00\n",
      "'The book will be published next month.' - Passive: True, Confidence: 1.00\n",
      "'The dog is walking.' - Passive: False, Confidence: 0.00\n",
      "'The building was destroyed by the earthquake.' - Passive: True, Confidence: 1.00\n",
      "'It was decided to cancel the event.' - Passive: True, Confidence: 0.80\n",
      "'The results are expected soon.' - Passive: True, Confidence: 1.00\n",
      "'The paper has been submitted.' - Passive: True, Confidence: 1.00\n",
      "'The city was founded in 1850.' - Passive: True, Confidence: 1.00\n",
      "'The problem is solved easily.' - Passive: True, Confidence: 0.80\n",
      "'The concert was attended by thousands.' - Passive: True, Confidence: 1.00\n",
      "'The decision is made by the committee.' - Passive: True, Confidence: 1.00\n",
      "'He got promoted last week.' - Passive: True, Confidence: 1.00\n",
      "'The project has been completed on time.' - Passive: True, Confidence: 1.00\n",
      "'The floor got waxed yesterday.' - Passive: True, Confidence: 1.00\n",
      "'She is gone.' - Passive: False, Confidence: 0.00\n",
      "'The work is done.' - Passive: False, Confidence: 0.40\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def is_state_description(sentence):\n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    state_adjectives = {'done', 'finished', 'completed', 'solved', 'gone', 'ready'}\n",
    "    \n",
    "    return any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words) and \\\n",
    "           any(word.lemma.lower() in state_adjectives for word in sentence.words)\n",
    "\n",
    "def passive_confidence(sentence):\n",
    "    score = 0\n",
    "    has_aux_pass = any(word.deprel == 'aux:pass' for word in sentence.words)\n",
    "    has_nsubj_pass = any(word.deprel == 'nsubj:pass' for word in sentence.words)\n",
    "    \n",
    "    if has_aux_pass:\n",
    "        score += 0.4\n",
    "    if has_nsubj_pass:\n",
    "        score += 0.4\n",
    "    \n",
    "    has_vbn = any(\n",
    "        word.upos == 'VERB' and \n",
    "        word.feats and 'VerbForm=Part' in word.feats and\n",
    "        not (word.deprel == 'amod' or  \n",
    "             (word.deprel == 'root' and not has_aux_pass and not has_nsubj_pass))\n",
    "        for word in sentence.words\n",
    "    )\n",
    "    if has_vbn:\n",
    "        score += 0.2\n",
    "    \n",
    "    copular_verbs = {'be', 'seem', 'appear', 'look', 'sound', 'smell', 'taste', 'feel', 'become', 'get'}\n",
    "    is_copular = any(word.lemma.lower() in copular_verbs and word.deprel == 'cop' for word in sentence.words)\n",
    "    if is_copular:\n",
    "        score -= 0.2\n",
    "    \n",
    "    \n",
    "    state_verbs = {'solve', 'finish', 'complete', 'do', 'decide', 'resolve', 'settle', 'end', 'conclude'}\n",
    "    action_verbs = {'chase', 'write', 'bake', 'sign', 'make', 'believe', 'paint', 'postpone', \n",
    "                    'elect', 'publish', 'destroy', 'expect', 'submit', 'found', 'attend'}\n",
    "    \n",
    "    main_verb = next((word for word in sentence.words if word.upos == 'VERB' and word.deprel == 'root'), None)\n",
    "    \n",
    "    if main_verb:\n",
    "        if main_verb.lemma in state_verbs:\n",
    "            score -= 0.2\n",
    "        elif main_verb.lemma in action_verbs:\n",
    "            score += 0.1\n",
    "    \n",
    "    \n",
    "    if any(word.lemma == 'get' and word.deprel == 'aux' and \n",
    "           any(w.feats and 'VerbForm=Part' in w.feats for w in sentence.words) \n",
    "           for word in sentence.words):\n",
    "        score += 0.2\n",
    "    \n",
    "    \n",
    "    if any(word.lemma == 'have' and word.deprel == 'aux' and\n",
    "           any(w.lemma == 'be' for w in sentence.words) and\n",
    "           any(w.feats and 'VerbForm=Part' in w.feats for w in sentence.words)\n",
    "           for word in sentence.words):\n",
    "        score += 0.2\n",
    "    \n",
    "    \n",
    "    if any(word.lemma == 'be' and word.deprel == 'aux' and \n",
    "           any(w.lemma == 'be' and w.feats and 'VerbForm=Part' in w.feats for w in sentence.words) \n",
    "           for word in sentence.words):\n",
    "        score += 0.1\n",
    "\n",
    "    \n",
    "    if any(word.lemma == 'it' and word.deprel == 'expl' and\n",
    "           any(w.deprel == 'aux:pass' for w in sentence.words)\n",
    "           for word in sentence.words):\n",
    "        score += 0.1\n",
    "    \n",
    "    \n",
    "    if is_state_description(sentence):\n",
    "        score -= 0.3\n",
    "    \n",
    "    return max(0, min(score, 1.0))\n",
    "\n",
    "def analyze_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    confidence = passive_confidence(doc.sentences[0])\n",
    "    is_passive = confidence > 0.5\n",
    "    return f\"'{text}' - Passive: {is_passive}, Confidence: {confidence:.2f}\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The cat chased the mouse.\",\n",
    "    \"The mouse was chased by the cat.\",\n",
    "    \"The report was written.\",\n",
    "    \"The cake is being baked.\",\n",
    "    \"The document had been signed before noon.\",\n",
    "    \"Mistakes were made.\",\n",
    "    \"It is believed that the project will succeed.\",\n",
    "    \"The house is painted white.\",\n",
    "    \"The meeting was postponed due to bad weather.\",\n",
    "    \"John was elected president.\",\n",
    "    \"The book will be published next month.\",\n",
    "    \"The dog is walking.\",\n",
    "    \"The building was destroyed by the earthquake.\",\n",
    "    \"It was decided to cancel the event.\",\n",
    "    \"The results are expected soon.\",\n",
    "    \"The paper has been submitted.\",\n",
    "    \"The city was founded in 1850.\",\n",
    "    \"The problem is solved easily.\",\n",
    "    \"The concert was attended by thousands.\",\n",
    "    \"The decision is made by the committee.\",\n",
    "    \"He got promoted last week.\",\n",
    "    \"The project has been completed on time.\",\n",
    "    \"The floor got waxed yesterday.\",\n",
    "    \"She is gone.\",\n",
    "    \"The work is done.\",\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(analyze_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 58.3MB/s]                    \n",
      "2024-08-26 09:35:08 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:35:08 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:35:09 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:35:11 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:35:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 40.2MB/s]                    \n",
      "2024-08-26 09:35:11 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:35:11 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:35:11 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:35:11 INFO: Using device: cpu\n",
      "2024-08-26 09:35:11 INFO: Loading: tokenize\n",
      "2024-08-26 09:35:11 INFO: Loading: mwt\n",
      "2024-08-26 09:35:11 INFO: Loading: pos\n",
      "2024-08-26 09:35:11 INFO: Loading: lemma\n",
      "2024-08-26 09:35:13 INFO: Loading: depparse\n",
      "2024-08-26 09:35:13 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was John who broke the vase.\n",
      "[CLEFT] What I need is a good night's sleep.\n",
      "[ --- ] The person who broke the vase was John.\n",
      "[CLEFT] It is in Paris that we first met.\n",
      "[CLEFT] What makes this dish special is the secret sauce.\n",
      "[ --- ] The cat chased the mouse.\n",
      "[ --- ] It seems that he is late.\n",
      "[ --- ] Who arrives first gets the prize.\n",
      "[CLEFT] It is obvious that she's lying.\n",
      "[ --- ] The book that I bought yesterday is interesting.\n",
      "[ --- ] What happened next surprised everyone.\n",
      "[CLEFT] It was yesterday when I saw him last.\n",
      "[ --- ] The reason why he left is unclear.\n",
      "[CLEFT] What you see is what you get.\n",
      "[CLEFT] It is not the strongest species that survive.\n",
      "[CLEFT] It's the early bird that catches the worm.\n",
      "[CLEFT] What I don't understand is why he did it.\n",
      "[ --- ] Where I come from is none of your business.\n",
      "[CLEFT] It's to the library that I go to study.\n",
      "[ --- ] Why he left is still a mystery.\n",
      "[CLEFT] How they managed to escape is what puzzles me.\n",
      "[ --- ] The winner is whoever finishes first.\n",
      "[CLEFT] It was because of the rain that we cancelled the picnic.\n",
      "[CLEFT] When the party starts is when I'll arrive.\n",
      "[CLEFT] What you need to do is focus.\n",
      "[CLEFT] It's not what you know, it's who you know that matters.\n",
      "[ --- ] The place where I grew up is beautiful.\n",
      "[CLEFT] It is with great pleasure that I announce the winner.\n",
      "[CLEFT] What I love most about her is her kindness.\n",
      "[CLEFT] The time when we were happiest was during our vacation.\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 19\n",
      "Ratio of cleft sentences: 0.63\n",
      "\n",
      "Ratio of cleft sentences in the sample text: 0.50\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "import re\n",
    "\n",
    "import re\n",
    " \n",
    "import re\n",
    "\n",
    "def is_cleft_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is a cleft sentence based on dependency parsing, POS tags, and pattern matching.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentence is a cleft sentence, False otherwise.\n",
    "    \"\"\"\n",
    "    words = sentence.words\n",
    "    text = ' '.join(word.text.lower() for word in words)\n",
    "    \n",
    "    \n",
    "    if words[0].text.lower() == 'it':\n",
    "        cop_found = False\n",
    "        for word in words[1:]:\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                cop_found = True\n",
    "            elif cop_found and word.pos in ['PRON', 'SCONJ'] and word.text.lower() in ['who', 'whom', 'that', 'which', 'when', 'where', 'why', 'how']:\n",
    "                return True\n",
    "        if cop_found and re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+(that|who|whom|which|when|where|why|how)', text):\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    wh_words = ['what', 'who', 'whom', 'which', 'where', 'when', 'why', 'how']\n",
    "    if words[0].text.lower() in wh_words:\n",
    "        for i, word in enumerate(words[1:], 1):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                if word.head == 1 or (i+1 < len(words) and words[i+1].deprel in ['nsubj', 'ccomp', 'xcomp', 'advcl']):\n",
    "                    return True\n",
    "    \n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "            if i > 0 and words[i-1].text.lower() in wh_words:\n",
    "                return True\n",
    "            if i+1 < len(words) and words[i+1].text.lower() in wh_words:\n",
    "                return True\n",
    "    \n",
    "    \n",
    "    patterns = [\n",
    "        r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhere\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhen\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhy\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bhow\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwho\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhom\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhich\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\b(is|was|are|were)\\s+(what|where|when|why|how|who|whom|which)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(who|that|which)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bit\\s+(is|was)\\s+\\w+\\s+(when|where|why|how)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(when|where)\\s+\\w+\\s+(is|was|are|were)\\b'\n",
    "    ]\n",
    "    if any(re.search(pattern, text) for pattern in patterns):\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    if re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+that\\b', text):\n",
    "        return True\n",
    "    if re.search(r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\s+.+', text):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def ratio_of_cleft_sentences(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of cleft sentences in the given document.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The input document.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of cleft sentences in the document.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    cleft_count = sum(1 for sentence in sentences if is_cleft_sentence(sentence))\n",
    "    \n",
    "    ratio = cleft_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "    test_sentences = [\n",
    "        \"It was John who broke the vase.\",\n",
    "        \"What I need is a good night's sleep.\",\n",
    "        \"The person who broke the vase was John.\",\n",
    "        \"It is in Paris that we first met.\",\n",
    "        \"What makes this dish special is the secret sauce.\",\n",
    "        \"The cat chased the mouse.\",\n",
    "        \"It seems that he is late.\",\n",
    "        \"Who arrives first gets the prize.\",\n",
    "        \"It is obvious that she's lying.\",\n",
    "        \"The book that I bought yesterday is interesting.\",\n",
    "        \"What happened next surprised everyone.\",\n",
    "        \"It was yesterday when I saw him last.\",\n",
    "        \"The reason why he left is unclear.\",\n",
    "        \"What you see is what you get.\",\n",
    "        \"It is not the strongest species that survive.\",\n",
    "        \"It's the early bird that catches the worm.\",\n",
    "        \"What I don't understand is why he did it.\",\n",
    "        \"Where I come from is none of your business.\",\n",
    "        \"It's to the library that I go to study.\",\n",
    "        \"Why he left is still a mystery.\",\n",
    "        \"How they managed to escape is what puzzles me.\",\n",
    "        \"The winner is whoever finishes first.\",\n",
    "        \"It was because of the rain that we cancelled the picnic.\",\n",
    "        \"When the party starts is when I'll arrive.\",\n",
    "        \"What you need to do is focus.\",\n",
    "        \"It's not what you know, it's who you know that matters.\",\n",
    "        \"The place where I grew up is beautiful.\",\n",
    "        \"It is with great pleasure that I announce the winner.\",\n",
    "        \"What I love most about her is her kindness.\",\n",
    "        \"The time when we were happiest was during our vacation.\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence}\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "It was the best of times, it was the worst of times. What we need is more time.\n",
    "It is in adversity that we often discover our true strength. The book that I read yesterday was fascinating.\n",
    "Who arrives first gets the prize. It seems that history repeats itself.\n",
    "It's not what you say, it's how you say it that matters. Where you go is where I'll follow.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "cleft_ratio = ratio_of_cleft_sentences(doc)\n",
    "print(f\"\\nRatio of cleft sentences in the sample text: {cleft_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 50.2MB/s]                    \n",
      "2024-08-26 09:52:47 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:52:47 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:52:48 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:52:50 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:52:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 39.6MB/s]                    \n",
      "2024-08-26 09:52:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:52:50 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:52:51 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:52:51 INFO: Using device: cpu\n",
      "2024-08-26 09:52:51 INFO: Loading: tokenize\n",
      "2024-08-26 09:52:51 INFO: Loading: mwt\n",
      "2024-08-26 09:52:51 INFO: Loading: pos\n",
      "2024-08-26 09:52:51 INFO: Loading: lemma\n",
      "2024-08-26 09:52:51 INFO: Loading: depparse\n",
      "2024-08-26 09:52:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was Mary who baked the cake.\n",
      "[CLEFT] What he wanted was a new car.\n",
      "[CLEFT] It is my brother who lives in New York.\n",
      "[CLEFT] What you need is a vacation.\n",
      "[CLEFT] It's in the garage where you'll find the toolbox.\n",
      "[CLEFT] It was last night when the accident happened.\n",
      "[CLEFT] What they discovered was astonishing.\n",
      "[CLEFT] It is on this very day that we celebrate our independence.\n",
      "[CLEFT] The one who broke the vase was John.\n",
      "[CLEFT] It was because of the traffic that I was late.\n",
      "[CLEFT] The cat sat on the mat.\n",
      "[CLEFT] She went to the store to buy some milk.\n",
      "[CLEFT] The book on the table is mine.\n",
      "[CLEFT] He plays the guitar beautifully.\n",
      "[CLEFT] They are planning a trip to the mountains.\n",
      "[CLEFT] The company was founded in 1990.\n",
      "[CLEFT] I enjoy reading mystery novels.\n",
      "[CLEFT] The conference was held in Paris.\n",
      "[CLEFT] He often goes for a run in the morning.\n",
      "[CLEFT] The children were playing in the garden.\n",
      "[CLEFT] The man who you saw yesterday is my uncle.\n",
      "[CLEFT] It's not just about winning, it's about how you play the game.\n",
      "[CLEFT] What I don't understand is why he left so suddenly.\n",
      "[CLEFT] It seems that the meeting has been postponed.\n",
      "[CLEFT] Whoever finds the treasure first gets to keep it.\n",
      "[CLEFT] The book that was on the shelf has been moved.\n",
      "[CLEFT] It was at midnight that the clock struck twelve.\n",
      "[CLEFT] What makes this movie great is its storyline.\n",
      "[CLEFT] It appears that there is a mistake in the report.\n",
      "[CLEFT] The last thing I remember is the car speeding away.\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 30\n",
      "Ratio of cleft sentences: 1.00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tuple.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 226\u001b[0m\n\u001b[1;32m    224\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(sample_text)\n\u001b[1;32m    225\u001b[0m cleft_ratio \u001b[38;5;241m=\u001b[39m ratio_of_cleft_sentences(doc)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRatio of cleft sentences in the sample text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleft_ratio\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "import re\n",
    "\n",
    "def is_cleft_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is a cleft sentence based on dependency parsing, POS tags, and pattern matching.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (bool, float) - (is_cleft, confidence_score)\n",
    "    \"\"\"\n",
    "    words = sentence.words\n",
    "    text = ' '.join(word.text.lower() for word in words)\n",
    "    \n",
    "    confidence_score = 0.0\n",
    "    \n",
    "    \n",
    "    if words[0].text.lower() == 'it':\n",
    "        cop_found = False\n",
    "        for word in words[1:]:\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                cop_found = True\n",
    "            elif cop_found and word.pos in ['PRON', 'SCONJ'] and word.text.lower() in ['who', 'whom', 'that', 'which', 'when', 'where', 'why', 'how']:\n",
    "                confidence_score = 1.0\n",
    "                break\n",
    "        if cop_found and re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+(that|who|whom|which|when|where|why|how)', text):\n",
    "            confidence_score = max(confidence_score, 0.9)\n",
    "    \n",
    "    \n",
    "    wh_words = ['what', 'who', 'whom', 'which', 'where', 'when', 'why', 'how']\n",
    "    if words[0].text.lower() in wh_words:\n",
    "        for i, word in enumerate(words[1:], 1):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                if word.head == 1 or (i+1 < len(words) and words[i+1].deprel in ['nsubj', 'ccomp', 'xcomp', 'advcl']):\n",
    "                    confidence_score = max(confidence_score, 0.9)\n",
    "    \n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "            if i > 0 and words[i-1].text.lower() in wh_words:\n",
    "                confidence_score = max(confidence_score, 0.8)\n",
    "            if i+1 < len(words) and words[i+1].text.lower() in wh_words:\n",
    "                confidence_score = max(confidence_score, 0.8)\n",
    "    \n",
    "    \n",
    "    patterns = [\n",
    "        (r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bwhere\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bwhen\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bwhy\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bhow\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bwho\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bwhom\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\bwhich\\s+\\w+\\s+(is|was|are|were)\\b', 0.8),\n",
    "        (r'\\b(is|was|are|were)\\s+(what|where|when|why|how|who|whom|which)\\b', 0.8),\n",
    "        (r'\\bthe\\s+\\w+\\s+(who|that|which)\\s+\\w+\\s+(is|was|are|were)\\b', 0.7),\n",
    "        (r'\\bit\\s+(is|was)\\s+\\w+\\s+(when|where|why|how)\\b', 0.9),\n",
    "        (r'\\bthe\\s+\\w+\\s+(when|where)\\s+\\w+\\s+(is|was|are|were)\\b', 0.7),\n",
    "        (r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\s+.+', 0.7),\n",
    "        (r'\\b(who|what|where|when|why|how)\\s+\\w+\\s+\\w+\\s+.+', 0.6),  \n",
    "        (r'\\bthe\\s+(person|one|thing|reason|place|time)\\s+.+?\\s+(is|was|are|were)\\b', 0.6)  \n",
    "    ]\n",
    "    for pattern, score in patterns:\n",
    "        if re.search(pattern, text):\n",
    "            confidence_score = max(confidence_score, score)\n",
    "    \n",
    "    \n",
    "    if re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+that\\b', text):\n",
    "        \n",
    "        if not re.search(r'\\bit\\s+(is|was)\\s+(obvious|clear|evident|apparent|likely|possible|probable|certain|sure|true|known|assumed|believed|thought|said|reported|claimed|alleged|suspected|doubtful|unclear|uncertain|ambiguous|vague|questionable|debatable|controversial|surprising|interesting|important|significant|crucial|essential|necessary|vital|critical|useful|helpful|beneficial|advantageous|desirable|preferable|better|worse|harder|easier|simpler|more complex|more difficult|more important|more interesting|more significant|more surprising|more useful|more helpful|more beneficial|more advantageous|more desirable|more preferable)\\s+that\\b', text, re.IGNORECASE):\n",
    "            confidence_score = max(confidence_score, 0.9)\n",
    "    \n",
    "    return confidence_score >= 0.7, confidence_score\n",
    "\n",
    "\n",
    "def ratio_of_cleft_sentences(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of cleft sentences in the given document.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The input document.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (float, float, float) - (definite_cleft_ratio, likely_cleft_ratio, possible_cleft_ratio)\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    definite_cleft_count = 0\n",
    "    likely_cleft_count = 0\n",
    "    possible_cleft_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        is_cleft, confidence = is_cleft_sentence(sentence)\n",
    "        if is_cleft:\n",
    "            if confidence >= 0.9:\n",
    "                definite_cleft_count += 1\n",
    "            elif confidence >= 0.8:\n",
    "                likely_cleft_count += 1\n",
    "            else:\n",
    "                possible_cleft_count += 1\n",
    "    \n",
    "    total_sentences = len(sentences)\n",
    "    definite_ratio = definite_cleft_count / total_sentences\n",
    "    likely_ratio = likely_cleft_count / total_sentences\n",
    "    possible_ratio = possible_cleft_count / total_sentences\n",
    "    \n",
    "    return definite_ratio, likely_ratio, possible_ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    test_sentences = [\n",
    "    \n",
    "    \"It was Mary who baked the cake.\",  \n",
    "    \"What he wanted was a new car.\",  \n",
    "    \"It is my brother who lives in New York.\",  \n",
    "    \"What you need is a vacation.\",  \n",
    "    \"It's in the garage where you'll find the toolbox.\",  \n",
    "    \"It was last night when the accident happened.\",  \n",
    "    \"What they discovered was astonishing.\",  \n",
    "    \"It is on this very day that we celebrate our independence.\",  \n",
    "    \"The one who broke the vase was John.\",  \n",
    "    \"It was because of the traffic that I was late.\",  \n",
    "    \n",
    "    \n",
    "    \"The cat sat on the mat.\",  \n",
    "    \"She went to the store to buy some milk.\",  \n",
    "    \"The book on the table is mine.\",  \n",
    "    \"He plays the guitar beautifully.\",  \n",
    "    \"They are planning a trip to the mountains.\",  \n",
    "    \"The company was founded in 1990.\",  \n",
    "    \"I enjoy reading mystery novels.\",  \n",
    "    \"The conference was held in Paris.\",  \n",
    "    \"He often goes for a run in the morning.\",  \n",
    "    \"The children were playing in the garden.\",  \n",
    "\n",
    "    \n",
    "    \"The man who you saw yesterday is my uncle.\",  \n",
    "    \"It's not just about winning, it's about how you play the game.\",  \n",
    "    \"What I don't understand is why he left so suddenly.\",  \n",
    "    \"It seems that the meeting has been postponed.\",  \n",
    "    \"Whoever finds the treasure first gets to keep it.\",  \n",
    "    \"The book that was on the shelf has been moved.\",  \n",
    "    \"It was at midnight that the clock struck twelve.\",  \n",
    "    \"What makes this movie great is its storyline.\",  \n",
    "    \"It appears that there is a mistake in the report.\",  \n",
    "    \"The last thing I remember is the car speeding away.\",  \n",
    "]\n",
    "\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence}\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "It was the best of times, it was the worst of times. What we need is more time.\n",
    "It is in adversity that we often discover our true strength. The book that I read yesterday was fascinating.\n",
    "Who arrives first gets the prize. It seems that history repeats itself.\n",
    "It's not what you say, it's how you say it that matters. Where you go is where I'll follow.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "cleft_ratio = ratio_of_cleft_sentences(doc)\n",
    "print(f\"\\nRatio of cleft sentences in the sample text: {cleft_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was Mary who baked the cake. (Confidence: 1.00)\n",
      "[CLEFT] What he wanted was a new car. (Confidence: 0.80)\n",
      "[CLEFT] It is my brother who lives in New York. (Confidence: 1.00)\n",
      "[CLEFT] What you need is a vacation. (Confidence: 0.80)\n",
      "[ --- ] It's in the garage where you'll find the toolbox. (Confidence: 0.00)\n",
      "[ --- ] It was last night when the accident happened. (Confidence: 0.60)\n",
      "[CLEFT] What they discovered was astonishing. (Confidence: 0.80)\n",
      "[CLEFT] It is on this very day that we celebrate our independence. (Confidence: 1.00)\n",
      "[ --- ] The one who broke the vase was John. (Confidence: 0.60)\n",
      "[CLEFT] It was because of the traffic that I was late. (Confidence: 1.00)\n",
      "[ --- ] The cat sat on the mat. (Confidence: 0.00)\n",
      "[ --- ] She went to the store to buy some milk. (Confidence: 0.00)\n",
      "[ --- ] The book on the table is mine. (Confidence: 0.00)\n",
      "[ --- ] He plays the guitar beautifully. (Confidence: 0.00)\n",
      "[ --- ] They are planning a trip to the mountains. (Confidence: 0.00)\n",
      "[ --- ] The company was founded in 1990. (Confidence: 0.00)\n",
      "[ --- ] I enjoy reading mystery novels. (Confidence: 0.00)\n",
      "[ --- ] The conference was held in Paris. (Confidence: 0.00)\n",
      "[ --- ] He often goes for a run in the morning. (Confidence: 0.00)\n",
      "[ --- ] The children were playing in the garden. (Confidence: 0.00)\n",
      "[ --- ] The man who you saw yesterday is my uncle. (Confidence: 0.60)\n",
      "[ --- ] It's not just about winning, it's about how you play the game. (Confidence: 0.60)\n",
      "[CLEFT] What I don't understand is why he left so suddenly. (Confidence: 0.80)\n",
      "[ --- ] It seems that the meeting has been postponed. (Confidence: 0.00)\n",
      "[ --- ] Whoever finds the treasure first gets to keep it. (Confidence: 0.00)\n",
      "[ --- ] The book that was on the shelf has been moved. (Confidence: 0.00)\n",
      "[CLEFT] It was at midnight that the clock struck twelve. (Confidence: 1.00)\n",
      "[CLEFT] What makes this movie great is its storyline. (Confidence: 0.80)\n",
      "[ --- ] It appears that there is a mistake in the report. (Confidence: 0.00)\n",
      "[ --- ] The last thing I remember is the car speeding away. (Confidence: 0.00)\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 10\n",
      "Ratio of cleft sentences: 0.33\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \n",
    "    \"It was Mary who baked the cake.\",  \n",
    "    \"What he wanted was a new car.\",  \n",
    "    \"It is my brother who lives in New York.\",  \n",
    "    \"What you need is a vacation.\",  \n",
    "    \"It's in the garage where you'll find the toolbox.\",  \n",
    "    \"It was last night when the accident happened.\",  \n",
    "    \"What they discovered was astonishing.\",  \n",
    "    \"It is on this very day that we celebrate our independence.\",  \n",
    "    \"The one who broke the vase was John.\",  \n",
    "    \"It was because of the traffic that I was late.\",  \n",
    "    \n",
    "    \n",
    "    \"The cat sat on the mat.\",  \n",
    "    \"She went to the store to buy some milk.\",  \n",
    "    \"The book on the table is mine.\",  \n",
    "    \"He plays the guitar beautifully.\",  \n",
    "    \"They are planning a trip to the mountains.\",  \n",
    "    \"The company was founded in 1990.\",  \n",
    "    \"I enjoy reading mystery novels.\",  \n",
    "    \"The conference was held in Paris.\",  \n",
    "    \"He often goes for a run in the morning.\",  \n",
    "    \"The children were playing in the garden.\",  \n",
    "\n",
    "    \n",
    "    \"The man who you saw yesterday is my uncle.\",  \n",
    "    \"It's not just about winning, it's about how you play the game.\",  \n",
    "    \"What I don't understand is why he left so suddenly.\",  \n",
    "    \"It seems that the meeting has been postponed.\",  \n",
    "    \"Whoever finds the treasure first gets to keep it.\",  \n",
    "    \"The book that was on the shelf has been moved.\",  \n",
    "    \"It was at midnight that the clock struck twelve.\",  \n",
    "    \"What makes this movie great is its storyline.\",  \n",
    "    \"It appears that there is a mistake in the report.\",  \n",
    "    \"The last thing I remember is the car speeding away.\",  \n",
    "]\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft, confidence = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft, confidence))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft, confidence in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft, _ in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 117MB/s]                     \n",
      "2024-08-26 09:57:44 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:57:44 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:57:45 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:57:47 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:57:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 61.7MB/s]                    \n",
      "2024-08-26 09:57:47 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:57:47 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:57:47 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:57:47 INFO: Using device: cpu\n",
      "2024-08-26 09:57:47 INFO: Loading: tokenize\n",
      "2024-08-26 09:57:47 INFO: Loading: mwt\n",
      "2024-08-26 09:57:47 INFO: Loading: pos\n",
      "2024-08-26 09:57:47 INFO: Loading: lemma\n",
      "2024-08-26 09:57:48 INFO: Loading: depparse\n",
      "2024-08-26 09:57:48 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was John who broke the vase.\n",
      "[CLEFT] What I need is a good night's sleep.\n",
      "[ --- ] The person who broke the vase was John.\n",
      "[CLEFT] It is in Paris that we first met.\n",
      "[CLEFT] What makes this dish special is the secret sauce.\n",
      "[ --- ] The cat chased the mouse.\n",
      "[ --- ] It seems that he is late.\n",
      "[ --- ] Who arrives first gets the prize.\n",
      "[CLEFT] It is obvious that she's lying.\n",
      "[ --- ] The book that I bought yesterday is interesting.\n",
      "[ --- ] What happened next surprised everyone.\n",
      "[CLEFT] It was yesterday when I saw him last.\n",
      "[ --- ] The reason why he left is unclear.\n",
      "[CLEFT] What you see is what you get.\n",
      "[CLEFT] It is not the strongest species that survive.\n",
      "[CLEFT] It's the early bird that catches the worm.\n",
      "[CLEFT] What I don't understand is why he did it.\n",
      "[ --- ] Where I come from is none of your business.\n",
      "[CLEFT] It's to the library that I go to study.\n",
      "[ --- ] Why he left is still a mystery.\n",
      "[CLEFT] How they managed to escape is what puzzles me.\n",
      "[ --- ] The winner is whoever finishes first.\n",
      "[CLEFT] It was because of the rain that we cancelled the picnic.\n",
      "[CLEFT] When the party starts is when I'll arrive.\n",
      "[CLEFT] What you need to do is focus.\n",
      "[CLEFT] It's not what you know, it's who you know that matters.\n",
      "[ --- ] The place where I grew up is beautiful.\n",
      "[CLEFT] It is with great pleasure that I announce the winner.\n",
      "[CLEFT] What I love most about her is her kindness.\n",
      "[CLEFT] The time when we were happiest was during our vacation.\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 19\n",
      "Ratio of cleft sentences: 0.63\n",
      "\n",
      "Ratio of cleft sentences in the sample text: 0.50\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import re\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def is_cleft_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is a cleft sentence based on dependency parsing, POS tags, and pattern matching.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentence is a cleft sentence, False otherwise.\n",
    "    \"\"\"\n",
    "    words = sentence.words\n",
    "    text = ' '.join(word.text.lower() for word in words)\n",
    "    \n",
    "    \n",
    "    if words[0].text.lower() == 'it':\n",
    "        cop_found = False\n",
    "        for i, word in enumerate(words[1:], start=2):  \n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                cop_found = True\n",
    "            elif cop_found and word.pos in ['PRON', 'SCONJ'] and word.text.lower() in ['who', 'whom', 'that', 'which', 'when', 'where', 'why', 'how']:\n",
    "                return True\n",
    "        if cop_found and re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+(that|who|whom|which|when|where|why|how)', text):\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    wh_words = ['what', 'who', 'whom', 'which', 'where', 'when', 'why', 'how']\n",
    "    if words[0].text.lower() in wh_words:\n",
    "        for i, word in enumerate(words[1:], start=2):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                if words[word.head - 1].text.lower() in wh_words:\n",
    "                    return True\n",
    "                if i < len(words) and words[i].deprel in ['nsubj', 'ccomp', 'xcomp', 'advcl']:\n",
    "                    return True\n",
    "    \n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "            if i > 0 and words[i - 1].text.lower() in wh_words:\n",
    "                return True\n",
    "            if i + 1 < len(words) and words[i + 1].text.lower() in wh_words:\n",
    "                return True\n",
    "    \n",
    "    \n",
    "    patterns = [\n",
    "        r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhere\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhen\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhy\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bhow\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwho\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhom\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhich\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\b(is|was|are|were)\\s+(what|where|when|why|how|who|whom|which)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(who|that|which)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bit\\s+(is|was)\\s+\\w+\\s+(when|where|why|how)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(when|where)\\s+\\w+\\s+(is|was|are|were)\\b'\n",
    "    ]\n",
    "    if any(re.search(pattern, text) for pattern in patterns):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def ratio_of_cleft_sentences(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of cleft sentences in the given document.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The input document.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of cleft sentences in the document.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    cleft_count = sum(1 for sentence in sentences if is_cleft_sentence(sentence))\n",
    "    \n",
    "    ratio = cleft_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "    test_sentences = [\n",
    "        \"It was John who broke the vase.\",\n",
    "        \"What I need is a good night's sleep.\",\n",
    "        \"The person who broke the vase was John.\",\n",
    "        \"It is in Paris that we first met.\",\n",
    "        \"What makes this dish special is the secret sauce.\",\n",
    "        \"The cat chased the mouse.\",\n",
    "        \"It seems that he is late.\",\n",
    "        \"Who arrives first gets the prize.\",\n",
    "        \"It is obvious that she's lying.\",\n",
    "        \"The book that I bought yesterday is interesting.\",\n",
    "        \"What happened next surprised everyone.\",\n",
    "        \"It was yesterday when I saw him last.\",\n",
    "        \"The reason why he left is unclear.\",\n",
    "        \"What you see is what you get.\",\n",
    "        \"It is not the strongest species that survive.\",\n",
    "        \"It's the early bird that catches the worm.\",\n",
    "        \"What I don't understand is why he did it.\",\n",
    "        \"Where I come from is none of your business.\",\n",
    "        \"It's to the library that I go to study.\",\n",
    "        \"Why he left is still a mystery.\",\n",
    "        \"How they managed to escape is what puzzles me.\",\n",
    "        \"The winner is whoever finishes first.\",\n",
    "        \"It was because of the rain that we cancelled the picnic.\",\n",
    "        \"When the party starts is when I'll arrive.\",\n",
    "        \"What you need to do is focus.\",\n",
    "        \"It's not what you know, it's who you know that matters.\",\n",
    "        \"The place where I grew up is beautiful.\",\n",
    "        \"It is with great pleasure that I announce the winner.\",\n",
    "        \"What I love most about her is her kindness.\",\n",
    "        \"The time when we were happiest was during our vacation.\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence}\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "It was the best of times, it was the worst of times. What we need is more time.\n",
    "It is in adversity that we often discover our true strength. The book that I read yesterday was fascinating.\n",
    "Who arrives first gets the prize. It seems that history repeats itself.\n",
    "It's not what you say, it's how you say it that matters. Where you go is where I'll follow.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "cleft_ratio = ratio_of_cleft_sentences(doc)\n",
    "print(f\"\\nRatio of cleft sentences in the sample text: {cleft_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 102MB/s]                     \n",
      "2024-08-26 09:59:17 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:59:17 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 09:59:18 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 09:59:20 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 09:59:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 52.1MB/s]                    \n",
      "2024-08-26 09:59:20 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 09:59:20 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 09:59:20 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 09:59:20 INFO: Using device: cpu\n",
      "2024-08-26 09:59:20 INFO: Loading: tokenize\n",
      "2024-08-26 09:59:20 INFO: Loading: mwt\n",
      "2024-08-26 09:59:20 INFO: Loading: pos\n",
      "2024-08-26 09:59:20 INFO: Loading: lemma\n",
      "2024-08-26 09:59:20 INFO: Loading: depparse\n",
      "2024-08-26 09:59:21 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was John who broke the vase.\n",
      "[CLEFT] What I need is a good night's sleep.\n",
      "[CLEFT] The person who broke the vase was John.\n",
      "[CLEFT] It is in Paris that we first met.\n",
      "[CLEFT] What makes this dish special is the secret sauce.\n",
      "[ --- ] The cat chased the mouse.\n",
      "[ --- ] It seems that he is late.\n",
      "[ --- ] Who arrives first gets the prize.\n",
      "[CLEFT] It is obvious that she's lying.\n",
      "[ --- ] The book that I bought yesterday is interesting.\n",
      "[ --- ] What happened next surprised everyone.\n",
      "[CLEFT] It was yesterday when I saw him last.\n",
      "[CLEFT] The reason why he left is unclear.\n",
      "[CLEFT] What you see is what you get.\n",
      "[CLEFT] It is not the strongest species that survive.\n",
      "[CLEFT] It's the early bird that catches the worm.\n",
      "[CLEFT] What I don't understand is why he did it.\n",
      "[CLEFT] Where I come from is none of your business.\n",
      "[CLEFT] It's to the library that I go to study.\n",
      "[CLEFT] Why he left is still a mystery.\n",
      "[CLEFT] How they managed to escape is what puzzles me.\n",
      "[ --- ] The winner is whoever finishes first.\n",
      "[CLEFT] It was because of the rain that we cancelled the picnic.\n",
      "[CLEFT] When the party starts is when I'll arrive.\n",
      "[CLEFT] What you need to do is focus.\n",
      "[CLEFT] It's not what you know, it's who you know that matters.\n",
      "[CLEFT] The place where I grew up is beautiful.\n",
      "[CLEFT] It is with great pleasure that I announce the winner.\n",
      "[CLEFT] What I love most about her is her kindness.\n",
      "[CLEFT] The time when we were happiest was during our vacation.\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 24\n",
      "Ratio of cleft sentences: 0.80\n",
      "\n",
      "Ratio of cleft sentences in the sample text: 0.50\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import re\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "\n",
    "def is_cleft_sentence(sentence):\n",
    "    words = sentence.words\n",
    "    text = ' '.join(word.text.lower() for word in words)\n",
    "    \n",
    "    \n",
    "    if words[0].text.lower() == 'it':\n",
    "        cop_found = False\n",
    "        for i, word in enumerate(words[1:], start=2):  \n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                cop_found = True\n",
    "            elif cop_found and word.pos in ['PRON', 'SCONJ'] and word.text.lower() in ['who', 'whom', 'that', 'which', 'when', 'where', 'why', 'how']:\n",
    "                return True\n",
    "        if cop_found and re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+(that|who|whom|which|when|where|why|how)', text):\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    wh_words = ['what', 'who', 'whom', 'which', 'where', 'when', 'why', 'how']\n",
    "    if words[0].text.lower() in wh_words:\n",
    "        for i, word in enumerate(words[1:], start=2):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                if words[word.head - 1].text.lower() in wh_words or \\\n",
    "                   (i < len(words) and words[i].deprel in ['nsubj', 'ccomp', 'xcomp', 'advcl']):\n",
    "                    return True\n",
    "    \n",
    "    \n",
    "    if any(re.search(rf'\\b{wh}\\s.+?\\s+(is|was|are|were)\\s.+\\b', text) for wh in wh_words):\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    patterns = [\n",
    "        r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhere\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhen\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhy\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bhow\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwho\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhom\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhich\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\b(is|was|are|were)\\s+(what|where|when|why|how|who|whom|which)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(who|that|which)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bit\\s+(is|was)\\s+\\w+\\s+(when|where|why|how)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(when|where)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhat\\s+\\w+\\s+(is|was)\\s+.+\\b',\n",
    "        r'\\b(is|was)\\s+the\\s+\\w+\\s+that\\s+.+\\b'\n",
    "    ]\n",
    "    if any(re.search(pattern, text) for pattern in patterns):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def ratio_of_cleft_sentences(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of cleft sentences in the given document.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The input document.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of cleft sentences in the document.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    cleft_count = sum(1 for sentence in sentences if is_cleft_sentence(sentence))\n",
    "    \n",
    "    ratio = cleft_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "    test_sentences = [\n",
    "        \"It was John who broke the vase.\",\n",
    "        \"What I need is a good night's sleep.\",\n",
    "        \"The person who broke the vase was John.\",\n",
    "        \"It is in Paris that we first met.\",\n",
    "        \"What makes this dish special is the secret sauce.\",\n",
    "        \"The cat chased the mouse.\",\n",
    "        \"It seems that he is late.\",\n",
    "        \"Who arrives first gets the prize.\",\n",
    "        \"It is obvious that she's lying.\",\n",
    "        \"The book that I bought yesterday is interesting.\",\n",
    "        \"What happened next surprised everyone.\",\n",
    "        \"It was yesterday when I saw him last.\",\n",
    "        \"The reason why he left is unclear.\",\n",
    "        \"What you see is what you get.\",\n",
    "        \"It is not the strongest species that survive.\",\n",
    "        \"It's the early bird that catches the worm.\",\n",
    "        \"What I don't understand is why he did it.\",\n",
    "        \"Where I come from is none of your business.\",\n",
    "        \"It's to the library that I go to study.\",\n",
    "        \"Why he left is still a mystery.\",\n",
    "        \"How they managed to escape is what puzzles me.\",\n",
    "        \"The winner is whoever finishes first.\",\n",
    "        \"It was because of the rain that we cancelled the picnic.\",\n",
    "        \"When the party starts is when I'll arrive.\",\n",
    "        \"What you need to do is focus.\",\n",
    "        \"It's not what you know, it's who you know that matters.\",\n",
    "        \"The place where I grew up is beautiful.\",\n",
    "        \"It is with great pleasure that I announce the winner.\",\n",
    "        \"What I love most about her is her kindness.\",\n",
    "        \"The time when we were happiest was during our vacation.\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence}\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "It was the best of times, it was the worst of times. What we need is more time.\n",
    "It is in adversity that we often discover our true strength. The book that I read yesterday was fascinating.\n",
    "Who arrives first gets the prize. It seems that history repeats itself.\n",
    "It's not what you say, it's how you say it that matters. Where you go is where I'll follow.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "cleft_ratio = ratio_of_cleft_sentences(doc)\n",
    "print(f\"\\nRatio of cleft sentences in the sample text: {cleft_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 53.1MB/s]                    \n",
      "2024-08-26 10:04:33 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 10:04:33 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 10:04:34 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 10:04:36 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 10:04:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.9MB/s]                    \n",
      "2024-08-26 10:04:36 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 10:04:36 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 10:04:37 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 10:04:37 INFO: Using device: cpu\n",
      "2024-08-26 10:04:37 INFO: Loading: tokenize\n",
      "2024-08-26 10:04:37 INFO: Loading: mwt\n",
      "2024-08-26 10:04:37 INFO: Loading: pos\n",
      "2024-08-26 10:04:37 INFO: Loading: lemma\n",
      "2024-08-26 10:04:37 INFO: Loading: depparse\n",
      "2024-08-26 10:04:38 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was John who broke the vase.\n",
      "[CLEFT] What I need is a good night's sleep.\n",
      "[CLEFT] The person who broke the vase was John.\n",
      "[CLEFT] It is in Paris that we first met.\n",
      "[CLEFT] What makes this dish special is the secret sauce.\n",
      "[ --- ] The cat chased the mouse.\n",
      "[ --- ] It seems that he is late.\n",
      "[ --- ] Who arrives first gets the prize.\n",
      "[CLEFT] It is obvious that she's lying.\n",
      "[ --- ] The book that I bought yesterday is interesting.\n",
      "[ --- ] What happened next surprised everyone.\n",
      "[CLEFT] It was yesterday when I saw him last.\n",
      "[CLEFT] The reason why he left is unclear.\n",
      "[CLEFT] What you see is what you get.\n",
      "[CLEFT] It is not the strongest species that survive.\n",
      "[CLEFT] It's the early bird that catches the worm.\n",
      "[CLEFT] What I don't understand is why he did it.\n",
      "[CLEFT] Where I come from is none of your business.\n",
      "[CLEFT] It's to the library that I go to study.\n",
      "[CLEFT] Why he left is still a mystery.\n",
      "[CLEFT] How they managed to escape is what puzzles me.\n",
      "[ --- ] The winner is whoever finishes first.\n",
      "[CLEFT] It was because of the rain that we cancelled the picnic.\n",
      "[CLEFT] When the party starts is when I'll arrive.\n",
      "[CLEFT] What you need to do is focus.\n",
      "[CLEFT] It's not what you know, it's who you know that matters.\n",
      "[CLEFT] The place where I grew up is beautiful.\n",
      "[CLEFT] It is with great pleasure that I announce the winner.\n",
      "[CLEFT] What I love most about her is her kindness.\n",
      "[CLEFT] The time when we were happiest was during our vacation.\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 24\n",
      "Ratio of cleft sentences: 0.80\n",
      "\n",
      "Ratio of cleft sentences in the sample text: 0.50\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import re\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def is_cleft_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Determines if a sentence is a cleft sentence based on dependency parsing, POS tags, and pattern matching.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentence is a cleft sentence, False otherwise.\n",
    "    \"\"\"\n",
    "    words = sentence.words\n",
    "    text = ' '.join(word.text.lower() for word in words)\n",
    "    \n",
    "    \n",
    "    if words[0].text.lower() == 'it':\n",
    "        cop_found = False\n",
    "        for i, word in enumerate(words[1:], start=2):  \n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                cop_found = True\n",
    "            elif cop_found and word.pos in ['PRON', 'SCONJ'] and word.text.lower() in ['who', 'whom', 'that', 'which', 'when', 'where', 'why', 'how']:\n",
    "                return True\n",
    "        if cop_found and re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+(that|who|whom|which|when|where|why|how)', text):\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    wh_words = ['what', 'who', 'whom', 'which', 'where', 'when', 'why', 'how']\n",
    "    if words[0].text.lower() in wh_words:\n",
    "        for i, word in enumerate(words[1:], start=2):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                if words[word.head - 1].text.lower() in wh_words or \\\n",
    "                   (i < len(words) and words[i].deprel in ['nsubj', 'ccomp', 'xcomp', 'advcl']):\n",
    "                    return True\n",
    "\n",
    "    \n",
    "    if any(re.search(rf'\\b{wh}\\s+.+?\\s+(is|was|are|were)\\s+.+\\b', text) for wh in wh_words):\n",
    "        return True\n",
    "\n",
    "    \n",
    "    patterns = [\n",
    "        r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhere\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhen\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhy\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bhow\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwho\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhom\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhich\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\b(is|was|are|were)\\s+(what|where|when|why|how|who|whom|which)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(who|that|which)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bit\\s+(is|was)\\s+\\w+\\s+(when|where|why|how)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(when|where)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhat\\s+\\w+\\s+(is|was)\\s+.+\\b',\n",
    "        r'\\b(is|was)\\s+the\\s+\\w+\\s+that\\s+.+\\b'\n",
    "    ]\n",
    "    if any(re.search(pattern, text) for pattern in patterns):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def ratio_of_cleft_sentences(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of cleft sentences in the given document.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The input document.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of cleft sentences in the document.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    cleft_count = sum(1 for sentence in sentences if is_cleft_sentence(sentence))\n",
    "    \n",
    "    ratio = cleft_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "    test_sentences = [\n",
    "        \"It was John who broke the vase.\",\n",
    "        \"What I need is a good night's sleep.\",\n",
    "        \"The person who broke the vase was John.\",\n",
    "        \"It is in Paris that we first met.\",\n",
    "        \"What makes this dish special is the secret sauce.\",\n",
    "        \"The cat chased the mouse.\",\n",
    "        \"It seems that he is late.\",\n",
    "        \"Who arrives first gets the prize.\",\n",
    "        \"It is obvious that she's lying.\",\n",
    "        \"The book that I bought yesterday is interesting.\",\n",
    "        \"What happened next surprised everyone.\",\n",
    "        \"It was yesterday when I saw him last.\",\n",
    "        \"The reason why he left is unclear.\",\n",
    "        \"What you see is what you get.\",\n",
    "        \"It is not the strongest species that survive.\",\n",
    "        \"It's the early bird that catches the worm.\",\n",
    "        \"What I don't understand is why he did it.\",\n",
    "        \"Where I come from is none of your business.\",\n",
    "        \"It's to the library that I go to study.\",\n",
    "        \"Why he left is still a mystery.\",\n",
    "        \"How they managed to escape is what puzzles me.\",\n",
    "        \"The winner is whoever finishes first.\",\n",
    "        \"It was because of the rain that we cancelled the picnic.\",\n",
    "        \"When the party starts is when I'll arrive.\",\n",
    "        \"What you need to do is focus.\",\n",
    "        \"It's not what you know, it's who you know that matters.\",\n",
    "        \"The place where I grew up is beautiful.\",\n",
    "        \"It is with great pleasure that I announce the winner.\",\n",
    "        \"What I love most about her is her kindness.\",\n",
    "        \"The time when we were happiest was during our vacation.\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence}\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "It was the best of times, it was the worst of times. What we need is more time.\n",
    "It is in adversity that we often discover our true strength. The book that I read yesterday was fascinating.\n",
    "Who arrives first gets the prize. It seems that history repeats itself.\n",
    "It's not what you say, it's how you say it that matters. Where you go is where I'll follow.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "cleft_ratio = ratio_of_cleft_sentences(doc)\n",
    "print(f\"\\nRatio of cleft sentences in the sample text: {cleft_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 57.6MB/s]                    \n",
      "2024-08-26 10:28:45 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 10:28:45 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 10:28:46 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 10:28:48 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 10:28:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 41.8MB/s]                    \n",
      "2024-08-26 10:28:48 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 10:28:48 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 10:28:48 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 10:28:48 INFO: Using device: cpu\n",
      "2024-08-26 10:28:48 INFO: Loading: tokenize\n",
      "2024-08-26 10:28:48 INFO: Loading: mwt\n",
      "2024-08-26 10:28:48 INFO: Loading: pos\n",
      "2024-08-26 10:28:49 INFO: Loading: lemma\n",
      "2024-08-26 10:28:49 INFO: Loading: depparse\n",
      "2024-08-26 10:28:49 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleft Sentence Detection Results:\n",
      "[CLEFT] It was John who broke the vase.\n",
      "[CLEFT] What I need is a good night's sleep.\n",
      "[CLEFT] The person who broke the vase was John.\n",
      "[CLEFT] It is in Paris that we first met.\n",
      "[CLEFT] What makes this dish special is the secret sauce.\n",
      "[CLEFT] It's the early bird that catches the worm.\n",
      "[CLEFT] Where I come from is none of your business.\n",
      "[CLEFT] It's to the library that I go to study.\n",
      "[CLEFT] Why he left is still a mystery.\n",
      "[CLEFT] The winner is whoever finishes first.\n",
      "[CLEFT] It was because of the rain that we cancelled the picnic.\n",
      "[CLEFT] What you need to do is focus.\n",
      "[CLEFT] It is the CEO who makes the final decision.\n",
      "[CLEFT] The thing that worries me most is the economy.\n",
      "[CLEFT] All I want for Christmas is you.\n",
      "[ --- ] The cat sat on the mat.\n",
      "[ --- ] I enjoy reading books in my free time.\n",
      "[ --- ] She sings beautifully.\n",
      "[ --- ] The sun rises in the east.\n",
      "[ --- ] They went to the movies last night.\n",
      "[ --- ] Apples are my favorite fruit.\n",
      "[ --- ] He runs every morning to stay fit.\n",
      "[ --- ] The children are playing in the park.\n",
      "[ --- ] I forgot to bring my umbrella today.\n",
      "[ --- ] The concert was cancelled due to bad weather.\n",
      "[ --- ] She speaks three languages fluently.\n",
      "[ --- ] The restaurant serves delicious Italian food.\n",
      "[ --- ] We should protect endangered species.\n",
      "[ --- ] The train arrives at 9 AM sharp.\n",
      "[ --- ] Learning a new skill takes time and patience.\n",
      "\n",
      "Statistics:\n",
      "Total sentences: 30\n",
      "Detected cleft sentences: 15\n",
      "Ratio of cleft sentences: 0.50\n",
      "\n",
      "Ratio of cleft sentences in the sample text: 0.62\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import re\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "import re\n",
    "\n",
    "def is_cleft_sentence(sentence):\n",
    "    words = sentence.words\n",
    "    text = ' '.join(word.text.lower() for word in words)\n",
    "    \n",
    "    \n",
    "    exclude_patterns = [\n",
    "        r'\\bit\\s+(is|was)\\s+(obvious|clear|evident|apparent|known|understood)\\s+that\\b',\n",
    "        r'\\bit\\s+seems\\s+that\\b'\n",
    "    ]\n",
    "    if any(re.search(pattern, text) for pattern in exclude_patterns):\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    if words[0].text.lower() == 'it':\n",
    "        cop_found = False\n",
    "        for i, word in enumerate(words[1:], start=1):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                cop_found = True\n",
    "            elif cop_found and word.pos in ['PRON', 'SCONJ'] and word.text.lower() in ['who', 'whom', 'that', 'which', 'when', 'where', 'why', 'how']:\n",
    "                return True\n",
    "        if cop_found and re.search(r'\\bit\\s+(is|was)\\s+\\w+\\s+(that|who|whom|which|when|where|why|how)', text):\n",
    "            return True\n",
    "    \n",
    "    \n",
    "    wh_words = ['what', 'who', 'whom', 'which', 'where', 'when', 'why', 'how']\n",
    "    if words[0].text.lower() in wh_words:\n",
    "        for i, word in enumerate(words[1:], start=1):\n",
    "            if word.deprel == 'cop' and word.pos == 'AUX':\n",
    "                return True\n",
    "            if i < len(words) - 1 and words[i+1].deprel in ['nsubj', 'ccomp', 'xcomp', 'advcl']:\n",
    "                return True\n",
    "\n",
    "    \n",
    "    if any(re.search(rf'\\b{wh}\\s+.+?\\s+(is|was|are|were)\\s+.+\\b', text) for wh in wh_words):\n",
    "        return True\n",
    "\n",
    "    \n",
    "    patterns = [\n",
    "        r'\\bwhat\\s+.+?\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhere\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhen\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhy\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bhow\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwho\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhom\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhich\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\b(is|was|are|were)\\s+(what|where|when|why|how|who|whom|which)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(who|that|which)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bit\\s+(is|was)\\s+\\w+\\s+(when|where|why|how)\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+(when|where)\\s+\\w+\\s+(is|was|are|were)\\b',\n",
    "        r'\\bwhat\\s+\\w+\\s+(is|was)\\s+.+\\b',\n",
    "        r'\\b(is|was)\\s+the\\s+\\w+\\s+that\\s+.+\\b',\n",
    "        r'\\bwho\\s+\\w+\\s+\\w+\\s+.+\\b',\n",
    "        r'\\bwhat\\s+\\w+\\s+\\w+\\s+.+\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+is\\s+\\w+\\s+\\w+\\s+.+\\b',\n",
    "        r'\\bthe\\s+\\w+\\s+that\\s+.+?\\s+is\\b',\n",
    "        r'\\ball\\s+\\w+\\s+\\w+\\s+.+?\\s+is\\b'\n",
    "    ]\n",
    "    if any(re.search(pattern, text) for pattern in patterns):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def ratio_of_cleft_sentences(doc):\n",
    "    \"\"\"\n",
    "    Computes the ratio of cleft sentences in the given document.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The input document.\n",
    "\n",
    "    Returns:\n",
    "        float: The ratio of cleft sentences in the document.\n",
    "    \"\"\"\n",
    "    sentences = doc.sentences\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    cleft_count = sum(1 for sentence in sentences if is_cleft_sentence(sentence))\n",
    "    \n",
    "    ratio = cleft_count / len(sentences)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "\n",
    "def test_cleft_detection():\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_sentences = [\n",
    "        \n",
    "        \"It was John who broke the vase.\",\n",
    "        \"What I need is a good night's sleep.\",\n",
    "        \"The person who broke the vase was John.\",\n",
    "        \"It is in Paris that we first met.\",\n",
    "        \"What makes this dish special is the secret sauce.\",\n",
    "        \"It's the early bird that catches the worm.\",\n",
    "        \"Where I come from is none of your business.\",\n",
    "        \"It's to the library that I go to study.\",\n",
    "        \"Why he left is still a mystery.\",\n",
    "        \"The winner is whoever finishes first.\",\n",
    "        \"It was because of the rain that we cancelled the picnic.\",\n",
    "        \"What you need to do is focus.\",\n",
    "        \"It is the CEO who makes the final decision.\",\n",
    "        \"The thing that worries me most is the economy.\",\n",
    "        \"All I want for Christmas is you.\",\n",
    "\n",
    "        \n",
    "        \"The cat sat on the mat.\",\n",
    "        \"I enjoy reading books in my free time.\",\n",
    "        \"She sings beautifully.\",\n",
    "        \"The sun rises in the east.\",\n",
    "        \"They went to the movies last night.\",\n",
    "        \"Apples are my favorite fruit.\",\n",
    "        \"He runs every morning to stay fit.\",\n",
    "        \"The children are playing in the park.\",\n",
    "        \"I forgot to bring my umbrella today.\",\n",
    "        \"The concert was cancelled due to bad weather.\",\n",
    "        \"She speaks three languages fluently.\",\n",
    "        \"The restaurant serves delicious Italian food.\",\n",
    "        \"We should protect endangered species.\",\n",
    "        \"The train arrives at 9 AM sharp.\",\n",
    "        \"Learning a new skill takes time and patience.\"\n",
    "    ]\n",
    "    results = []\n",
    "    for sentence in test_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        is_cleft = is_cleft_sentence(doc.sentences[0])\n",
    "        results.append((sentence, is_cleft))\n",
    "\n",
    "    print(\"Cleft Sentence Detection Results:\")\n",
    "    for sentence, is_cleft in results:\n",
    "        print(f\"{'[CLEFT]' if is_cleft else '[ --- ]'} {sentence}\")\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    total = len(results)\n",
    "    cleft_count = sum(1 for _, is_cleft in results if is_cleft)\n",
    "    print(f\"Total sentences: {total}\")\n",
    "    print(f\"Detected cleft sentences: {cleft_count}\")\n",
    "    print(f\"Ratio of cleft sentences: {cleft_count/total:.2f}\")\n",
    "\n",
    "\n",
    "test_cleft_detection()\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "It was the best of times, it was the worst of times. What we need is more time.\n",
    "It is in adversity that we often discover our true strength. The book that I read yesterday was fascinating.\n",
    "Who arrives first gets the prize. It seems that history repeats itself.\n",
    "It's not what you say, it's how you say it that matters. Where you go is where I'll follow.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "cleft_ratio = ratio_of_cleft_sentences(doc)\n",
    "print(f\"\\nRatio of cleft sentences in the sample text: {cleft_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 10:30:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 60.8MB/s]                    \n",
      "2024-08-26 10:30:40 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 10:30:42 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 10:30:42 INFO: Using device: cpu\n",
      "2024-08-26 10:30:42 INFO: Loading: tokenize\n",
      "2024-08-26 10:30:42 INFO: Loading: mwt\n",
      "2024-08-26 10:30:42 INFO: Loading: pos\n",
      "2024-08-26 10:30:42 INFO: Loading: lemma\n",
      "2024-08-26 10:30:42 INFO: Loading: constituency\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 10:30:42 INFO: Loading: depparse\n",
      "2024-08-26 10:30:42 INFO: Loading: sentiment\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 10:30:43 INFO: Loading: ner\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 10:30:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: High Overlap\n",
      "Text: The cat sat on the mat. The cat chased a mouse. A dog barked at the cat.\n",
      "Expected overlap: 0.67\n",
      "Calculated overlap: 0.33\n",
      "FAIL\n",
      "\n",
      "Test: No Overlap\n",
      "Text: The cat sat on the mat. A dog barked loudly. Birds flew in the sky.\n",
      "Expected overlap: 0.00\n",
      "Calculated overlap: 0.00\n",
      "PASS\n",
      "\n",
      "Test: Partial Overlap\n",
      "Text: The teacher explained the lesson. Students asked questions. The teacher assigned homework.\n",
      "Expected overlap: 0.33\n",
      "Calculated overlap: 0.00\n",
      "FAIL\n",
      "\n",
      "Test: Single Sentence\n",
      "Text: The sun shines brightly.\n",
      "Expected overlap: 0.00\n",
      "Calculated overlap: 0.00\n",
      "PASS\n",
      "\n",
      "Test: Complex Sentences\n",
      "Text: The old car broke down on the highway. Mechanics at the garage examined the engine. They found several issues with the car's transmission.\n",
      "Expected overlap: 0.67\n",
      "Calculated overlap: 0.00\n",
      "FAIL\n",
      "\n",
      "Test: Mixed Content\n",
      "Text: John loves apples. Apples are fruits. Bananas are also fruits. John eats bananas too.\n",
      "Expected overlap: 0.75\n",
      "Calculated overlap: 0.00\n",
      "FAIL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def calculate_noun_overlap(doc):\n",
    "    overlap_count = 0\n",
    "    previous_nouns = set()\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        current_nouns = {word.text for word in sentence.words if word.upos == 'NOUN' and word.deprel in {'nsubj', 'dobj', 'nmod'}}\n",
    "        if previous_nouns & current_nouns:\n",
    "            overlap_count += 1\n",
    "        previous_nouns = current_nouns\n",
    "    \n",
    "    return overlap_count / len(doc.sentences) if len(doc.sentences) > 0 else 0\n",
    "\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"High Overlap\",\n",
    "        \"text\": \"The cat sat on the mat. The cat chased a mouse. A dog barked at the cat.\",\n",
    "        \"expected_overlap\": 2/3  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"No Overlap\",\n",
    "        \"text\": \"The cat sat on the mat. A dog barked loudly. Birds flew in the sky.\",\n",
    "        \"expected_overlap\": 0  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Overlap\",\n",
    "        \"text\": \"The teacher explained the lesson. Students asked questions. The teacher assigned homework.\",\n",
    "        \"expected_overlap\": 1/3  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Single Sentence\",\n",
    "        \"text\": \"The sun shines brightly.\",\n",
    "        \"expected_overlap\": 0  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Complex Sentences\",\n",
    "        \"text\": \"The old car broke down on the highway. Mechanics at the garage examined the engine. They found several issues with the car's transmission.\",\n",
    "        \"expected_overlap\": 2/3  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Mixed Content\",\n",
    "        \"text\": \"John loves apples. Apples are fruits. Bananas are also fruits. John eats bananas too.\",\n",
    "        \"expected_overlap\": 3/4  \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for test_case in test_cases:\n",
    "    doc = nlp(test_case[\"text\"])\n",
    "    result = calculate_noun_overlap(doc)\n",
    "    print(f\"Test: {test_case['name']}\")\n",
    "    print(f\"Text: {test_case['text']}\")\n",
    "    print(f\"Expected overlap: {test_case['expected_overlap']:.2f}\")\n",
    "    print(f\"Calculated overlap: {result:.2f}\")\n",
    "    print(f\"{'PASS' if abs(result - test_case['expected_overlap']) < 0.01 else 'FAIL'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 10:35:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 27.4MB/s]                    \n",
      "2024-08-26 10:35:02 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 10:35:02 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-26 10:35:03 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-08-26 10:35:03 INFO: Using device: cpu\n",
      "2024-08-26 10:35:03 INFO: Loading: tokenize\n",
      "2024-08-26 10:35:03 INFO: Loading: mwt\n",
      "2024-08-26 10:35:03 INFO: Loading: pos\n",
      "2024-08-26 10:35:03 INFO: Loading: lemma\n",
      "2024-08-26 10:35:03 INFO: Loading: depparse\n",
      "2024-08-26 10:35:04 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: High Overlap\n",
      "Text: The cat sat on the mat. The cat chased a mouse. A dog barked at the cat.\n",
      "Expected overlap: 0.67\n",
      "Calculated overlap: 0.67\n",
      "PASS\n",
      "\n",
      "Test: No Overlap\n",
      "Text: The cat sat on the mat. A dog barked loudly. Birds flew in the sky.\n",
      "Expected overlap: 0.00\n",
      "Calculated overlap: 0.00\n",
      "PASS\n",
      "\n",
      "Test: Partial Overlap\n",
      "Text: The teacher explained the lesson. Students asked questions. The teacher assigned homework.\n",
      "Expected overlap: 0.33\n",
      "Calculated overlap: 0.33\n",
      "PASS\n",
      "\n",
      "Test: Single Sentence\n",
      "Text: The sun shines brightly.\n",
      "Expected overlap: 0.00\n",
      "Calculated overlap: 0.00\n",
      "PASS\n",
      "\n",
      "Test: Complex Sentences\n",
      "Text: The old car broke down on the highway. Mechanics at the garage examined the engine. They found several issues with the car's transmission.\n",
      "Expected overlap: 0.67\n",
      "Calculated overlap: 0.33\n",
      "FAIL\n",
      "\n",
      "Test: Mixed Content\n",
      "Text: John loves apples. Apples are fruits. Bananas are also fruits. John eats bananas too.\n",
      "Expected overlap: 0.75\n",
      "Calculated overlap: 0.75\n",
      "PASS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def calculate_noun_overlap(doc):\n",
    "    overlap_count = 0\n",
    "    previous_nouns = set()\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        current_nouns = {word.text.lower() for word in sentence.words if word.upos == 'NOUN'}\n",
    "        if previous_nouns & current_nouns:\n",
    "            overlap_count += 1\n",
    "        previous_nouns = previous_nouns | current_nouns  \n",
    "\n",
    "    return overlap_count / len(doc.sentences) if len(doc.sentences) > 0 else 0\n",
    "\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"High Overlap\",\n",
    "        \"text\": \"The cat sat on the mat. The cat chased a mouse. A dog barked at the cat.\",\n",
    "        \"expected_overlap\": 2/3  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"No Overlap\",\n",
    "        \"text\": \"The cat sat on the mat. A dog barked loudly. Birds flew in the sky.\",\n",
    "        \"expected_overlap\": 0  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Overlap\",\n",
    "        \"text\": \"The teacher explained the lesson. Students asked questions. The teacher assigned homework.\",\n",
    "        \"expected_overlap\": 1/3  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Single Sentence\",\n",
    "        \"text\": \"The sun shines brightly.\",\n",
    "        \"expected_overlap\": 0  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Complex Sentences\",\n",
    "        \"text\": \"The old car broke down on the highway. Mechanics at the garage examined the engine. They found several issues with the car's transmission.\",\n",
    "        \"expected_overlap\": 2/3  \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Mixed Content\",\n",
    "        \"text\": \"John loves apples. Apples are fruits. Bananas are also fruits. John eats bananas too.\",\n",
    "        \"expected_overlap\": 3/4  \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for test_case in test_cases:\n",
    "    doc = nlp(test_case[\"text\"])\n",
    "    result = calculate_noun_overlap(doc)\n",
    "    print(f\"Test: {test_case['name']}\")\n",
    "    print(f\"Text: {test_case['text']}\")\n",
    "    print(f\"Expected overlap: {test_case['expected_overlap']:.2f}\")\n",
    "    print(f\"Calculated overlap: {result:.2f}\")\n",
    "    print(f\"{'PASS' if abs(result - test_case['expected_overlap']) < 0.01 else 'FAIL'}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vm3_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
