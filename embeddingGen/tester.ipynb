{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 01:45:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 38.9MB/s]                    \n",
      "2024-08-24 01:45:26 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:45:27 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 01:45:27 INFO: Using device: cpu\n",
      "2024-08-24 01:45:27 INFO: Loading: tokenize\n",
      "2024-08-24 01:45:27 INFO: Loading: mwt\n",
      "2024-08-24 01:45:27 INFO: Loading: pos\n",
      "2024-08-24 01:45:27 INFO: Loading: lemma\n",
      "2024-08-24 01:45:27 INFO: Loading: constituency\n",
      "2024-08-24 01:45:27 INFO: Loading: depparse\n",
      "2024-08-24 01:45:27 INFO: Loading: sentiment\n",
      "2024-08-24 01:45:27 INFO: Loading: ner\n",
      "2024-08-24 01:45:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the doc is [\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"The\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 3,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"quick\",\n",
      "      \"lemma\": \"quick\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 4,\n",
      "      \"end_char\": 9,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"brown\",\n",
      "      \"lemma\": \"brown\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 10,\n",
      "      \"end_char\": 15,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"fox\",\n",
      "      \"lemma\": \"fox\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 16,\n",
      "      \"end_char\": 19,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"jumps\",\n",
      "      \"lemma\": \"jump\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 20,\n",
      "      \"end_char\": 25,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"over\",\n",
      "      \"lemma\": \"over\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 26,\n",
      "      \"end_char\": 30,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 31,\n",
      "      \"end_char\": 34,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"lazy\",\n",
      "      \"lemma\": \"lazy\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"xpos\": \"JJ\",\n",
      "      \"feats\": \"Degree=Pos\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"amod\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 39,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"dog\",\n",
      "      \"lemma\": \"dog\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 40,\n",
      "      \"end_char\": 43,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 43,\n",
      "      \"end_char\": 44,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"He\",\n",
      "      \"lemma\": \"he\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 45,\n",
      "      \"end_char\": 47,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"then\",\n",
      "      \"lemma\": \"then\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"feats\": \"PronType=Dem\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 48,\n",
      "      \"end_char\": 52,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"runs\",\n",
      "      \"lemma\": \"run\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 53,\n",
      "      \"end_char\": 57,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"away\",\n",
      "      \"lemma\": \"away\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 58,\n",
      "      \"end_char\": 62,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 62,\n",
      "      \"end_char\": 63,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"leaving\",\n",
      "      \"lemma\": \"leave\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBG\",\n",
      "      \"feats\": \"VerbForm=Ger\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"advcl\",\n",
      "      \"start_char\": 64,\n",
      "      \"end_char\": 71,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 72,\n",
      "      \"end_char\": 75,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"dog\",\n",
      "      \"lemma\": \"dog\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 76,\n",
      "      \"end_char\": 79,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"behind\",\n",
      "      \"lemma\": \"behind\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"compound:prt\",\n",
      "      \"start_char\": 80,\n",
      "      \"end_char\": 86,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 86,\n",
      "      \"end_char\": 87,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"After\",\n",
      "      \"lemma\": \"after\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 88,\n",
      "      \"end_char\": 93,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"that\",\n",
      "      \"lemma\": \"that\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Number=Sing|PronType=Dem\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 94,\n",
      "      \"end_char\": 98,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \",\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 98,\n",
      "      \"end_char\": 99,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 100,\n",
      "      \"end_char\": 103,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"fox\",\n",
      "      \"lemma\": \"fox\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 104,\n",
      "      \"end_char\": 107,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"went\",\n",
      "      \"lemma\": \"go\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBD\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 108,\n",
      "      \"end_char\": 112,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 113,\n",
      "      \"end_char\": 115,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"the\",\n",
      "      \"lemma\": \"the\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Def|PronType=Art\",\n",
      "      \"head\": 9,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 116,\n",
      "      \"end_char\": 119,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \"forest\",\n",
      "      \"lemma\": \"forest\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"obl\",\n",
      "      \"start_char\": 120,\n",
      "      \"end_char\": 126,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"PART\",\n",
      "      \"xpos\": \"TO\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"mark\",\n",
      "      \"start_char\": 127,\n",
      "      \"end_char\": 129,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 11,\n",
      "      \"text\": \"find\",\n",
      "      \"lemma\": \"find\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VB\",\n",
      "      \"feats\": \"VerbForm=Inf\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"advcl\",\n",
      "      \"start_char\": 130,\n",
      "      \"end_char\": 134,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 12,\n",
      "      \"text\": \"food\",\n",
      "      \"lemma\": \"food\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 11,\n",
      "      \"deprel\": \"obj\",\n",
      "      \"start_char\": 135,\n",
      "      \"end_char\": 139,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 13,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 139,\n",
      "      \"end_char\": 140,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ],\n",
      "      \"misc\": \"SpaceAfter=No\"\n",
      "    }\n",
      "  ]\n",
      "]\n",
      "Clause 1: The quick brown fox jumps over the lazy dog .\n",
      "Clause 2: He then runs away , leaving the dog behind .\n",
      "Clause 3: After that , the fox went to the forest to find food .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def extract_clauses(doc):\n",
    "\n",
    "    clauses = []\n",
    "    current_clause = []\n",
    "\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        \n",
    "        for word in sent.words:\n",
    "            if word.deprel in {'ccomp', 'xcomp', 'acl'}:  \n",
    "                if current_clause:\n",
    "                    clauses.append(current_clause)\n",
    "                    current_clause = []\n",
    "            current_clause.append(word.text)\n",
    "\n",
    "        \n",
    "        if current_clause:\n",
    "            clauses.append(current_clause)\n",
    "            current_clause = []\n",
    "\n",
    "    return clauses\n",
    "\n",
    "def main(text):\n",
    "    \n",
    "    nlp = stanza.Pipeline('en')\n",
    "\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    print('the doc is', doc)\n",
    "\n",
    "    \n",
    "    clauses = extract_clauses(doc)\n",
    "\n",
    "    \n",
    "    for i, clause in enumerate(clauses):\n",
    "        print(f\"Clause {i + 1}: {' '.join(clause)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = (\n",
    "        \"The quick brown fox jumps over the lazy dog. \"\n",
    "        \"He then runs away, leaving the dog behind. \"\n",
    "        \"After that, the fox went to the forest to find food.\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    main(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 48.0MB/s]                    \n",
      "2024-08-24 01:47:16 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:47:16 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 01:47:17 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 01:47:18 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 01:47:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 33.8MB/s]                    \n",
      "2024-08-24 01:47:18 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:47:19 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-08-24 01:47:19 INFO: Using device: cpu\n",
      "2024-08-24 01:47:19 INFO: Loading: tokenize\n",
      "2024-08-24 01:47:19 INFO: Loading: mwt\n",
      "2024-08-24 01:47:19 INFO: Loading: pos\n",
      "2024-08-24 01:47:19 INFO: Loading: lemma\n",
      "2024-08-24 01:47:19 INFO: Loading: depparse\n",
      "2024-08-24 01:47:19 INFO: Loading: ner\n",
      "2024-08-24 01:47:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stanza.models.common.doc.Document'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"In the heart of a city that never seemed to sleep, neon lights flickered like the last desperate gasps of a dying firefly. Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "print(type(doc))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clauses(doc):\n",
    "\n",
    "    clauses = []\n",
    "    current_clause = []\n",
    "\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        \n",
    "        for word in sent.words:\n",
    "            \n",
    "            if word.deprel in {'ccomp', 'xcomp', 'acl', 'advcl', 'relcl'}:\n",
    "                if current_clause:\n",
    "                    clauses.append(current_clause)\n",
    "                    current_clause = []\n",
    "            current_clause.append(word.text)\n",
    "\n",
    "        \n",
    "        if current_clause:\n",
    "            clauses.append(current_clause)\n",
    "            current_clause = []\n",
    "\n",
    "    return clauses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 41.7MB/s]                    \n",
      "2024-08-24 01:48:03 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:48:03 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 01:48:05 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 01:48:06 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 01:48:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 42.6MB/s]                    \n",
      "2024-08-24 01:48:06 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 01:48:07 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-08-24 01:48:07 INFO: Using device: cpu\n",
      "2024-08-24 01:48:07 INFO: Loading: tokenize\n",
      "2024-08-24 01:48:07 INFO: Loading: mwt\n",
      "2024-08-24 01:48:07 INFO: Loading: pos\n",
      "2024-08-24 01:48:07 INFO: Loading: lemma\n",
      "2024-08-24 01:48:07 INFO: Loading: depparse\n",
      "2024-08-24 01:48:07 INFO: Loading: ner\n",
      "2024-08-24 01:48:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1: In the heart of a city that never seemed to\n",
      "Clause 2: sleep , neon lights flickered like the last desperate gasps of a dying firefly .\n",
      "Clause 3: Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise - cracking , undead\n",
      "Clause 4: detective .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n",
    "\n",
    "\n",
    "text = \"In the heart of a city that never seemed to sleep, neon lights flickered like the last desperate gasps of a dying firefly. Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "clauses = extract_clauses(doc)\n",
    "for i, clause in enumerate(clauses):\n",
    "    print(f\"Clause {i + 1}: {' '.join(clause)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominalization(text):\n",
    "\n",
    "    results = readability.getmeasures(text, lang='en')\n",
    "    return results['word usage']['nominalization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominalization Score: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_paragraph = \"\"\"\n",
    "The government's decision to implement stricter regulations was met with widespread criticism. \n",
    "However, the justification for these regulations was primarily based on the perceived need to \n",
    "reduce environmental degradation. The opposition's argument was that such measures would \n",
    "lead to economic stagnation and job losses. Despite these concerns, the administration proceeded \n",
    "with the enforcement of the new laws, emphasizing the long-term benefits over short-term challenges.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "score = nominalization(test_paragraph)\n",
    "print(f\"Nominalization Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 105MB/s]                     \n",
      "2024-08-24 22:29:54 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 22:29:54 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 22:29:55 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 22:29:56 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 22:29:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 40.2MB/s]                    \n",
      "2024-08-24 22:29:56 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 22:29:57 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 22:29:57 INFO: Using device: cpu\n",
      "2024-08-24 22:29:57 INFO: Loading: tokenize\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: mwt\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: pos\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: lemma\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:57 INFO: Loading: constituency\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Loading: depparse\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Loading: sentiment\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Loading: ner\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-24 22:29:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Noisy Text: \n",
      "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
      "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
      "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
      "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
      "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Noisy Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_paragraph\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Reconstructed Text\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m reconstructed_text \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct_text_from_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructed Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreconstructed_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Readability Scores\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mreconstruct_text_from_doc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     23\u001b[0m word_text \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Handle cases where there's no space after the word (e.g., punctuation)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmisc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeats\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpaceAfter=No\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mwords[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfeats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmisc\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     26\u001b[0m     reconstructed_text\u001b[38;5;241m.\u001b[39mappend(word_text)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "from textstat import textstat\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "def reconstruct_text_from_doc(doc):\n",
    "\n",
    "    reconstructed_text = []\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            word_text = token.text\n",
    "            \n",
    "            if 'misc' in token.words[0].feats and 'SpaceAfter=No' in token.words[0].feats['misc']:\n",
    "                reconstructed_text.append(word_text)\n",
    "            else:\n",
    "                reconstructed_text.append(word_text + ' ')\n",
    "    return ''.join(reconstructed_text).strip()\n",
    "\n",
    "\n",
    "def flesch_reading_ease(doc):\n",
    " \n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def GFI(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.gunning_fog(text)\n",
    "\n",
    "def coleman_liau_index(doc):\n",
    " \n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.coleman_liau_index(text)\n",
    "\n",
    "def ari(doc):\n",
    " \n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.automated_readability_index(text)\n",
    "\n",
    "def dale_chall_readability_score(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.dale_chall_readability_score(text)\n",
    "\n",
    "def lix(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.lix(text)\n",
    "\n",
    "def smog_index(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.smog_index(text)\n",
    "\n",
    "def rix(doc):\n",
    "\n",
    "    text = reconstruct_text_from_doc(doc)\n",
    "    return textstat.rix(text)\n",
    "\n",
    "\n",
    "test_paragraph = \"\"\"\n",
    "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
    "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
    "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
    "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
    "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "doc = nlp(test_paragraph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original Noisy Text: {test_paragraph}\\n\")\n",
    "\n",
    "\n",
    "reconstructed_text = reconstruct_text_from_doc(doc)\n",
    "print(f\"Reconstructed Text: {reconstructed_text}\\n\")\n",
    "\n",
    "\n",
    "flesch_score = flesch_reading_ease(doc)\n",
    "print(f\"Flesch Reading Ease Score: {flesch_score:.2f}\")\n",
    "\n",
    "gfi_score = GFI(doc)\n",
    "print(f\"Gunning Fog Index: {gfi_score:.2f}\")\n",
    "\n",
    "cli_score = coleman_liau_index(doc)\n",
    "print(f\"Coleman-Liau Index: {cli_score:.2f}\")\n",
    "\n",
    "ari_score = ari(doc)\n",
    "print(f\"Automated Readability Index: {ari_score:.2f}\")\n",
    "\n",
    "dale_chall_score = dale_chall_readability_score(doc)\n",
    "print(f\"Dale-Chall Readability Score: {dale_chall_score:.2f}\")\n",
    "\n",
    "lix_score = lix(doc)\n",
    "print(f\"LIX Score: {lix_score:.2f}\")\n",
    "\n",
    "smog_score = smog_index(doc)\n",
    "print(f\"SMOG Index: {smog_score:.2f}\")\n",
    "\n",
    "rix_score = rix(doc)\n",
    "print(f\"RIX Score: {rix_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Noisy Text: \n",
      "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
      "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
      "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
      "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
      "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
      "\n",
      "\n",
      "Reconstructed Text: The government's decision to implement stricter regulations ( Figure 2.4 ) was met with widespread criticism . However , the justification for these regulations was primarily based on the perceived need to reduce environmental degradation . According to recent studies ( Smith et al. , 2020 ) , the degradation could cause economic losses up to $ 2.3 billion . The opposition's argument was that such measures would lead to economic stagnation ( see Table 3 ) , and possibly increase unemployment by 15 % . Despite these concerns , the administration proceeded with the enforcement of the new laws ( see Figure 1 ) , emphasizing the long - term benefits over short - term challenges .\n",
      "\n",
      "Flesch Reading Ease Score: 40.75\n",
      "Gunning Fog Index: 14.28\n",
      "Coleman-Liau Index: 15.01\n",
      "Automated Readability Index: 13.50\n",
      "Dale-Chall Readability Score: 11.79\n",
      "LIX Score: 43.26\n",
      "SMOG Index: 13.90\n",
      "RIX Score: 5.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def reconstruct_text_from_doc(doc):\n",
    "\n",
    "    reconstructed_text = []\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            word_text = token.text\n",
    "            \n",
    "            if token.words[0].feats and 'SpaceAfter=No' in token.words[0].feats:\n",
    "                reconstructed_text.append(word_text)\n",
    "            else:\n",
    "                reconstructed_text.append(word_text + ' ')\n",
    "    return ''.join(reconstructed_text).strip()\n",
    "\n",
    "\n",
    "test_paragraph = \"\"\"\n",
    "The government's decision to implement stricter regulations (Figure 2.4) was met with widespread criticism.\n",
    "However, the justification for these regulations was primarily based on the perceived need to reduce environmental degradation.\n",
    "According to recent studies (Smith et al., 2020), the degradation could cause economic losses up to $2.3 billion. \n",
    "The opposition's argument was that such measures would lead to economic stagnation (see Table 3), and possibly increase unemployment by 15%. \n",
    "Despite these concerns, the administration proceeded with the enforcement of the new laws (see Figure 1), emphasizing the long-term benefits over short-term challenges.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "doc = nlp(test_paragraph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original Noisy Text: {test_paragraph}\\n\")\n",
    "\n",
    "\n",
    "reconstructed_text = reconstruct_text_from_doc(doc)\n",
    "print(f\"Reconstructed Text: {reconstructed_text}\\n\")\n",
    "\n",
    "\n",
    "flesch_score = flesch_reading_ease(doc)\n",
    "print(f\"Flesch Reading Ease Score: {flesch_score:.2f}\")\n",
    "\n",
    "gfi_score = GFI(doc)\n",
    "print(f\"Gunning Fog Index: {gfi_score:.2f}\")\n",
    "\n",
    "cli_score = coleman_liau_index(doc)\n",
    "print(f\"Coleman-Liau Index: {cli_score:.2f}\")\n",
    "\n",
    "ari_score = ari(doc)\n",
    "print(f\"Automated Readability Index: {ari_score:.2f}\")\n",
    "\n",
    "dale_chall_score = dale_chall_readability_score(doc)\n",
    "print(f\"Dale-Chall Readability Score: {dale_chall_score:.2f}\")\n",
    "\n",
    "lix_score = lix(doc)\n",
    "print(f\"LIX Score: {lix_score:.2f}\")\n",
    "\n",
    "smog_score = smog_index(doc)\n",
    "print(f\"SMOG Index: {smog_score:.2f}\")\n",
    "\n",
    "rix_score = rix(doc)\n",
    "print(f\"RIX Score: {rix_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 23:08:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 88.8MB/s]                    \n",
      "2024-08-24 23:08:43 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:08:44 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:08:44 INFO: Using device: cpu\n",
      "2024-08-24 23:08:44 INFO: Loading: tokenize\n",
      "2024-08-24 23:08:44 INFO: Loading: mwt\n",
      "2024-08-24 23:08:44 INFO: Loading: pos\n",
      "2024-08-24 23:08:44 INFO: Loading: lemma\n",
      "2024-08-24 23:08:44 INFO: Loading: constituency\n",
      "2024-08-24 23:08:44 INFO: Loading: depparse\n",
      "2024-08-24 23:08:45 INFO: Loading: sentiment\n",
      "2024-08-24 23:08:45 INFO: Loading: ner\n",
      "2024-08-24 23:08:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frazier Depth for each sentence:\n",
      "Sentence 1: 7\n",
      "Sentence 2: 5\n",
      "Sentence 3: 8\n",
      "Sentence 4: 5\n",
      "Sentence 5: 5\n",
      "Sentence 6: 7\n",
      "\n",
      "Average Frazier Depth for the entire text: 6.17\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "def calculate_frazier_depth(sentence):\n",
    "    \"\"\"\n",
    "    Computes the Frazier Depth for a given sentence based on its dependency parse.\n",
    "    Parameters:\n",
    "    sentence (stanza.Sentence): A Stanza Sentence object.\n",
    "    Returns:\n",
    "    int: The depth of the syntactic structure.\n",
    "    \"\"\"\n",
    "    def build_tree(words):\n",
    "        tree = {word.id: [] for word in words}\n",
    "        for word in words:\n",
    "            if word.head != 0:  \n",
    "                tree[word.head].append(word.id)\n",
    "        return tree\n",
    "\n",
    "    def depth(node_id, tree, current_depth):\n",
    "        max_depth = current_depth\n",
    "        for child_id in tree[node_id]:\n",
    "            max_depth = max(max_depth, depth(child_id, tree, current_depth + 1))\n",
    "        return max_depth\n",
    "\n",
    "    \n",
    "    dependency_tree = build_tree(sentence.words)\n",
    "    \n",
    "    \n",
    "    root_id = next(word.id for word in sentence.words if word.head == 0)\n",
    "    \n",
    "    \n",
    "    return depth(root_id, dependency_tree, 0)\n",
    "\n",
    "def frazier_depth(doc):\n",
    "    \"\"\"\n",
    "    Computes the average Frazier Depth for all sentences in a document.\n",
    "    Parameters:\n",
    "    doc (stanza.Document): A Stanza Document object.\n",
    "    Returns:\n",
    "    float: The average Frazier Depth for the document.\n",
    "    \"\"\"\n",
    "    depths = []\n",
    "    for sentence in doc.sentences:\n",
    "        depths.append(calculate_frazier_depth(sentence))\n",
    "    \n",
    "    if not depths:\n",
    "        return 0.0\n",
    "    \n",
    "    return sum(depths) / len(depths)\n",
    "\n",
    "def test_frazier_depth():\n",
    "    \n",
    "    nlp = stanza.Pipeline('en')\n",
    "\n",
    "    text = \"\"\"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"You know,\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent street vendor offering 'mystical' hot dogs, \"if I had a penny for every time someone tried to sell me something magical, I'd be richer than the most miserly dragon you can imagine.\" Valkyrie shot him a look that combined exasperation with a hint of amusement. \"And if I had a nickel for every time you made a joke that wasn't terrible, I'd still be broke, but at least I'd have a better sense of humor.\" Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark and absurdly dangerous adventure lay ahead.\"\"\"\n",
    "\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    \n",
    "    print(\"Frazier Depth for each sentence:\")\n",
    "    for i, sentence in enumerate(doc.sentences, 1):\n",
    "        depth = calculate_frazier_depth(sentence)\n",
    "        print(f\"Sentence {i}: {depth}\")\n",
    "\n",
    "    \n",
    "    avg_depth = frazier_depth(doc)\n",
    "    print(f\"\\nAverage Frazier Depth for the entire text: {avg_depth:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_frazier_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: emoji in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.12.1)\n",
      "Requirement already satisfied: numpy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (5.27.3)\n",
      "Requirement already satisfied: requests in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: toml in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (4.66.5)\n",
      "Requirement already satisfied: filelock in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 30.4MB/s]                    \n",
      "2024-08-24 23:39:11 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:39:14 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 23:39:15 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 23:39:17 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 23:39:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 33.0MB/s]                    \n",
      "2024-08-24 23:39:17 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:39:18 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:39:18 INFO: Using device: cpu\n",
      "2024-08-24 23:39:18 INFO: Loading: tokenize\n",
      "2024-08-24 23:39:18 INFO: Loading: mwt\n",
      "2024-08-24 23:39:18 INFO: Loading: pos\n",
      "2024-08-24 23:39:18 INFO: Loading: lemma\n",
      "2024-08-24 23:39:18 INFO: Loading: constituency\n",
      "2024-08-24 23:39:18 INFO: Loading: depparse\n",
      "2024-08-24 23:39:19 INFO: Loading: sentiment\n",
      "2024-08-24 23:39:19 INFO: Loading: ner\n",
      "2024-08-24 23:39:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 1.0000\n",
      "expletive_inversion: 0.0000\n",
      "emphatic_inversion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install stanza\n",
    "\n",
    "\n",
    "import stanza\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "\n",
    "    \n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "        elif word.deprel == 'root':\n",
    "            verb_positions.append(i)\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos:\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        for expl_pos in expletive_positions:\n",
    "            for verb_pos in verb_positions:\n",
    "                if expl_pos < verb_pos:\n",
    "                    return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and subject_positions:\n",
    "        for emph_pos in emphatic_positions:\n",
    "            for subj_pos in subject_positions:\n",
    "                if emph_pos < subj_pos:\n",
    "                    return \"emphatic_inversion\"\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    \"\"\"\n",
    "    Computes the normalized frequency of different types of sentence inversions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the normalized frequency of each type of inversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, \"\n",
    "    \"desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of \"\n",
    "    \"nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind \"\n",
    "    \"him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"\n",
    "    \"\\\"You know,\\\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent \"\n",
    "    \"street vendor offering mystical hot dogs, \\\"if I had a penny for every time someone tried to sell me \"\n",
    "    \"something magical, Id be richer than the most miserly dragon you can imagine.\\\" Valkyrie shot him a look \"\n",
    "    \"that combined exasperation with a hint of amusement. \\\"And if I had a nickel for every time you made a \"\n",
    "    \"joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\\\" \"\n",
    "    \"Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark \"\n",
    "    \"and absurdly dangerous adventure lay ahead. There is no greater danger than underestimating the enemy, \"\n",
    "    \"especially when that enemy lurks in the shadows, biding its time, waiting to strike. There in the darkness, \"\n",
    "    \"waiting patiently, lies a threat that no amount of bravado can overcome. Here comes the night, sweeping \"\n",
    "    \"across the city like a veil, concealing the dangers that await the unprepared.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"Inversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: emoji in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.12.1)\n",
      "Requirement already satisfied: numpy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (5.27.3)\n",
      "Requirement already satisfied: requests in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: toml in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from stanza) (4.66.5)\n",
      "Requirement already satisfied: filelock in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from requests->stanza) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 29.2MB/s]                    \n",
      "2024-08-24 23:40:25 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:40:25 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 23:40:26 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 23:40:28 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 23:40:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 39.0MB/s]                    \n",
      "2024-08-24 23:40:28 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:40:29 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:40:29 INFO: Using device: cpu\n",
      "2024-08-24 23:40:29 INFO: Loading: tokenize\n",
      "2024-08-24 23:40:29 INFO: Loading: mwt\n",
      "2024-08-24 23:40:29 INFO: Loading: pos\n",
      "2024-08-24 23:40:29 INFO: Loading: lemma\n",
      "2024-08-24 23:40:29 INFO: Loading: constituency\n",
      "2024-08-24 23:40:29 INFO: Loading: depparse\n",
      "2024-08-24 23:40:30 INFO: Loading: sentiment\n",
      "2024-08-24 23:40:30 INFO: Loading: ner\n",
      "2024-08-24 23:40:30 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'In the heart of a city that never seemed to sleep, where neon lights flickered like the last, desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.'\n",
      "Word: In, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: heart, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 6\n",
      "Word: never, DepRel: advmod, UPOS: ADV\n",
      "Word: seemed, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sleep, DepRel: xcomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: where, DepRel: advmod, UPOS: ADV\n",
      "Word: neon, DepRel: compound, UPOS: NOUN\n",
      "Word: lights, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: flickered, DepRel: advcl, UPOS: VERB\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: last, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: desperate, DepRel: amod, UPOS: ADJ\n",
      "Word: gasps, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: dying, DepRel: amod, UPOS: NOUN\n",
      "Word: firefly, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 27\n",
      "Word: Pleasant, DepRel: flat, UPOS: PROPN\n",
      "Word: strolled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 29\n",
      "Word: down, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: alley, DepRel: obl, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: kind, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: nonchalance, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 38\n",
      "Word: only, DepRel: advmod, UPOS: ADV\n",
      "Word: comes, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: from, DepRel: mark, UPOS: SCONJ\n",
      "Word: being, DepRel: cop, UPOS: AUX\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: wise, DepRel: compound, UPOS: ADJ\n",
      "Word: -, DepRel: punct, UPOS: PUNCT\n",
      "Word: cracking, DepRel: amod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: undead, DepRel: amod, UPOS: ADJ\n",
      "Word: detective, DepRel: advcl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'His overcoat flapped behind him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble.'\n",
      "Word: His, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: overcoat, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 1\n",
      "Word: flapped, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 2\n",
      "Word: behind, DepRel: case, UPOS: ADP\n",
      "Word: him, DepRel: obl, UPOS: PRON\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: ragged, DepRel: amod, UPOS: ADJ\n",
      "Word: flag, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: rebellion, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: while, DepRel: mark, UPOS: SCONJ\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: eyes, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sharp, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: knowing, DepRel: conj, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: scanned, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: trouble, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"You know,\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent street vendor offering mystical hot dogs, \"if I had a penny for every time someone tried to sell me something magical, Id be richer than the most miserly dragon you can imagine.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: You, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 1\n",
      "Word: know, DepRel: ccomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: said, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 6\n",
      "Word: to, DepRel: case, UPOS: ADP\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: companion, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Valkyrie, DepRel: appos, UPOS: PROPN\n",
      "Word: Cain, DepRel: flat, UPOS: PROPN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: who, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: busy, DepRel: acl:relcl, UPOS: ADJ\n",
      "Word: fending, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: particularly, DepRel: advmod, UPOS: ADV\n",
      "Word: persistent, DepRel: amod, UPOS: ADJ\n",
      "Word: street, DepRel: compound, UPOS: NOUN\n",
      "Word: vendor, DepRel: obj, UPOS: NOUN\n",
      "Word: offering, DepRel: acl, UPOS: VERB\n",
      "Word: , DepRel: punct, UPOS: PUNCT\n",
      "Word: mystical, DepRel: nmod:poss, UPOS: ADJ\n",
      "Word: , DepRel: case, UPOS: PART\n",
      "Word: hot, DepRel: amod, UPOS: ADJ\n",
      "Word: dogs, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 33\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: penny, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: someone, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 40\n",
      "Word: tried, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sell, DepRel: xcomp, UPOS: VERB\n",
      "Word: me, DepRel: iobj, UPOS: PRON\n",
      "Word: something, DepRel: obj, UPOS: PRON\n",
      "Word: magical, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 48\n",
      "Word: d, DepRel: aux, UPOS: AUX\n",
      "Word: be, DepRel: cop, UPOS: AUX\n",
      "Word: richer, DepRel: parataxis, UPOS: ADJ\n",
      "Word: than, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: most, DepRel: advmod, UPOS: ADV\n",
      "Word: miserly, DepRel: amod, UPOS: ADJ\n",
      "Word: dragon, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 57\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: imagine, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Valkyrie shot him a look that combined exasperation with a hint of amusement.'\n",
      "Word: Valkyrie, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: shot, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: him, DepRel: iobj, UPOS: PRON\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: look, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: combined, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: exasperation, DepRel: obj, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: hint, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: amusement, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"And if I had a nickel for every time you made a joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: And, DepRel: cc, UPOS: CCONJ\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 3\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: nickel, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 10\n",
      "Word: made, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: joke, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: nt, DepRel: advmod, UPOS: PART\n",
      "Word: terrible, DepRel: parataxis, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 19\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: still, DepRel: advmod, UPOS: ADV\n",
      "Word: be, DepRel: aux, UPOS: AUX\n",
      "Word: broke, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 23\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: but, DepRel: cc, UPOS: CCONJ\n",
      "Word: at, DepRel: advmod, UPOS: ADP\n",
      "Word: least, DepRel: fixed, UPOS: ADJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 28\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: have, DepRel: conj, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: better, DepRel: amod, UPOS: ADJ\n",
      "Word: sense, DepRel: obj, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: humor, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark and absurdly dangerous adventure lay ahead.'\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: chuckled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: sound, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: echoing, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: graffiti-, DepRel: advmod, UPOS: ADJ\n",
      "Word: covered, DepRel: amod, UPOS: VERB\n",
      "Word: walls, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: as, DepRel: mark, UPOS: SCONJ\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 13\n",
      "Word: prepared, DepRel: advcl, UPOS: VERB\n",
      "Word: for, DepRel: mark, UPOS: ADP\n",
      "Word: whatever, DepRel: det, UPOS: DET\n",
      "Word: dark, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: absurdly, DepRel: advmod, UPOS: ADV\n",
      "Word: dangerous, DepRel: conj, UPOS: ADJ\n",
      "Word: adventure, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 21\n",
      "Word: lay, DepRel: advcl, UPOS: VERB\n",
      "Word: ahead, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is no greater danger than underestimating the enemy, especially when that enemy lurks in the shadows, biding its time, waiting to strike.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: greater, DepRel: amod, UPOS: ADJ\n",
      "Word: danger, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: than, DepRel: mark, UPOS: SCONJ\n",
      "Word: underestimating, DepRel: acl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: especially, DepRel: advmod, UPOS: ADV\n",
      "Word: when, DepRel: advmod, UPOS: ADV\n",
      "Word: that, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: lurks, DepRel: advcl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: biding, DepRel: advcl, UPOS: VERB\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: time, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: strike, DepRel: xcomp, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There in the darkness, waiting patiently, lies a threat that no amount of bravado can overcome.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: darkness, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: patiently, DepRel: advmod, UPOS: ADV\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 8\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: threat, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: mark, UPOS: SCONJ\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: amount, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: bravado, DepRel: nmod, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: overcome, DepRel: acl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the night, sweeping across the city like a veil, concealing the dangers that await the unprepared.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: night, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sweeping, DepRel: advcl, UPOS: VERB\n",
      "Word: across, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: obl, UPOS: NOUN\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: veil, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: concealing, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: dangers, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 16\n",
      "Word: await, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: unprepared, DepRel: obj, UPOS: ADJ\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 1.0000\n",
      "expletive_inversion: 0.0000\n",
      "emphatic_inversion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install stanza\n",
    "\n",
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "\n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel == 'root':\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb (root) found at position {i}\")\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        for expl_pos in expletive_positions:\n",
    "            for verb_pos in verb_positions:\n",
    "                if expl_pos < verb_pos:\n",
    "                    print(\" -> Expletive inversion detected\")\n",
    "                    return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and subject_positions:\n",
    "        for emph_pos in emphatic_positions:\n",
    "            for subj_pos in subject_positions:\n",
    "                if emph_pos < subj_pos:\n",
    "                    print(\" -> Emphatic inversion detected\")\n",
    "                    return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    \"\"\"\n",
    "    Computes the normalized frequency of different types of sentence inversions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the normalized frequency of each type of inversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, \"\n",
    "    \"desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of \"\n",
    "    \"nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind \"\n",
    "    \"him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"\n",
    "    \"\\\"You know,\\\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent \"\n",
    "    \"street vendor offering mystical hot dogs, \\\"if I had a penny for every time someone tried to sell me \"\n",
    "    \"something magical, Id be richer than the most miserly dragon you can imagine.\\\" Valkyrie shot him a look \"\n",
    "    \"that combined exasperation with a hint of amusement. \\\"And if I had a nickel for every time you made a \"\n",
    "    \"joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\\\" \"\n",
    "    \"Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark \"\n",
    "    \"and absurdly dangerous adventure lay ahead. There is no greater danger than underestimating the enemy, \"\n",
    "    \"especially when that enemy lurks in the shadows, biding its time, waiting to strike. There in the darkness, \"\n",
    "    \"waiting patiently, lies a threat that no amount of bravado can overcome. Here comes the night, sweeping \"\n",
    "    \"across the city like a veil, concealing the dangers that await the unprepared.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.5MB/s]                    \n",
      "2024-08-24 23:44:52 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:44:52 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-24 23:44:53 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-24 23:44:55 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-24 23:44:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 26.0MB/s]                    \n",
      "2024-08-24 23:44:55 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-24 23:44:56 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-24 23:44:56 INFO: Using device: cpu\n",
      "2024-08-24 23:44:56 INFO: Loading: tokenize\n",
      "2024-08-24 23:44:56 INFO: Loading: mwt\n",
      "2024-08-24 23:44:56 INFO: Loading: pos\n",
      "2024-08-24 23:44:56 INFO: Loading: lemma\n",
      "2024-08-24 23:44:56 INFO: Loading: constituency\n",
      "2024-08-24 23:44:57 INFO: Loading: depparse\n",
      "2024-08-24 23:44:57 INFO: Loading: sentiment\n",
      "2024-08-24 23:44:57 INFO: Loading: ner\n",
      "2024-08-24 23:44:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'In the heart of a city that never seemed to sleep, where neon lights flickered like the last, desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of nonchalance that only comes from being a wise-cracking, undead detective.'\n",
      "Word: In, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: heart, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 6\n",
      "Word: never, DepRel: advmod, UPOS: ADV\n",
      "Word: seemed, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sleep, DepRel: xcomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: where, DepRel: advmod, UPOS: ADV\n",
      "Word: neon, DepRel: compound, UPOS: NOUN\n",
      "Word: lights, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: flickered, DepRel: advcl, UPOS: VERB\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: last, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: desperate, DepRel: amod, UPOS: ADJ\n",
      "Word: gasps, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: dying, DepRel: amod, UPOS: NOUN\n",
      "Word: firefly, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 27\n",
      "Word: Pleasant, DepRel: flat, UPOS: PROPN\n",
      "Word: strolled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 29\n",
      "Word: down, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: alley, DepRel: obl, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: kind, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: nonchalance, DepRel: nmod, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 38\n",
      "Word: only, DepRel: advmod, UPOS: ADV\n",
      "Word: comes, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: from, DepRel: mark, UPOS: SCONJ\n",
      "Word: being, DepRel: cop, UPOS: AUX\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: wise, DepRel: compound, UPOS: ADJ\n",
      "Word: -, DepRel: punct, UPOS: PUNCT\n",
      "Word: cracking, DepRel: amod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: undead, DepRel: amod, UPOS: ADJ\n",
      "Word: detective, DepRel: advcl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'His overcoat flapped behind him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble.'\n",
      "Word: His, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: overcoat, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 1\n",
      "Word: flapped, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 2\n",
      "Word: behind, DepRel: case, UPOS: ADP\n",
      "Word: him, DepRel: obl, UPOS: PRON\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: ragged, DepRel: amod, UPOS: ADJ\n",
      "Word: flag, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: rebellion, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: while, DepRel: mark, UPOS: SCONJ\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: eyes, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 14\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sharp, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: knowing, DepRel: conj, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: scanned, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: trouble, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"You know,\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent street vendor offering mystical hot dogs, \"if I had a penny for every time someone tried to sell me something magical, Id be richer than the most miserly dragon you can imagine.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: You, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 1\n",
      "Word: know, DepRel: ccomp, UPOS: VERB\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: said, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 6\n",
      "Word: to, DepRel: case, UPOS: ADP\n",
      "Word: his, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: companion, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: Valkyrie, DepRel: appos, UPOS: PROPN\n",
      "Word: Cain, DepRel: flat, UPOS: PROPN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: who, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: busy, DepRel: acl:relcl, UPOS: ADJ\n",
      "Word: fending, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: particularly, DepRel: advmod, UPOS: ADV\n",
      "Word: persistent, DepRel: amod, UPOS: ADJ\n",
      "Word: street, DepRel: compound, UPOS: NOUN\n",
      "Word: vendor, DepRel: obj, UPOS: NOUN\n",
      "Word: offering, DepRel: acl, UPOS: VERB\n",
      "Word: , DepRel: punct, UPOS: PUNCT\n",
      "Word: mystical, DepRel: nmod:poss, UPOS: ADJ\n",
      "Word: , DepRel: case, UPOS: PART\n",
      "Word: hot, DepRel: amod, UPOS: ADJ\n",
      "Word: dogs, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 33\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: penny, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: someone, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 40\n",
      "Word: tried, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: sell, DepRel: xcomp, UPOS: VERB\n",
      "Word: me, DepRel: iobj, UPOS: PRON\n",
      "Word: something, DepRel: obj, UPOS: PRON\n",
      "Word: magical, DepRel: amod, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 48\n",
      "Word: d, DepRel: aux, UPOS: AUX\n",
      "Word: be, DepRel: cop, UPOS: AUX\n",
      "Word: richer, DepRel: parataxis, UPOS: ADJ\n",
      "Word: than, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: most, DepRel: advmod, UPOS: ADV\n",
      "Word: miserly, DepRel: amod, UPOS: ADJ\n",
      "Word: dragon, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 57\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: imagine, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Valkyrie shot him a look that combined exasperation with a hint of amusement.'\n",
      "Word: Valkyrie, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: shot, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: him, DepRel: iobj, UPOS: PRON\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: look, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 5\n",
      "Word: combined, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: exasperation, DepRel: obj, UPOS: NOUN\n",
      "Word: with, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: hint, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: amusement, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: '\"And if I had a nickel for every time you made a joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\"'\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      "Word: And, DepRel: cc, UPOS: CCONJ\n",
      "Word: if, DepRel: mark, UPOS: SCONJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 3\n",
      "Word: had, DepRel: advcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: nickel, DepRel: obj, UPOS: NOUN\n",
      "Word: for, DepRel: case, UPOS: ADP\n",
      "Word: every, DepRel: det, UPOS: DET\n",
      "Word: time, DepRel: obl, UPOS: NOUN\n",
      "Word: you, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 10\n",
      "Word: made, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: joke, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 14\n",
      "Word: was, DepRel: cop, UPOS: AUX\n",
      "Word: nt, DepRel: advmod, UPOS: PART\n",
      "Word: terrible, DepRel: parataxis, UPOS: ADJ\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 19\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: still, DepRel: advmod, UPOS: ADV\n",
      "Word: be, DepRel: aux, UPOS: AUX\n",
      "Word: broke, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 23\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: but, DepRel: cc, UPOS: CCONJ\n",
      "Word: at, DepRel: advmod, UPOS: ADP\n",
      "Word: least, DepRel: fixed, UPOS: ADJ\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 28\n",
      "Word: 'd, DepRel: aux, UPOS: AUX\n",
      "Word: have, DepRel: conj, UPOS: VERB\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: better, DepRel: amod, UPOS: ADJ\n",
      "Word: sense, DepRel: obj, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: humor, DepRel: nmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      "Word: \", DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark and absurdly dangerous adventure lay ahead.'\n",
      "Word: Skulduggery, DepRel: nsubj, UPOS: PROPN\n",
      " - Subject found at position 0\n",
      "Word: chuckled, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: sound, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: echoing, DepRel: advcl, UPOS: VERB\n",
      "Word: off, DepRel: compound:prt, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: graffiti-, DepRel: advmod, UPOS: ADJ\n",
      "Word: covered, DepRel: amod, UPOS: VERB\n",
      "Word: walls, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: as, DepRel: mark, UPOS: SCONJ\n",
      "Word: he, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 13\n",
      "Word: prepared, DepRel: advcl, UPOS: VERB\n",
      "Word: for, DepRel: mark, UPOS: ADP\n",
      "Word: whatever, DepRel: det, UPOS: DET\n",
      "Word: dark, DepRel: amod, UPOS: ADJ\n",
      "Word: and, DepRel: cc, UPOS: CCONJ\n",
      "Word: absurdly, DepRel: advmod, UPOS: ADV\n",
      "Word: dangerous, DepRel: conj, UPOS: ADJ\n",
      "Word: adventure, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 21\n",
      "Word: lay, DepRel: advcl, UPOS: VERB\n",
      "Word: ahead, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is no greater danger than underestimating the enemy, especially when that enemy lurks in the shadows, biding its time, waiting to strike.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: greater, DepRel: amod, UPOS: ADJ\n",
      "Word: danger, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: than, DepRel: mark, UPOS: SCONJ\n",
      "Word: underestimating, DepRel: acl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: especially, DepRel: advmod, UPOS: ADV\n",
      "Word: when, DepRel: advmod, UPOS: ADV\n",
      "Word: that, DepRel: det, UPOS: DET\n",
      "Word: enemy, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: lurks, DepRel: advcl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: shadows, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: biding, DepRel: advcl, UPOS: VERB\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: time, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: to, DepRel: mark, UPOS: PART\n",
      "Word: strike, DepRel: xcomp, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There in the darkness, waiting patiently, lies a threat that no amount of bravado can overcome.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: darkness, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: waiting, DepRel: advcl, UPOS: VERB\n",
      "Word: patiently, DepRel: advmod, UPOS: ADV\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 8\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: threat, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: mark, UPOS: SCONJ\n",
      "Word: no, DepRel: det, UPOS: DET\n",
      "Word: amount, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 13\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: bravado, DepRel: nmod, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      "Word: overcome, DepRel: acl, UPOS: VERB\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the night, sweeping across the city like a veil, concealing the dangers that await the unprepared.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: night, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: sweeping, DepRel: advcl, UPOS: VERB\n",
      "Word: across, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: obl, UPOS: NOUN\n",
      "Word: like, DepRel: case, UPOS: ADP\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: veil, DepRel: obl, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: concealing, DepRel: advcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: dangers, DepRel: obj, UPOS: NOUN\n",
      "Word: that, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 16\n",
      "Word: await, DepRel: acl:relcl, UPOS: VERB\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: unprepared, DepRel: obj, UPOS: ADJ\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 1.0000\n",
      "expletive_inversion: 0.0000\n",
      "emphatic_inversion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "\n",
    "    \n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel == 'root':\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb (root) found at position {i}\")\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        for expl_pos in expletive_positions:\n",
    "            for verb_pos in verb_positions:\n",
    "                if expl_pos < verb_pos:\n",
    "                    print(\" -> Expletive inversion detected\")\n",
    "                    return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and subject_positions:\n",
    "        for emph_pos in emphatic_positions:\n",
    "            for subj_pos in subject_positions:\n",
    "                if emph_pos < subj_pos:\n",
    "                    print(\" -> Emphatic inversion detected\")\n",
    "                    return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"In the heart of a city that never seemed to sleep, where neon lights flickered like the last, \"\n",
    "    \"desperate gasps of a dying firefly, Skulduggery Pleasant strolled down the alley with the kind of \"\n",
    "    \"nonchalance that only comes from being a wise-cracking, undead detective. His overcoat flapped behind \"\n",
    "    \"him like a ragged flag of rebellion, while his eyes, sharp and knowing, scanned the shadows for trouble. \"\n",
    "    \"\\\"You know,\\\" he said to his companion, Valkyrie Cain, who was busy fending off a particularly persistent \"\n",
    "    \"street vendor offering mystical hot dogs, \\\"if I had a penny for every time someone tried to sell me \"\n",
    "    \"something magical, Id be richer than the most miserly dragon you can imagine.\\\" Valkyrie shot him a look \"\n",
    "    \"that combined exasperation with a hint of amusement. \\\"And if I had a nickel for every time you made a \"\n",
    "    \"joke that wasnt terrible, I'd still be broke, but at least I'd have a better sense of humor.\\\" \"\n",
    "    \"Skulduggery chuckled, the sound echoing off the graffiti-covered walls, as he prepared for whatever dark \"\n",
    "    \"and absurdly dangerous adventure lay ahead. There is no greater danger than underestimating the enemy, \"\n",
    "    \"especially when that enemy lurks in the shadows, biding its time, waiting to strike. There in the darkness, \"\n",
    "    \"waiting patiently, lies a threat that no amount of bravado can overcome. Here comes the night, sweeping \"\n",
    "    \"across the city like a veil, concealing the dangers that await the unprepared.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 31.0MB/s]                    \n",
      "2024-08-25 13:56:51 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:56:51 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 13:56:52 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 13:56:54 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 13:56:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 33.8MB/s]                    \n",
      "2024-08-25 13:56:54 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:56:55 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 13:56:55 INFO: Using device: cpu\n",
      "2024-08-25 13:56:55 INFO: Loading: tokenize\n",
      "2024-08-25 13:56:55 INFO: Loading: mwt\n",
      "2024-08-25 13:56:55 INFO: Loading: pos\n",
      "2024-08-25 13:56:55 INFO: Loading: lemma\n",
      "2024-08-25 13:56:55 INFO: Loading: constituency\n",
      "2024-08-25 13:56:55 INFO: Loading: depparse\n",
      "2024-08-25 13:56:56 INFO: Loading: sentiment\n",
      "2024-08-25 13:56:56 INFO: Loading: ner\n",
      "2024-08-25 13:56:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 47.3MB/s]                    \n",
      "2024-08-25 13:56:57 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:56:57 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 13:56:58 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 13:56:59 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 13:56:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 35.8MB/s]                    \n",
      "2024-08-25 13:56:59 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:57:00 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 13:57:00 INFO: Using device: cpu\n",
      "2024-08-25 13:57:00 INFO: Loading: tokenize\n",
      "2024-08-25 13:57:00 INFO: Loading: mwt\n",
      "2024-08-25 13:57:00 INFO: Loading: pos\n",
      "2024-08-25 13:57:00 INFO: Loading: lemma\n",
      "2024-08-25 13:57:01 INFO: Loading: constituency\n",
      "2024-08-25 13:57:01 INFO: Loading: depparse\n",
      "2024-08-25 13:57:01 INFO: Loading: sentiment\n",
      "2024-08-25 13:57:01 INFO: Loading: ner\n",
      "2024-08-25 13:57:02 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 57.2MB/s]                    \n",
      "2024-08-25 13:57:02 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:57:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 13:57:04 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 13:57:05 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 13:57:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.9MB/s]                    \n",
      "2024-08-25 13:57:05 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 13:57:06 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 13:57:06 INFO: Using device: cpu\n",
      "2024-08-25 13:57:06 INFO: Loading: tokenize\n",
      "2024-08-25 13:57:06 INFO: Loading: mwt\n",
      "2024-08-25 13:57:06 INFO: Loading: pos\n",
      "2024-08-25 13:57:06 INFO: Loading: lemma\n",
      "2024-08-25 13:57:06 INFO: Loading: constituency\n",
      "2024-08-25 13:57:06 INFO: Loading: depparse\n",
      "2024-08-25 13:57:06 INFO: Loading: sentiment\n",
      "2024-08-25 13:57:07 INFO: Loading: ner\n",
      "2024-08-25 13:57:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb (root or auxiliary) found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb (root or auxiliary) found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "  \n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel in ('root', 'aux', 'aux:pass'):\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb (root or auxiliary) found at position {i}\")\n",
    "        elif word.deprel == 'expl':  \n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.deprel == 'advmod' and word.text.lower() in ('here', 'there'):  \n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "\n",
    "    \n",
    "    for subj_pos in subject_positions:\n",
    "        for verb_pos in verb_positions:\n",
    "            if subj_pos > verb_pos and not expletive_positions and not emphatic_positions:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        print(\" -> Expletive inversion detected\")\n",
    "        return \"expletive_inversion\"\n",
    "\n",
    "    if emphatic_positions:\n",
    "        print(\" -> Emphatic inversion detected\")\n",
    "        return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    \"\"\"\n",
    "    Computes the normalized frequency of different types of sentence inversions in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        doc (stanza.Document): The Stanza Document object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the normalized frequency of each type of inversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        \n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    \n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 10.9MB/s]                    \n",
      "2024-08-25 21:16:55 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 21:16:55 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-25 21:16:56 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-25 21:16:57 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-25 21:16:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 26.2MB/s]                    \n",
      "2024-08-25 21:16:57 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-25 21:16:58 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-25 21:16:58 INFO: Using device: cpu\n",
      "2024-08-25 21:16:58 INFO: Loading: tokenize\n",
      "2024-08-25 21:16:58 INFO: Loading: mwt\n",
      "2024-08-25 21:16:58 INFO: Loading: pos\n",
      "2024-08-25 21:16:58 INFO: Loading: lemma\n",
      "2024-08-25 21:16:58 INFO: Loading: constituency\n",
      "2024-08-25 21:16:58 INFO: Loading: depparse\n",
      "2024-08-25 21:16:59 INFO: Loading: sentiment\n",
      "2024-08-25 21:16:59 INFO: Loading: ner\n",
      "2024-08-25 21:16:59 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 6\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb)\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      " - Verb found at position 8\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> No inversion detected\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb)\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      " - Verb found at position 7\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.2857\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "    adv_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.upos in ('VERB', 'AUX'):\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb found at position {i}\")\n",
    "        elif word.deprel == 'expl':\n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.text.lower() in ('here', 'there') and word.deprel == 'advmod':\n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "        elif word.upos == 'ADV':\n",
    "            adv_positions.append(i)\n",
    "            print(f\" - Adverb found at position {i}\")\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        if min(expletive_positions) < min(verb_positions):\n",
    "            print(\" -> Expletive inversion detected\")\n",
    "            return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and verb_positions:\n",
    "        if min(emphatic_positions) < min(verb_positions):\n",
    "            print(\" -> Emphatic inversion detected\")\n",
    "            return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    if subject_positions and verb_positions:\n",
    "        if min(subject_positions) > min(verb_positions):\n",
    "            \n",
    "            if adv_positions and min(adv_positions) == 0:\n",
    "                print(\" -> Classic inversion detected (with initial adverb)\")\n",
    "                return \"classic_inversion\"\n",
    "            elif not adv_positions:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 123MB/s]                     \n",
      "2024-08-26 08:06:20 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:06:20 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:06:21 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:06:22 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:06:22 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 52.0MB/s]                    \n",
      "2024-08-26 08:06:22 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:06:23 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:06:23 INFO: Using device: cpu\n",
      "2024-08-26 08:06:23 INFO: Loading: tokenize\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: mwt\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: pos\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: lemma\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:23 INFO: Loading: constituency\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Loading: depparse\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Loading: sentiment\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Loading: ner\n",
      "/Users/sean/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-26 08:06:24 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sentence: 'Never have I seen such beauty before.'\n",
      "Word: Never, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: have, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 1\n",
      "Word: I, DepRel: nsubj, UPOS: PRON\n",
      " - Subject found at position 2\n",
      "Word: seen, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: such, DepRel: amod, UPOS: ADJ\n",
      "Word: beauty, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 5\n",
      "Word: before, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 6\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb or prepositional phrase)\n",
      "\n",
      "Processing sentence: 'There is a certain magic in the air tonight.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: is, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: certain, DepRel: amod, UPOS: ADJ\n",
      "Word: magic, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: air, DepRel: obl, UPOS: NOUN\n",
      "Word: tonight, DepRel: obl:tmod, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here comes the rain, soaking everything in its path.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: comes, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 1\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: rain, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 3\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: soaking, DepRel: advcl, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: everything, DepRel: obj, UPOS: PRON\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: path, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Processing sentence: 'Under the bridge stood a lone figure, shrouded in mystery.'\n",
      "Word: Under, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: bridge, DepRel: obl, UPOS: NOUN\n",
      "Word: stood, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 3\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: lone, DepRel: amod, UPOS: ADJ\n",
      "Word: figure, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 6\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: shrouded, DepRel: acl, UPOS: VERB\n",
      " - Verb found at position 8\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: mystery, DepRel: obl, UPOS: NOUN\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb or prepositional phrase)\n",
      "\n",
      "Processing sentence: 'Only by night does the city reveal its true face.'\n",
      "Word: Only, DepRel: advmod, UPOS: ADV\n",
      " - Adverb found at position 0\n",
      "Word: by, DepRel: case, UPOS: ADP\n",
      "Word: night, DepRel: obl, UPOS: NOUN\n",
      "Word: does, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 3\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: city, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 5\n",
      "Word: reveal, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 6\n",
      "Word: its, DepRel: nmod:poss, UPOS: PRON\n",
      "Word: true, DepRel: amod, UPOS: ADJ\n",
      "Word: face, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 9\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Classic inversion detected (with initial adverb or prepositional phrase)\n",
      "\n",
      "Processing sentence: 'There beneath the willow tree lies a forgotten grave.'\n",
      "Word: There, DepRel: expl, UPOS: PRON\n",
      " - Expletive found at position 0\n",
      "Word: beneath, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: willow, DepRel: compound, UPOS: NOUN\n",
      "Word: tree, DepRel: nsubj, UPOS: NOUN\n",
      " - Subject found at position 4\n",
      "Word: lies, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 5\n",
      "Word: a, DepRel: det, UPOS: DET\n",
      "Word: forgotten, DepRel: amod, UPOS: VERB\n",
      " - Verb found at position 7\n",
      "Word: grave, DepRel: obj, UPOS: NOUN\n",
      " - Potential subject found at position 8\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Expletive inversion detected\n",
      "\n",
      "Processing sentence: 'Here in the quiet of the morning, peace can be found.'\n",
      "Word: Here, DepRel: advmod, UPOS: ADV\n",
      " - Emphatic word found at position 0\n",
      "Word: in, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: quiet, DepRel: obl, UPOS: NOUN\n",
      "Word: of, DepRel: case, UPOS: ADP\n",
      "Word: the, DepRel: det, UPOS: DET\n",
      "Word: morning, DepRel: nmod, UPOS: NOUN\n",
      "Word: ,, DepRel: punct, UPOS: PUNCT\n",
      "Word: peace, DepRel: nsubj:pass, UPOS: NOUN\n",
      "Word: can, DepRel: aux, UPOS: AUX\n",
      " - Verb found at position 9\n",
      "Word: be, DepRel: aux:pass, UPOS: AUX\n",
      " - Verb found at position 10\n",
      "Word: found, DepRel: root, UPOS: VERB\n",
      " - Verb found at position 11\n",
      "Word: ., DepRel: punct, UPOS: PUNCT\n",
      " -> Emphatic inversion detected\n",
      "\n",
      "Inversion Frequencies (Normalized):\n",
      "classic_inversion: 0.4286\n",
      "expletive_inversion: 0.2857\n",
      "emphatic_inversion: 0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "def is_inverted_structure(sentence):\n",
    "    subject_positions = []\n",
    "    verb_positions = []\n",
    "    expletive_positions = []\n",
    "    emphatic_positions = []\n",
    "    adv_positions = []\n",
    "    potential_subject_positions = []\n",
    "\n",
    "    print(f\"\\nProcessing sentence: '{sentence.text}'\")\n",
    "\n",
    "    for i, word in enumerate(sentence.words):\n",
    "        print(f\"Word: {word.text}, DepRel: {word.deprel}, UPOS: {word.upos}\")\n",
    "        if word.deprel in ('nsubj', 'nsubjpass', 'csubj', 'csubjpass'):\n",
    "            subject_positions.append(i)\n",
    "            print(f\" - Subject found at position {i}\")\n",
    "        elif word.deprel in ('obj', 'iobj') and word.upos == 'NOUN':\n",
    "            potential_subject_positions.append(i)\n",
    "            print(f\" - Potential subject found at position {i}\")\n",
    "        elif word.upos in ('VERB', 'AUX'):\n",
    "            verb_positions.append(i)\n",
    "            print(f\" - Verb found at position {i}\")\n",
    "        elif word.deprel == 'expl':\n",
    "            expletive_positions.append(i)\n",
    "            print(f\" - Expletive found at position {i}\")\n",
    "        elif word.text.lower() in ('here', 'there') and word.deprel == 'advmod':\n",
    "            emphatic_positions.append(i)\n",
    "            print(f\" - Emphatic word found at position {i}\")\n",
    "        elif word.upos == 'ADV':\n",
    "            adv_positions.append(i)\n",
    "            print(f\" - Adverb found at position {i}\")\n",
    "\n",
    "    \n",
    "    if expletive_positions and verb_positions:\n",
    "        if min(expletive_positions) < min(verb_positions):\n",
    "            print(\" -> Expletive inversion detected\")\n",
    "            return \"expletive_inversion\"\n",
    "\n",
    "    \n",
    "    if emphatic_positions and verb_positions:\n",
    "        if min(emphatic_positions) < min(verb_positions):\n",
    "            print(\" -> Emphatic inversion detected\")\n",
    "            return \"emphatic_inversion\"\n",
    "\n",
    "    \n",
    "    all_subject_positions = subject_positions + potential_subject_positions\n",
    "    if all_subject_positions and verb_positions:\n",
    "        if min(all_subject_positions) > min(verb_positions):\n",
    "            \n",
    "            if (adv_positions and min(adv_positions) == 0) or (sentence.words[0].upos == 'ADP'):\n",
    "                print(\" -> Classic inversion detected (with initial adverb or prepositional phrase)\")\n",
    "                return \"classic_inversion\"\n",
    "            else:\n",
    "                print(\" -> Classic inversion detected\")\n",
    "                return \"classic_inversion\"\n",
    "\n",
    "    \n",
    "    print(\" -> No inversion detected\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def compute_inversion_frequencies(doc):\n",
    "    inversion_counts = {\n",
    "        \"classic_inversion\": 0,\n",
    "        \"expletive_inversion\": 0,\n",
    "        \"emphatic_inversion\": 0\n",
    "    }\n",
    "    \n",
    "    total_sentences = len(doc.sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        return {key: 0.0 for key in inversion_counts}\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        inversion_type = is_inverted_structure(sentence)\n",
    "        if inversion_type:\n",
    "            inversion_counts[inversion_type] += 1\n",
    "\n",
    "    normalized_frequencies = {key: count / total_sentences for key, count in inversion_counts.items()}\n",
    "    \n",
    "    return normalized_frequencies\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"Never have I seen such beauty before. \"  \n",
    "    \"There is a certain magic in the air tonight. \"  \n",
    "    \"Here comes the rain, soaking everything in its path. \"  \n",
    "    \"Under the bridge stood a lone figure, shrouded in mystery. \"  \n",
    "    \"Only by night does the city reveal its true face. \"  \n",
    "    \"There beneath the willow tree lies a forgotten grave. \"  \n",
    "    \"Here in the quiet of the morning, peace can be found.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "inversion_frequencies = compute_inversion_frequencies(doc)\n",
    "\n",
    "\n",
    "print(\"\\nInversion Frequencies (Normalized):\")\n",
    "for inversion_type, frequency in inversion_frequencies.items():\n",
    "    print(f\"{inversion_type}: {frequency:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifiers_per_noun_phrase(doc):\n",
    "\n",
    "    total_modifiers = 0\n",
    "    total_noun_phrases = 0\n",
    "    sentence_modifiers = []  \n",
    "    \n",
    "    \n",
    "    modifier_relations = {'amod', 'nmod', 'acl', 'advmod', 'det', 'appos'}\n",
    "    \n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        noun_phrases = [word for word in sentence.words if word.upos == 'NOUN' or word.upos == 'PROPN']\n",
    "        \n",
    "        if not noun_phrases:\n",
    "            print(f\"DEBUG: No noun phrases found in sentence: '{sentence.text}'\")\n",
    "            continue\n",
    "        \n",
    "        total_noun_phrases += len(noun_phrases)\n",
    "        sentence_mod_count = 0  \n",
    "        \n",
    "        \n",
    "        for noun_phrase in noun_phrases:\n",
    "            modifiers = [word for word in sentence.words if word.deprel in modifier_relations and word.head == noun_phrase.id]\n",
    "            print(f\"DEBUG: Noun phrase '{noun_phrase.text}' has {len(modifiers)} modifiers.\")\n",
    "            total_modifiers += len(modifiers)\n",
    "            sentence_mod_count += len(modifiers)\n",
    "        \n",
    "        sentence_modifiers.append(sentence_mod_count)\n",
    "        print(f\"DEBUG: Sentence '{sentence.text}' has {sentence_mod_count} total modifiers.\")\n",
    "    \n",
    "    \n",
    "    if total_noun_phrases == 0:\n",
    "        print(\"DEBUG: No noun phrases found in the entire document.\")\n",
    "        return 0  \n",
    "\n",
    "    avg_modifiers_per_noun_phrase = total_modifiers / total_noun_phrases\n",
    "    \n",
    "    print(f\"DEBUG: Total noun phrases = {total_noun_phrases}, Total modifiers = {total_modifiers}, Average = {avg_modifiers_per_noun_phrase}\")\n",
    "    print(f\"DEBUG: Modifiers per sentence: {sentence_modifiers}\")\n",
    "    \n",
    "    return avg_modifiers_per_noun_phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: True\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 6, Ratio = 1.0\n",
      "\n",
      "Embedded Clause Ratio: 1.0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Noun phrase 'fox' has 3 modifiers.\n",
      "DEBUG: Noun phrase 'dog' has 2 modifiers.\n",
      "DEBUG: Sentence 'The quick brown fox jumps over the lazy dog.' has 5 total modifiers.\n",
      "DEBUG: Noun phrase 'rabbit' has 2 modifiers.\n",
      "DEBUG: Noun phrase 'speeding' has 0 modifiers.\n",
      "DEBUG: Noun phrase 'car' has 2 modifiers.\n",
      "DEBUG: Sentence 'An incredibly fast and agile rabbit dodged the speeding car.' has 4 total modifiers.\n",
      "DEBUG: Noun phrase 'house' has 3 modifiers.\n",
      "DEBUG: Noun phrase 'figure' has 2 modifiers.\n",
      "DEBUG: Noun phrase 'shadows' has 1 modifiers.\n",
      "DEBUG: Sentence 'In the old abandoned house, the mysterious figure lurked in the shadows.' has 6 total modifiers.\n",
      "DEBUG: Noun phrase 'bouquet' has 3 modifiers.\n",
      "DEBUG: Noun phrase 'flowers' has 1 modifiers.\n",
      "DEBUG: Sentence 'She gave her a beautiful bouquet of fresh flowers.' has 4 total modifiers.\n",
      "DEBUG: Total noun phrases = 10, Total modifiers = 19, Average = 1.9\n",
      "DEBUG: Modifiers per sentence: [5, 4, 6, 4]\n",
      "\n",
      "Average Number of Modifiers per Noun Phrase: 1.9000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_text = (\n",
    "    \"The quick brown fox jumps over the lazy dog. \"  \n",
    "    \"An incredibly fast and agile rabbit dodged the speeding car. \"  \n",
    "    \"In the old abandoned house, the mysterious figure lurked in the shadows.\"  \n",
    "    \"She gave her a beautiful bouquet of fresh flowers.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "avg_modifiers = modifiers_per_noun_phrase(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nAverage Number of Modifiers per Noun Phrase: {avg_modifiers:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: False\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: False\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: False\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 2, Ratio = 0.3333\n",
      "\n",
      "Embedded Clause Ratio: 0.3333\n"
     ]
    }
   ],
   "source": [
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded_clause = any(is_embedded_clause(token) for token in sent.words)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded_clause}\")\n",
    "        if has_embedded_clause:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"  \n",
    "    \"I believe that he is correct. \"  \n",
    "    \"She said that she would come to the party. \"  \n",
    "    \"The cake, which was delicious, was eaten quickly. \"  \n",
    "    \"He runs every morning. \"  \n",
    "    \"The book I bought yesterday is on the table.\"  \n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 68.0MB/s]                    \n",
      "2024-08-26 08:29:48 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:29:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:29:49 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:29:50 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:29:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 55.8MB/s]                    \n",
      "2024-08-26 08:29:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:29:51 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:29:51 INFO: Using device: cpu\n",
      "2024-08-26 08:29:51 INFO: Loading: tokenize\n",
      "2024-08-26 08:29:51 INFO: Loading: mwt\n",
      "2024-08-26 08:29:51 INFO: Loading: pos\n",
      "2024-08-26 08:29:52 INFO: Loading: lemma\n",
      "2024-08-26 08:29:52 INFO: Loading: constituency\n",
      "2024-08-26 08:29:52 INFO: Loading: depparse\n",
      "2024-08-26 08:29:52 INFO: Loading: sentiment\n",
      "2024-08-26 08:29:52 INFO: Loading: ner\n",
      "2024-08-26 08:29:53 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: False\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: False\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: False\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 2, Ratio = 0.3333\n",
      "\n",
      "Embedded Clause Ratio: 0.3333\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            return True\n",
    "        \n",
    "        if word.deprel == \"acl\" and word.upos == \"VERB\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 55.2MB/s]                    \n",
      "2024-08-26 08:30:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:30:53 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:30:54 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:30:56 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:30:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 46.5MB/s]                    \n",
      "2024-08-26 08:30:56 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:30:57 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:30:57 INFO: Using device: cpu\n",
      "2024-08-26 08:30:57 INFO: Loading: tokenize\n",
      "2024-08-26 08:30:57 INFO: Loading: mwt\n",
      "2024-08-26 08:30:57 INFO: Loading: pos\n",
      "2024-08-26 08:30:57 INFO: Loading: lemma\n",
      "2024-08-26 08:30:57 INFO: Loading: constituency\n",
      "2024-08-26 08:30:57 INFO: Loading: depparse\n",
      "2024-08-26 08:30:57 INFO: Loading: sentiment\n",
      "2024-08-26 08:30:58 INFO: Loading: ner\n",
      "2024-08-26 08:30:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedded clause detected: sitting (acl:relcl)\n",
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: correct (ccomp)\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: come (ccomp)\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: delicious (acl:relcl)\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: bought (acl:relcl)\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Total sentences = 6, Sentences with embedded clauses = 5, Ratio = 0.8333\n",
      "\n",
      "Embedded Clause Ratio: 0.8333\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"acl:relcl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            print(f\"DEBUG: Embedded clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "        \n",
    "        if word.deprel == \"acl\" and word.upos == \"VERB\":\n",
    "            print(f\"DEBUG: Reduced relative clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table.\"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 77.3MB/s]                    \n",
      "2024-08-26 08:35:48 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:35:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:35:49 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:35:50 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:35:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.2MB/s]                    \n",
      "2024-08-26 08:35:50 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:35:51 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:35:51 INFO: Using device: cpu\n",
      "2024-08-26 08:35:51 INFO: Loading: tokenize\n",
      "2024-08-26 08:35:51 INFO: Loading: mwt\n",
      "2024-08-26 08:35:51 INFO: Loading: pos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Download and load the English model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m stanza\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_embedded_clause\u001b[39m(token):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/pipeline/core.py:308\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(curr_processor_config)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name] \u001b[38;5;241m=\u001b[39m \u001b[43mNAME_TO_PROCESSOR_CLASS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_processor_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessorRequirementsException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     pipeline_reqs_exceptions\u001b[38;5;241m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/pipeline/processor.py:193\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_variant\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# build the final config for the processor\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/pipeline/pos_processor.py:32\u001b[0m, in \u001b[0;36mPOSProcessor._set_up_model\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharlm_forward_file\u001b[39m\u001b[38;5;124m'\u001b[39m: config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_charlm_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharlm_backward_file\u001b[39m\u001b[38;5;124m'\u001b[39m: config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward_charlm_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)}\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# set up trainer\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:34\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, vocab, pretrain, model_file, device, foundation_cache)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pretrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, foundation_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# load everything from file\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# build model from scratch\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/trainer.py:178\u001b[0m, in \u001b[0;36mTrainer.load\u001b[0;34m(self, filename, pretrain, args, foundation_cache)\u001b[0m\n\u001b[1;32m    176\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m has a finetuned transformer.  Not using transformer cache to make sure the finetuned version of the transformer isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt accidentally used elsewhere\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m    177\u001b[0m     foundation_cache \u001b[38;5;241m=\u001b[39m NoTransformerFoundationCache(foundation_cache)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_hid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshare_hid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_bert_saved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_bert_saved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/model.py:114\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, args, vocab, emb_matrix, share_hid, foundation_cache, bert_model, bert_tokenizer, force_bert_saved, peft_name)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf\u001b[38;5;241m.\u001b[39mappend(clf_constructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposite_deep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], l))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf \u001b[38;5;241m=\u001b[39m \u001b[43mclf_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeep_biaff_hidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m share_hid:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/pos/model.py:107\u001b[0m, in \u001b[0;36mTagger.__init__.<locals>.<lambda>\u001b[0;34m(insize, outsize)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_hid \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpos\u001b[39m\u001b[38;5;124m'\u001b[39m], CompositeVocab) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposite_deep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mufeats_hid \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomposite_deep_biaff_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m     clf_constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m insize, outsize: \u001b[43mBiaffineScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag_emb_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpos\u001b[39m\u001b[38;5;124m'\u001b[39m], CompositeVocab):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpos_clf \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/stanza/models/common/biaffine.py:38\u001b[0m, in \u001b[0;36mBiaffineScorer.__init__\u001b[0;34m(self, input1_size, input2_size, output_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1_size, input2_size, output_size):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_bilin \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBilinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_bilin\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_bilin\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/modules/linear.py:192\u001b[0m, in \u001b[0;36mBilinear.__init__\u001b[0;34m(self, in1_features, in2_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/modules/linear.py:196\u001b[0m, in \u001b[0;36mBilinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 196\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         init\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;241m-\u001b[39mbound, bound)\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/init.py:149\u001b[0m, in \u001b[0;36muniform_\u001b[0;34m(tensor, a, b, generator)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    147\u001b[0m         uniform_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, a\u001b[38;5;241m=\u001b[39ma, b\u001b[38;5;241m=\u001b[39mb, generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m    148\u001b[0m     )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/vm/datasets/vm3_310/lib/python3.10/site-packages/torch/nn/init.py:16\u001b[0m, in \u001b[0;36m_no_grad_uniform_\u001b[0;34m(tensor, a, b, generator)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_uniform_\u001b[39m(tensor, a, b, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"acl:relcl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            print(f\"DEBUG: Embedded clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "        \n",
    "        if word.deprel in {\"acl\", \"acl:relcl\"} and word.upos == \"VERB\":\n",
    "            print(f\"DEBUG: Reduced relative clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table. \"\n",
    "    \"When it rains, the streets get wet. \"\n",
    "    \"The man wearing a red hat walked by. \"\n",
    "    \"I wonder whether she'll arrive on time. \"\n",
    "    \"They decided to go to the beach. \"\n",
    "    \"The car, although old, still runs well. \"\n",
    "    \"She sang while she was cooking dinner. \"\n",
    "    \"The flowers in the garden are beautiful. \"\n",
    "    \"He asked where I had been. \"\n",
    "    \"The movie we watched last night was exciting. \"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedded clause detected: sitting (acl:relcl)\n",
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: correct (ccomp)\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: come (ccomp)\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: delicious (acl:relcl)\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: bought (acl:relcl)\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: rains (advcl)\n",
      "DEBUG: Sentence 'When it rains, the streets get wet.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: wearing (acl)\n",
      "DEBUG: Sentence 'The man wearing a red hat walked by.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: arrive (ccomp)\n",
      "DEBUG: Sentence 'I wonder whether she'll arrive on time.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: go (xcomp)\n",
      "DEBUG: Sentence 'They decided to go to the beach.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: old (advcl)\n",
      "DEBUG: Sentence 'The car, although old, still runs well.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: cooking (advcl)\n",
      "DEBUG: Sentence 'She sang while she was cooking dinner.' has embedded clause: True\n",
      "DEBUG: Sentence 'The flowers in the garden are beautiful.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: where (ccomp)\n",
      "DEBUG: Sentence 'He asked where I had been.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: watched (acl:relcl)\n",
      "DEBUG: Sentence 'The movie we watched last night was exciting.' has embedded clause: True\n",
      "DEBUG: Total sentences = 15, Sentences with embedded clauses = 13, Ratio = 0.8667\n",
      "\n",
      "Embedded Clause Ratio: 0.8667\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: When it rains, the streets get wet.\n",
      "Word: When, POS: ADV, Dependency: advmod\n",
      "Word: it, POS: PRON, Dependency: nsubj\n",
      "Word: rains, POS: VERB, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: streets, POS: NOUN, Dependency: nsubj\n",
      "Word: get, POS: VERB, Dependency: root\n",
      "Word: wet, POS: ADJ, Dependency: xcomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The man wearing a red hat walked by.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: man, POS: NOUN, Dependency: nsubj\n",
      "Word: wearing, POS: VERB, Dependency: acl\n",
      "Word: a, POS: DET, Dependency: det\n",
      "Word: red, POS: ADJ, Dependency: amod\n",
      "Word: hat, POS: NOUN, Dependency: obj\n",
      "Word: walked, POS: VERB, Dependency: root\n",
      "Word: by, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I wonder whether she'll arrive on time.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: wonder, POS: VERB, Dependency: root\n",
      "Word: whether, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: 'll, POS: AUX, Dependency: aux\n",
      "Word: arrive, POS: VERB, Dependency: ccomp\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: time, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: They decided to go to the beach.\n",
      "Word: They, POS: PRON, Dependency: nsubj\n",
      "Word: decided, POS: VERB, Dependency: root\n",
      "Word: to, POS: PART, Dependency: mark\n",
      "Word: go, POS: VERB, Dependency: xcomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: beach, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The car, although old, still runs well.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: car, POS: NOUN, Dependency: nsubj\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: although, POS: SCONJ, Dependency: mark\n",
      "Word: old, POS: ADJ, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: still, POS: ADV, Dependency: advmod\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: well, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She sang while she was cooking dinner.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: sang, POS: VERB, Dependency: root\n",
      "Word: while, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: cooking, POS: VERB, Dependency: advcl\n",
      "Word: dinner, POS: NOUN, Dependency: obj\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The flowers in the garden are beautiful.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: flowers, POS: NOUN, Dependency: nsubj\n",
      "Word: in, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: garden, POS: NOUN, Dependency: nmod\n",
      "Word: are, POS: AUX, Dependency: cop\n",
      "Word: beautiful, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He asked where I had been.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: asked, POS: VERB, Dependency: root\n",
      "Word: where, POS: ADV, Dependency: ccomp\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: had, POS: AUX, Dependency: aux\n",
      "Word: been, POS: AUX, Dependency: cop\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The movie we watched last night was exciting.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: movie, POS: NOUN, Dependency: nsubj\n",
      "Word: we, POS: PRON, Dependency: nsubj\n",
      "Word: watched, POS: VERB, Dependency: acl:relcl\n",
      "Word: last, POS: ADJ, Dependency: amod\n",
      "Word: night, POS: NOUN, Dependency: obl:tmod\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: exciting, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table. \"\n",
    "    \"When it rains, the streets get wet. \"\n",
    "    \"The man wearing a red hat walked by. \"\n",
    "    \"I wonder whether she'll arrive on time. \"\n",
    "    \"They decided to go to the beach. \"\n",
    "    \"The car, although old, still runs well. \"\n",
    "    \"She sang while she was cooking dinner. \"\n",
    "    \"The flowers in the garden are beautiful. \"\n",
    "    \"He asked where I had been. \"\n",
    "    \"The movie we watched last night was exciting. \"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 40.6MB/s]                    \n",
      "2024-08-26 08:36:00 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:36:00 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-08-26 08:36:01 INFO: File exists: /Users/sean/stanza_resources/en/default.zip\n",
      "2024-08-26 08:36:02 INFO: Finished downloading models and saved to /Users/sean/stanza_resources\n",
      "2024-08-26 08:36:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 43.7MB/s]                    \n",
      "2024-08-26 08:36:02 INFO: Downloaded file to /Users/sean/stanza_resources/resources.json\n",
      "2024-08-26 08:36:03 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-08-26 08:36:03 INFO: Using device: cpu\n",
      "2024-08-26 08:36:03 INFO: Loading: tokenize\n",
      "2024-08-26 08:36:03 INFO: Loading: mwt\n",
      "2024-08-26 08:36:03 INFO: Loading: pos\n",
      "2024-08-26 08:36:04 INFO: Loading: lemma\n",
      "2024-08-26 08:36:04 INFO: Loading: constituency\n",
      "2024-08-26 08:36:04 INFO: Loading: depparse\n",
      "2024-08-26 08:36:04 INFO: Loading: sentiment\n",
      "2024-08-26 08:36:04 INFO: Loading: ner\n",
      "2024-08-26 08:36:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedded clause detected: sitting (acl:relcl)\n",
      "DEBUG: Sentence 'The cat that was sitting on the mat looked up.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: correct (ccomp)\n",
      "DEBUG: Sentence 'I believe that he is correct.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: come (ccomp)\n",
      "DEBUG: Sentence 'She said that she would come to the party.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: delicious (acl:relcl)\n",
      "DEBUG: Sentence 'The cake, which was delicious, was eaten quickly.' has embedded clause: True\n",
      "DEBUG: Sentence 'He runs every morning.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: bought (acl:relcl)\n",
      "DEBUG: Sentence 'The book I bought yesterday is on the table.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: rains (advcl)\n",
      "DEBUG: Sentence 'When it rains, the streets get wet.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: wearing (acl)\n",
      "DEBUG: Sentence 'The man wearing a red hat walked by.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: arrive (ccomp)\n",
      "DEBUG: Sentence 'I wonder whether she'll arrive on time.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: go (xcomp)\n",
      "DEBUG: Sentence 'They decided to go to the beach.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: old (advcl)\n",
      "DEBUG: Sentence 'The car, although old, still runs well.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: cooking (advcl)\n",
      "DEBUG: Sentence 'She sang while she was cooking dinner.' has embedded clause: True\n",
      "DEBUG: Sentence 'The flowers in the garden are beautiful.' has embedded clause: False\n",
      "DEBUG: Embedded clause detected: where (ccomp)\n",
      "DEBUG: Sentence 'He asked where I had been.' has embedded clause: True\n",
      "DEBUG: Embedded clause detected: watched (acl:relcl)\n",
      "DEBUG: Sentence 'The movie we watched last night was exciting.' has embedded clause: True\n",
      "DEBUG: Total sentences = 15, Sentences with embedded clauses = 13, Ratio = 0.8667\n",
      "\n",
      "Embedded Clause Ratio: 0.8667\n",
      "\n",
      "Detailed sentence analysis:\n",
      "\n",
      "Sentence: The cat that was sitting on the mat looked up.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cat, POS: NOUN, Dependency: nsubj\n",
      "Word: that, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: sitting, POS: VERB, Dependency: acl:relcl\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: mat, POS: NOUN, Dependency: obl\n",
      "Word: looked, POS: VERB, Dependency: root\n",
      "Word: up, POS: ADP, Dependency: compound:prt\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I believe that he is correct.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: believe, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: he, POS: PRON, Dependency: nsubj\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: correct, POS: ADJ, Dependency: ccomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She said that she would come to the party.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: said, POS: VERB, Dependency: root\n",
      "Word: that, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: would, POS: AUX, Dependency: aux\n",
      "Word: come, POS: VERB, Dependency: ccomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: party, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The cake, which was delicious, was eaten quickly.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: cake, POS: NOUN, Dependency: nsubj:pass\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: which, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: delicious, POS: ADJ, Dependency: acl:relcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: was, POS: AUX, Dependency: aux:pass\n",
      "Word: eaten, POS: VERB, Dependency: root\n",
      "Word: quickly, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He runs every morning.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: every, POS: DET, Dependency: det\n",
      "Word: morning, POS: NOUN, Dependency: obl:tmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The book I bought yesterday is on the table.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: book, POS: NOUN, Dependency: nsubj\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: bought, POS: VERB, Dependency: acl:relcl\n",
      "Word: yesterday, POS: NOUN, Dependency: obl:tmod\n",
      "Word: is, POS: AUX, Dependency: cop\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: table, POS: NOUN, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: When it rains, the streets get wet.\n",
      "Word: When, POS: ADV, Dependency: advmod\n",
      "Word: it, POS: PRON, Dependency: nsubj\n",
      "Word: rains, POS: VERB, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: streets, POS: NOUN, Dependency: nsubj\n",
      "Word: get, POS: VERB, Dependency: root\n",
      "Word: wet, POS: ADJ, Dependency: xcomp\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The man wearing a red hat walked by.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: man, POS: NOUN, Dependency: nsubj\n",
      "Word: wearing, POS: VERB, Dependency: acl\n",
      "Word: a, POS: DET, Dependency: det\n",
      "Word: red, POS: ADJ, Dependency: amod\n",
      "Word: hat, POS: NOUN, Dependency: obj\n",
      "Word: walked, POS: VERB, Dependency: root\n",
      "Word: by, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: I wonder whether she'll arrive on time.\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: wonder, POS: VERB, Dependency: root\n",
      "Word: whether, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: 'll, POS: AUX, Dependency: aux\n",
      "Word: arrive, POS: VERB, Dependency: ccomp\n",
      "Word: on, POS: ADP, Dependency: case\n",
      "Word: time, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: They decided to go to the beach.\n",
      "Word: They, POS: PRON, Dependency: nsubj\n",
      "Word: decided, POS: VERB, Dependency: root\n",
      "Word: to, POS: PART, Dependency: mark\n",
      "Word: go, POS: VERB, Dependency: xcomp\n",
      "Word: to, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: beach, POS: NOUN, Dependency: obl\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The car, although old, still runs well.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: car, POS: NOUN, Dependency: nsubj\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: although, POS: SCONJ, Dependency: mark\n",
      "Word: old, POS: ADJ, Dependency: advcl\n",
      "Word: ,, POS: PUNCT, Dependency: punct\n",
      "Word: still, POS: ADV, Dependency: advmod\n",
      "Word: runs, POS: VERB, Dependency: root\n",
      "Word: well, POS: ADV, Dependency: advmod\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: She sang while she was cooking dinner.\n",
      "Word: She, POS: PRON, Dependency: nsubj\n",
      "Word: sang, POS: VERB, Dependency: root\n",
      "Word: while, POS: SCONJ, Dependency: mark\n",
      "Word: she, POS: PRON, Dependency: nsubj\n",
      "Word: was, POS: AUX, Dependency: aux\n",
      "Word: cooking, POS: VERB, Dependency: advcl\n",
      "Word: dinner, POS: NOUN, Dependency: obj\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The flowers in the garden are beautiful.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: flowers, POS: NOUN, Dependency: nsubj\n",
      "Word: in, POS: ADP, Dependency: case\n",
      "Word: the, POS: DET, Dependency: det\n",
      "Word: garden, POS: NOUN, Dependency: nmod\n",
      "Word: are, POS: AUX, Dependency: cop\n",
      "Word: beautiful, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: He asked where I had been.\n",
      "Word: He, POS: PRON, Dependency: nsubj\n",
      "Word: asked, POS: VERB, Dependency: root\n",
      "Word: where, POS: ADV, Dependency: ccomp\n",
      "Word: I, POS: PRON, Dependency: nsubj\n",
      "Word: had, POS: AUX, Dependency: aux\n",
      "Word: been, POS: AUX, Dependency: cop\n",
      "Word: ., POS: PUNCT, Dependency: punct\n",
      "\n",
      "Sentence: The movie we watched last night was exciting.\n",
      "Word: The, POS: DET, Dependency: det\n",
      "Word: movie, POS: NOUN, Dependency: nsubj\n",
      "Word: we, POS: PRON, Dependency: nsubj\n",
      "Word: watched, POS: VERB, Dependency: acl:relcl\n",
      "Word: last, POS: ADJ, Dependency: amod\n",
      "Word: night, POS: NOUN, Dependency: obl:tmod\n",
      "Word: was, POS: AUX, Dependency: cop\n",
      "Word: exciting, POS: ADJ, Dependency: root\n",
      "Word: ., POS: PUNCT, Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def is_embedded_clause(token):\n",
    "    \"\"\"Check if a token indicates an embedded clause based on its dependency relation.\"\"\"\n",
    "    return token.deprel in {\"acl\", \"acl:relcl\", \"relcl\", \"ccomp\", \"xcomp\", \"advcl\"}\n",
    "\n",
    "def has_embedded_clause(sentence):\n",
    "    \"\"\"Check if a sentence has an embedded clause.\"\"\"\n",
    "    for word in sentence.words:\n",
    "        if is_embedded_clause(word):\n",
    "            print(f\"DEBUG: Embedded clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "        \n",
    "        if word.deprel in {\"acl\", \"acl:relcl\"} and word.upos == \"VERB\":\n",
    "            print(f\"DEBUG: Reduced relative clause detected: {word.text} ({word.deprel})\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_embedded_clause_ratio(doc):\n",
    "    \"\"\"Calculate the ratio of sentences with embedded clauses in a given text.\"\"\"\n",
    "    embedded_clauses = 0\n",
    "    total_sentences = len(doc.sentences)\n",
    "    \n",
    "    if total_sentences == 0:\n",
    "        print(\"DEBUG: No sentences found in the document.\")\n",
    "        return 0.0\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        has_embedded = has_embedded_clause(sent)\n",
    "        print(f\"DEBUG: Sentence '{sent.text}' has embedded clause: {has_embedded}\")\n",
    "        if has_embedded:\n",
    "            embedded_clauses += 1\n",
    "    \n",
    "    ratio = embedded_clauses / total_sentences\n",
    "    print(f\"DEBUG: Total sentences = {total_sentences}, Sentences with embedded clauses = {embedded_clauses}, Ratio = {ratio:.4f}\")\n",
    "    return ratio\n",
    "\n",
    "\n",
    "sample_text = (\n",
    "    \"The cat that was sitting on the mat looked up. \"\n",
    "    \"I believe that he is correct. \"\n",
    "    \"She said that she would come to the party. \"\n",
    "    \"The cake, which was delicious, was eaten quickly. \"\n",
    "    \"He runs every morning. \"\n",
    "    \"The book I bought yesterday is on the table. \"\n",
    "    \"When it rains, the streets get wet. \"\n",
    "    \"The man wearing a red hat walked by. \"\n",
    "    \"I wonder whether she'll arrive on time. \"\n",
    "    \"They decided to go to the beach. \"\n",
    "    \"The car, although old, still runs well. \"\n",
    "    \"She sang while she was cooking dinner. \"\n",
    "    \"The flowers in the garden are beautiful. \"\n",
    "    \"He asked where I had been. \"\n",
    "    \"The movie we watched last night was exciting. \"\n",
    ")\n",
    "\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "\n",
    "embedded_clause_ratio = calculate_embedded_clause_ratio(doc)\n",
    "\n",
    "\n",
    "print(f\"\\nEmbedded Clause Ratio: {embedded_clause_ratio:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed sentence analysis:\")\n",
    "for sent in doc.sentences:\n",
    "    print(f\"\\nSentence: {sent.text}\")\n",
    "    for word in sent.words:\n",
    "        print(f\"Word: {word.text}, POS: {word.upos}, Dependency: {word.deprel}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vm3_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
