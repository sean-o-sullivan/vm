import pandas as pd
from tqdm.auto import tqdm
import ast
import itertools
from math import comb
from random import sample

def generate_all_true_pairs(df, virtual_text_limit=20):
    max_pairs_limit = comb(virtual_text_limit, 3) * (virtual_text_limit - 3)
    all_true_pairs = []
    all_context_embeddings_with_index = []
    context_index = 0

    for author_id, group in tqdm(df.groupby('author'), desc='Processing authors', dynamic_ncols=True):
        print(f"Processing author ID: {author_id}, Number of texts: {len(group)}")

        author_texts = group.drop(columns=['author', 'embedding_id'])
        if len(author_texts) > virtual_text_limit:
            sampled_indices = sample(range(len(author_texts)), virtual_text_limit)
            author_texts_sampled = author_texts.iloc[sampled_indices]
        else:
            author_texts_sampled = author_texts
        pairs, context_embeddings = generate_true_pairs_for_author(
            author_texts_sampled, author_id, context_index)
        for pair, embedding in zip(pairs[:max_pairs_limit], context_embeddings[:max_pairs_limit]):
            all_true_pairs.append([context_index, pair[1]])
            all_context_embeddings_with_index.append([context_index] + embedding)
            context_index += 1

        print(f"Total true pairs: {len(all_true_pairs)}")
    return all_true_pairs, all_context_embeddings_with_index

def generate_true_pairs_for_author(author_texts, author_id, start_context_index, max_context_size=3):
    true_pairs_list = []
    context_embeddings_list = []
    context_index = start_context_index
    for context_combination in itertools.combinations(author_texts.index, max_context_size):
        context_embedding = author_texts.loc[list(context_combination)].mean().values.tolist()
        context_embeddings_list.append(context_embedding + [author_id])

        remaining_texts = list(set(author_texts.index) - set(context_combination))
        for check_index in remaining_texts:
            check_embedding = author_texts.loc[check_index].values.tolist() + [author_id]
            true_pairs_list.append([context_index, check_embedding])
        context_index += 1
    return true_pairs_list, context_embeddings_list

datasetpath = r"C:\Users\S\Desktop\VerifyMe\datasets\Reuters\RAuthors_20.csv"
df = pd.read_csv(datasetpath)
true_pairs_list, context_embeddings_with_index = generate_all_true_pairs(df)
true_pairs_df = pd.DataFrame(true_pairs_list, columns=['context_index', 'positive_embedding'])
context_embeddings_df = pd.DataFrame(context_embeddings_with_index, 
                                     columns=['context_index'] + [str(i) for i in range(len(context_embeddings_with_index[0]) - 2)] + ['author'])
true_pairs_df['positive_embedding'] = true_pairs_df['positive_embedding'].apply(lambda x: str(x[:-1]))
true_pairs_df = true_pairs_df.drop('context_index', axis=1)

context_embeddings_df['anchor_embedding'] = context_embeddings_df.iloc[:, 1:-1].apply(lambda row: str(row.tolist()), axis=1)
context_embeddings_df = context_embeddings_df.drop(['context_index', 'author'], axis=1)
combined_df = pd.concat([context_embeddings_df['anchor_embedding'], true_pairs_df], axis=1)

false_pairs_list = []
for index, context_row in tqdm(context_embeddings_df.iterrows(), total=context_embeddings_df.shape[0], desc='Generating false pairs', dynamic_ncols=True):
    context_embed = ast.literal_eval(context_row['anchor_embedding'])
    author = df.loc[df.iloc[:, :-1].apply(lambda row: row.tolist() == context_embed, axis=1).any(), 'author'].values[0]
    
    other_authors_data = df[df['author'] != author].drop(columns=['author', 'embedding_id'])
    negative_embed = other_authors_data.sample(1).values.flatten().tolist()
    
    false_pairs_list.append({'anchor_embedding': context_row['anchor_embedding'],
                             'negative_embedding': str(negative_embed)})

false_pairs_df = pd.DataFrame(false_pairs_list)
triplets = pd.concat([combined_df, false_pairs_df['negative_embedding']], axis=1)
print("\nThe triplet has the following columns:", triplets.columns)
print(triplets.head())
triplets_shuffled = triplets.sample(frac=1).reset_index(drop=True)
triplets_shuffled.to_csv(r"Final-Triplets.csv", index=False)

print("Triplet dataset is fianlly saved!")
